<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/logo.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/logo.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo.jpg">
  <link rel="mask-icon" href="/images/logo.jpg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"limokii.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本篇主要介绍早期的PEFT方法，包括Adapter适配器方法、PET、Prefix Tuning以及Prompt Tuning。">
<meta property="og:type" content="article">
<meta property="og:title" content="Parameter Efficient Fine-Tuning(PEFT)系列论文总结(一)">
<meta property="og:url" content="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="Okii&#39;s blog">
<meta property="og:description" content="本篇主要介绍早期的PEFT方法，包括Adapter适配器方法、PET、Prefix Tuning以及Prompt Tuning。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226185027290.png">
<meta property="og:image" content="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226192458146.png">
<meta property="og:image" content="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240101220641347.png">
<meta property="og:image" content="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/957788384.png">
<meta property="og:image" content="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/2581387139.png">
<meta property="og:image" content="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240106145327787.png">
<meta property="og:image" content="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226200610499.png">
<meta property="og:image" content="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231228111353270.png">
<meta property="og:image" content="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240107102116261.png">
<meta property="og:image" content="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240107103047631.png">
<meta property="article:published_time" content="2023-12-24T15:12:11.000Z">
<meta property="article:modified_time" content="2024-01-11T03:18:17.239Z">
<meta property="article:author" content="Okii">
<meta property="article:tag" content="PEFT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226185027290.png">

<link rel="canonical" href="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Parameter Efficient Fine-Tuning(PEFT)系列论文总结(一) | Okii's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Okii's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">仅作为记录自己的学习过程</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">8</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">5</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">10</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Okii">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Okii's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Parameter Efficient Fine-Tuning(PEFT)系列论文总结(一)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-24 23:12:11" itemprop="dateCreated datePublished" datetime="2023-12-24T23:12:11+08:00">2023-12-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本篇主要介绍早期的<code>PEFT</code>方法，包括<code>Adapter</code>适配器方法、<code>PET</code>、<code>Prefix Tuning</code>以及<code>Prompt Tuning</code>。</p>
<span id="more"></span>

<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>当前主流微调方法分为：<code>Fine-tune</code>和<code>PEFT</code>。</p>
<p><code>Fine-tune</code>，也叫全参微调。在<code>LLM</code>出现之前，<code>Bert</code>系列微调模型一直用的这种方法，即模型的全部参数权重参与更新以适配领域数据(有硬件条件的话自然是最好的选择)。</p>
<p><code>PEFT</code>, 包括<code>Prefix Tuning</code>、<code>P-Tuning V1/V2</code>、<code>LoRA</code>、<code>AdaLoRA</code>、<code>QLoRA</code>等方法，即部分模型参数参与微调。这种方式训练快，显存占用少，但是效果可能跟<code>FT（fine-tune）</code>比会稍有损失。</p>
<h2 id="二、Adapter适配器方法"><a href="#二、Adapter适配器方法" class="headerlink" title="二、Adapter适配器方法"></a>二、Adapter适配器方法</h2><p>谷歌的研究人员于2019年在论文<code>《Parameter-Efficient Transfer Learning for NLP》</code>提出针对 <code>BERT </code>的 <code>PEFT</code> 微调方式，拉开了 <code>PEFT </code>研究的序幕。他们指出：</p>
<ul>
<li>在面对特定的下游任务时，如果进行 <code>Full-fintuning</code>（即预训练模型中的所有参数都进行微调），过于低效</li>
<li>而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果</li>
</ul>
<p>于是他们设计了如下图所示的 <code>Adapter</code> 结构，作为全模型微调的一种替代方案:</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226185027290.png" alt="image-20231226185027290" style="zoom:67%;">

<p>在预训练模型每一层(或某些层)中添加<code>Adapter</code>模块(如上图左侧结构所示)，微调时冻结预训练模型主体，由<code>Adapter</code>模块学习特定下游任务的知识。每个<code>Adapter</code>模块由两个前馈子层组成，第一个前馈子层将<code>Transformer</code>块的输出作为输入，将原始输入维度<code>d</code>投影到<code>m</code>，通过控制<code>m</code>的大小来限制<code>Adapte</code>r模块的参数量，通常情况下<code>m&lt;&lt;d</code>。在输出阶段，通过第二个前馈子层还原输入维度，将<code>m</code>重新投影到<code>d</code>，作为<code>Adapter</code>模块的输出(如上图右侧结构)。</p>
<p>可以看到每一个<code>Adapter Layer</code>需要训练的参数，包括偏置的话是: <code>2md + m + d</code></p>
<p>通过添加<code>Adapter</code>模块来产生一个易于扩展的下游模型，每当出现新的下游任务，通过添加<code>Adapter</code>模块来避免全模型微调与灾难性遗忘的问题。<code>Adapter</code>方法不需要微调预训练模型的全部参数，通过引入少量针对特定任务的参数，来存储有关该任务的知识，降低对模型微调的算力要求。</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226192458146.png" alt="image-20231226192458146" style="zoom:67%;">

<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240101220641347.png" alt="image-20240101220641347" style="zoom:80%;">

<p>从实验结果来看，该方法能够在只额外对增加的<code>3.6%</code>参数规模（相比原来预训练模型的参数量）的情况下取得和<code>Full-finetuning</code>接近的效果（<code>GLUE</code>指标在<code>0.4%</code>以内）</p>
<h2 id="三、Pattern-Exploiting-Training-PET"><a href="#三、Pattern-Exploiting-Training-PET" class="headerlink" title="三、Pattern-Exploiting Training(PET)"></a>三、Pattern-Exploiting Training(PET)</h2><p>想要更好的理解下文将讲的<code>Prefix Tuning/P-Tuning</code>，便不得不提<code>Pattern-Exploiting Training(PET)</code>，所谓<code>PET</code>，主要的思想是借助由自然语言构成的模版(英文常称<code>Pattern</code>或<code>Prompt</code>)，将下游任务也转化为一个完形填空任务，这样就可以用<code>BERT</code>的<code>MLM</code>模型来进行预测了。</p>
<p>比如下图中通过条件前缀来实现情感分类和主题分类的例子:</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/957788384.png" alt="img" style="zoom:80%;">

<p>当然，这种方案也不是只有<code>MLM</code>模型可行，用<code>GPT</code>这样的单向语言模型其实也很简单：</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/2581387139.png" alt="img" style="zoom:80%;">

<p>不过由于语言模型是从左往右解码的，因此预测部分只能放在句末了(但还可以往补充前缀说明，只不过预测部分放在最后)</p>
<p>这种人为构造提示模板，就是在输入上加<code>Prompt</code>文本，再对输出进行映射。但这种方式怎么想都不是很优雅，无法避免人工的介入。即使有方法可以批量挖掘，但也有些复杂（有这个功夫能标不少高质量语料），而且模型毕竟是黑盒，对离散文本输入的鲁棒性很差。</p>
<h2 id="四、Prefix-Tuning"><a href="#四、Prefix-Tuning" class="headerlink" title="四、Prefix Tuning"></a>四、Prefix Tuning</h2><p>在<code>Prefix Tuning</code>之前的工作主要是人工设计离散的<code>template</code>或者自动化搜索离散template，问题在于最终的性能对人工设计的<code>template</code>的特别敏感：加一个词或者少一个词，或者变动位置，都会造成很大的变化，所以这种离散化的<code>token</code>的搜索出来的结果可能并不是最优的，下图给出的是一个例子：<br><img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240106145327787.png" alt="image-20240106145327787"></p>
<p><strong>论文摘要：</strong>微调是利用大型预训练语言模型执行下游任务的实际方法。然而，它修改了所有语言模型参数，因此需要为每个任务存储一个完整的副本。在本文中，我们提出了一种名为“前缀调优”的轻量级微调替代方法，用于自然语言生成任务。这种方法保持语言模型参数不变，但优化了一个小型的连续任务特定向量(称为前缀)。前缀调优从提示方法中获得灵感，允许后续标记关注这个前缀，就像它们是”虚拟标记”。我们将前缀调优应用于<code>GPT-2</code>进行表格到文本的生成，以及使用BART进行摘要生成。我们发现，通过仅学习<code>0.1%</code>的参数，前缀调优在完整数据集中获得了与微调相当的性能，在低数据设置中表现更好，并且能够更好地推广到训练中未见的主题。</p>
<p><code>Prefix Tuning</code>是<code>PEFT</code>方法之一，<code>Prefix Tuning</code>之前的工作主要是人工设计模板或者自动化搜索模板，也是<code>prompt</code>范式的第一阶段，就是在输入上加上<code>prompt</code>文本，再对输出进行映射。这种离散模板对模型的鲁棒性很差。所以后续的研究都将离散的方式转成连续。<code>Prefix Tuning</code>在模型输入前添加一个连续的且任务特定的向量序列称之为<code>prefix</code>，固定<code>PLM(预训练模型)</code>的所有参数，只更新优化特定任务的<code>prefix</code>。</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226200610499.png" alt="image-20231226200610499" style="zoom:67%;">

<h3 id="4-1-适配不同任务的prefix构造形式"><a href="#4-1-适配不同任务的prefix构造形式" class="headerlink" title="4.1 适配不同任务的prefix构造形式"></a>4.1 适配不同任务的prefix构造形式</h3><p>针对不同的模型结构，需要构造不同的 <code>Prefix</code>。</p>
<ul>
<li>针对自回归架构模型：在句子前面添加前缀，得到 <code>z = [PREFIX; x; y]</code>，合适的上文能够在固定 <code>LM</code> 的情况下去引导生成下文（比如：<code>GPT3</code>的上下文学习）。</li>
<li>针对编码器-解码器架构模型：<code>Encoder</code>和<code>Decoder</code>都增加了前缀，得到 <code>z = [PREFIX; x; PREFIX&#39;; y]</code>。</li>
<li><code>Encoder</code>端增加前缀是为了引导输入部分的编码，<code>Decoder</code> 端增加前缀是为了引导后续<code>token</code>的生成。</li>
</ul>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231228111353270.png" alt="image-20231228111353270" style="zoom:67%;">

<h3 id="4-2-对virtual-token的编码方式"><a href="#4-2-对virtual-token的编码方式" class="headerlink" title="4.2 对virtual token的编码方式"></a>4.2 对virtual token的编码方式</h3><p>​		同时，为了防止直接更新 <code>Prefix</code> 的参数导致训练不稳定和性能下降的情况，在 <code>Prefix</code> 层前面加了 <code>MLP</code> 结构，训练完成后，只保留 <code>Prefix</code> 的参数。</p>
<p>​		除此之外，通过消融实验证实，只调整<code>embedding</code>层的表现力不够，将导致性能显著下降，因此，在每层<code>Transformer</code>的输入部分都加了<code>prompt</code>的参数，改动较大。</p>
<h2 id="四、Prompt-Tuning"><a href="#四、Prompt-Tuning" class="headerlink" title="四、Prompt Tuning"></a>四、Prompt Tuning</h2><p>2021年4月，<code>Google Research</code>通过此篇论文《<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.08691">The Power of Scale for Parameter-Efficient Prompt Tuning</a>》提出了<code>Prompt Tuning</code>，论文中指出，该方法可以看作是<code>Prefix Tuning</code>的简化版本。</p>
<ul>
<li><p><code>Prefix Tuning</code>在每层<code>Transformer</code>的输入部分都加了<code>prompt</code>的参数，相比之下，<code>Prompt Tuning</code>使用单个提示表示，该表示前置于嵌入式输入。除了需要更少的参数外，所提出方法允许<code>Transformer</code>更新中间层任务表示。</p>
</li>
<li><p>此外，<code>Prefix tuning</code>也依赖于前缀的重新参数化来稳定学习，这在训练期间增加了大量参数，而<code>Prefix tuning</code>的配置不需要这种重新参数化，并且在<code>SuperGLUE</code>任务和模型尺寸上都是鲁棒的。</p>
</li>
<li><p>它冻结整个预训练模型，只允许每个下游任务在输入文本前添加额外的k个可调<code>tokens</code>(意味着它给每个任务都定义了自己的<code>Prompt</code>，在输入层加入<code>prompt tokens</code>)</p>
</li>
</ul>
<p>具体而言，如下图所示：</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240107102116261.png" alt="image-20240107102116261" style="zoom:80%;">

<ul>
<li><p><code>Model Tuning</code>需要为每个下游任务生成整个预训练模型的任务特定副本，并且推理必须分批执行</p>
</li>
<li><p><code>Prompt Tuning</code>只需要为每个任务存储一个小的特定于任务的提示，并使用原始的预训练模型支持混合任务推理</p>
</li>
</ul>
<p>且通过实验发现，随着预训练模型参数量的增加，<code>Prompt Tuning</code>的方法会逼近全参数微调的结果</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240107103047631.png" alt="image-20240107103047631" style="zoom:80%;">

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Okii
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://limokii.github.io/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/" title="Parameter Efficient Fine-Tuning(PEFT)系列论文总结(一)">https://limokii.github.io/2023/12/24/Parameter Efficient Fine-Tuning(PEFT)系列论文总结/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PEFT/" rel="tag"># PEFT</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/12/18/Huggingface%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E9%95%9C%E5%83%8F%E4%B8%8B%E8%BD%BD/" rel="prev" title="Huggingface 服务器端镜像下载">
      <i class="fa fa-chevron-left"></i> Huggingface 服务器端镜像下载
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/12/27/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/" rel="next" title="Parameter Efficient Fine-Tuning(PEFT)系列论文总结(二)">
      Parameter Efficient Fine-Tuning(PEFT)系列论文总结(二) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E5%89%8D%E8%A8%80"><span class="nav-text">一、前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Adapter%E9%80%82%E9%85%8D%E5%99%A8%E6%96%B9%E6%B3%95"><span class="nav-text">二、Adapter适配器方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81Pattern-Exploiting-Training-PET"><span class="nav-text">三、Pattern-Exploiting Training(PET)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81Prefix-Tuning"><span class="nav-text">四、Prefix Tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E9%80%82%E9%85%8D%E4%B8%8D%E5%90%8C%E4%BB%BB%E5%8A%A1%E7%9A%84prefix%E6%9E%84%E9%80%A0%E5%BD%A2%E5%BC%8F"><span class="nav-text">4.1 适配不同任务的prefix构造形式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E5%AF%B9virtual-token%E7%9A%84%E7%BC%96%E7%A0%81%E6%96%B9%E5%BC%8F"><span class="nav-text">4.2 对virtual token的编码方式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81Prompt-Tuning"><span class="nav-text">四、Prompt Tuning</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Okii"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Okii</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/LimOkii" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;LimOkii" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lyfei1126@gmail.com" title="E-Mail → mailto:lyfei1126@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Okii</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
