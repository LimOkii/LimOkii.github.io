<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Huggingface 服务器端镜像下载</title>
    <url>/2023/12/18/Huggingface%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E9%95%9C%E5%83%8F%E4%B8%8B%E8%BD%BD/</url>
    <content><![CDATA[<p>使用<code>Huggingface</code> 官方提供的 <a href="https://hf-mirror.com/docs/huggingface_hub/guides/download#download-from-the-cli">*<em><code>huggingface-cli</code>*</em> </a>命令行工具在服务器端镜像下载权重或文件。</p>
<span id="more"></span>

<h2 id="1、安装依赖"><a href="#1、安装依赖" class="headerlink" title="1、安装依赖"></a>1、安装依赖</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ pip install -U huggingface_hub</span><br></pre></td></tr></table></figure>



<h2 id="2、基本命令示例"><a href="#2、基本命令示例" class="headerlink" title="2、基本命令示例"></a>2、基本命令示例</h2><ul>
<li>在.bashrc文件中添加下面这条镜像配置</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ export HF_ENDPOINT=https://hf-mirror.com</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ huggingface-cli download --resume-download --local-dir-use-symlinks False THUDM/chatglm2-6b --local-dir chatglm2-6b</span><br></pre></td></tr></table></figure>

<ul>
<li><code>download</code>: Hugging Face CLI 的下载命令。</li>
<li><code>--resume-download</code>: 如果下载中断，该选项会尝试继续下载而不是重新开始。</li>
<li><code>--local-dir-use-symlinks False</code>: 该选项指示不使用符号链接。符号链接是一种链接到其他文件或目录的特殊类型的文件，这里指示不使用它们。</li>
<li><code>THUDM/chatglm2-6b</code>: 模型的名称或模型 ID。在这里，它下载的是 <code>THUDM/chatglm2-6b</code>。</li>
<li><code>--local-dir chatglm2-6b</code>: 该选项指定本地保存模型的目录名称，即 <code>chatglm2-6b</code>。</li>
</ul>
<h2 id="3、下载需要登录的模型（Gated-Model）"><a href="#3、下载需要登录的模型（Gated-Model）" class="headerlink" title="3、下载需要登录的模型（Gated Model）"></a>3、下载需要登录的模型（Gated Model）</h2><p>请添加<code>--token hf_***</code>参数，其中<code>hf_***</code>是 <em>access token</em>，请在<a href="https://huggingface.co/settings/tokens">huggingface官网这里</a>获取。示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ huggingface-cli download --token hf_*** --resume-download --local-dir-use-symlinks False meta-llama/Llama-2-7b-hf --local-dir Llama-2-7b-hf</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>服务器命令</category>
      </categories>
      <tags>
        <tag>Huggingface</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo + Typora + Github 博客搭建</title>
    <url>/2023/12/13/Hexo%20+%20Typora%20+%20Github%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<p>Hexo 是一个高效简洁的静态博客框架，支持 Markdown 写作语法，插件丰富，主题优雅，部署方便。目前已成为多数人博客建站的选择，本博客采用Hexo搭建，Markdown编辑软件为Typora并且部署在Github Page上。</p>
<span id="more"></span>

<h1 id="1、Hexo-环境准备"><a href="#1、Hexo-环境准备" class="headerlink" title="1、Hexo 环境准备"></a>1、Hexo 环境准备</h1><p><strong>Hexo 依赖于 <a href="https://nodejs.org/zh-cn/">Node.js</a> 和 <a href="https://git-scm.com/download/">git</a>，所以在安装 Hexo 之前先确保已安装了这两项应用。本教程不再赘述这两项应用的安装教程，可自行查看网上的其他教程</strong></p>
<p>在命令行中通过 npm 来安装 Hexo：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure>

<p><code>-g</code> 表示全局安装，会将 Hexo 命令加入环境变量中，以使其在 cmd 下有效。</p>
<p>新建博客目录，然后在该路径下执行初始化命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo init</span><br></pre></td></tr></table></figure>

<p>执行完毕后，将会生成以下文件结构：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── node_modules       //依赖安装目录</span><br><span class="line">├── scaffolds          //模板文件夹，新建的文章将会从此目录下的文件中继承格式</span><br><span class="line">|   ├── draft.md         //草稿模板</span><br><span class="line">|   ├── page.md          //页面模板</span><br><span class="line">|   └── post.md          //文章模板</span><br><span class="line">├── source             //资源文件夹，用于放置图片、数据、文章等资源</span><br><span class="line">|   └── _posts           //文章目录</span><br><span class="line">├── themes             //主题文件夹</span><br><span class="line">|   └── landscape        //默认主题</span><br><span class="line">├── .gitignore         //指定不纳入git版本控制的文件</span><br><span class="line">├── _config.yml        //站点配置文件</span><br><span class="line">├── db.json            </span><br><span class="line">├── package.json</span><br><span class="line">└── package-lock.json</span><br></pre></td></tr></table></figure>

<p>在根目录下执行如下命令启动 hexo 的内置 Web 服务器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>该命令将会调用 Markdown 引擎解析项目中的博客内容生成网页资源，资源将会存于内存中，所以用户执行完命令之后在项目文件夹中是找不到相关的 Web 资源目录的。该命令还会启动一个简易的 Web 服务器用于提供对内存中网页资源的访问（工作机制类似于 webpack-dev-server），Web 服务器默认监听 4000 端口，用户可在浏览器中通过地址 <code>localhost:4000</code> 访问博客。</p>
<p>此外，可以通过添加命令行参数来支持高级用法：</p>
<ul>
<li>当 4000 端口已被其他应用占用时，可以添加 <code>-p</code> &#x2F; <code>--port</code> 参数来设置 Web 服务监听的端口号，如<code>hexo s -p 8000</code></li>
</ul>
<h1 id="2、Hexo-新建文章"><a href="#2、Hexo-新建文章" class="headerlink" title="2、Hexo 新建文章"></a>2、Hexo 新建文章</h1><p>1、在Hexo根目录下，使用<code>git</code>输入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo new &quot;New&quot;</span><br></pre></td></tr></table></figure>

<p>就创建了一篇新的博文，博文名字为<code>New</code></p>
<p>2、在Hexo目录下的<code>source</code>&#x2F;<code>_posts</code>文件夹内，就会出现刚刚创建的博文：<code>New.md</code></p>
<p><strong>注：对于已经写好的.md文件，可以直接复制到<code>source</code>&#x2F;<code>_posts</code>文件夹内</strong></p>
<h1 id="3、Hexo-博客引擎编译"><a href="#3、Hexo-博客引擎编译" class="headerlink" title="3、Hexo 博客引擎编译"></a>3、Hexo 博客引擎编译</h1><p>在写完博文后（<code>.md</code>文件），回到Hexo根目录，使用<code>git</code>输入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo g</span><br></pre></td></tr></table></figure>

<p>即可编译完成，如果有错误，可以再尝试输入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo g</span><br></pre></td></tr></table></figure>

<p><strong>Hexo更多的基础配置可查看官方文档</strong></p>
<h1 id="4、Hexo-使用主题"><a href="#4、Hexo-使用主题" class="headerlink" title="4、Hexo 使用主题"></a>4、Hexo 使用主题</h1><p>Hexo 中切换主题的方式非常简单，只需要将主题文件拷贝至根目录下的 <code>themes</code> 文件夹中， 然后修改 <code>_config.yml</code> 文件中的 <code>theme</code> 字段即可。本博客使用的是Next主题，Next 作为一款符合广大程序员审美的主题，还是有着较高的出场率的。</p>
<p>在根目录下执行以下命令下载主题文件：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git clone https://github.com/theme-next/hexo-theme-next.git themes/next</span><br></pre></td></tr></table></figure>

<p>也可以在 <a href="https://github.com/theme-next/hexo-theme-next/releases">NexT 版本发布页面</a> 手动下载然后解压到根目录下的 <code>theme</code> 文件夹下，并将文件夹命名为 <code>next</code> 。</p>
<p>打开根目录下的站点配置文件，将 <code>theme</code> 字段的值修改为 <code>next</code>。</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">next</span></span><br></pre></td></tr></table></figure>

<p>这个时候刷新浏览器页面并不会发生变化，需要重启服务器并刷新才能使主题生效。</p>
<p>如果重启服务器仍无效，尝试使用 <code>hexo clean</code> 清除缓存</p>
<p>Next 默认主题风格为 <code>Muse</code>，用户可以在主题配置文件中修改 <code>scheme</code> 字段以选择自己喜欢的主题风格：</p>
<p>本博客采用的<code>Pisces</code></p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Schemes</span></span><br><span class="line"><span class="comment">#scheme: Muse</span></span><br><span class="line"><span class="comment">#scheme: Mist</span></span><br><span class="line"><span class="attr">scheme:</span> <span class="string">Pisces</span></span><br><span class="line"><span class="comment">#scheme: Gemini</span></span><br></pre></td></tr></table></figure>



<h1 id="5、Github-部署"><a href="#5、Github-部署" class="headerlink" title="5、Github 部署"></a>5、Github 部署</h1><h2 id="5-1-连接Github"><a href="#5-1-连接Github" class="headerlink" title="5.1 连接Github"></a>5.1 连接Github</h2><ul>
<li>博客项目根目录   右键  –&gt; <code>Git Bash Here</code>  <strong>设置用户名和邮箱</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git config --global user.name &quot;GitHub的用户名&quot;</span><br><span class="line">$ git config --global user.email &quot;GitHub的邮箱&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li>创建 SSH 密匙</li>
</ul>
<p>​	输入 <code>ssh-keygen -t rsa -C &quot;GitHub 邮箱&quot;</code>，然后一路回车</p>
<ul>
<li>添加密钥</li>
</ul>
<p>​	进入 <code>C:\Users\用户名\.ssh</code> 目录（要勾选显示“隐藏的项目”），用记事本打开公钥 <code>id_rsa.pub</code> 文件并复制里面的内容</p>
<p>​	登陆 <code>GitHub</code> ，进入 <code>Settings</code> 页面，选择左边栏的 <code>SSH and GPG keys</code>，点击 <code>New SSH key</code></p>
<p>​	<code>Title</code> 随便取个名字，粘贴复制的 <code>id_rsa.pub</code> 内容到 <code>Key</code> 中，点击 <code>Add SSH key</code> 完成添加。</p>
<img src="/2023/12/13/Hexo%20+%20Typora%20+%20Github%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20231224114351461.png" alt="image-20231224114351461" style="zoom: 50%;">

<ul>
<li>验证连接</li>
</ul>
<p>​	打开 <code>Git Bash</code>，输入 <code>ssh -T git@github.com</code> 出现 <code>“Are you sure……”</code>，输入 <code>yes</code> 回车确认。</p>
<p>​	显示 <code>“Hi xxx! You&#39;ve successfully……”</code> 即连接成功。</p>
<h2 id="5-2-创建Github仓库"><a href="#5-2-创建Github仓库" class="headerlink" title="5.2 创建Github仓库"></a>5.2 创建Github仓库</h2><p><code>GitHub</code> 主页右上角加号 –&gt; <code>New repository</code>：</p>
<ul>
<li><code>Repository name</code> 中输入 <code>用户名.github.io</code></li>
<li>勾选 <code>“Initialize this repository with a README”</code></li>
<li><code>Description </code>选填</li>
</ul>
<img src="/2023/12/13/Hexo%20+%20Typora%20+%20Github%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20231224115909546.png" alt="image-20231224115909546" style="zoom: 67%;">

<ul>
<li>创建后默认自动启用 <code>HTTPS</code>，博客地址为：<code>https://用户名.github.io</code></li>
</ul>
<h2 id="5-3-部署到Github"><a href="#5-3-部署到Github" class="headerlink" title="5.3 部署到Github"></a>5.3 部署到Github</h2><ul>
<li><p>本地博客测试成功后，就是上传到 <code>GitHub</code> 进行部署，使其能够在网络上访问</p>
</li>
<li><p>首先安装 <code>hexo-deployer-git</code>：</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>

<p>然后修改项目根目录下的<code>_config.yml</code>文件末尾的 <code>Deployment </code>部分，修改成如下：</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repository:</span> <span class="string">git@github.com:用户名/用户名.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">main</span></span><br></pre></td></tr></table></figure>

<p>​	完成后项目根目录<code>Git bash</code>中运行 <code>hexo d</code> 将网站上传部署到 <code>GitHub Pages</code>。</p>
<p>​	这时访问我们的 <code>https://用户名.github.io</code> 就可以看到 <code>Hexo</code> 网站了</p>
<h1 id="6、Typora-编写博客"><a href="#6、Typora-编写博客" class="headerlink" title="6、Typora 编写博客"></a>6、Typora 编写博客</h1><p>用<code>Typora</code>编写<code>md</code>文件，利用<code>Hexo</code>解析时存在图片无法显示的问题</p>
<p>关于这个配置使得<code>md文件-&gt;hexo生成-&gt;所见即所得</code>,我花费了1个晚上的时间才搞明白，主要是别的博客质量太次了，要么照搬，要么方法矛盾，<code>bug</code>了好久</p>
<p>直接上方法</p>
<h2 id="6-1-config-yml配置"><a href="#6-1-config-yml配置" class="headerlink" title="6.1 _config.yml配置"></a>6.1 _config.yml配置</h2><ul>
<li>将项目根目录下的<code>_config.yml</code> 文件中的<code>post_asset_folder</code> 选项设为 <code>true</code></li>
</ul>
<p>​		该操作的作用就是在使用<code>hexo new xxx</code>指令新建博文时，在相同路径下同步创建一个<code>xxx</code>文件夹，而<code>xxx</code>文件夹的作用就是用来存放图片资源；<br>​		就我个人而言，我偏好于直接在<code>source\_posts</code>文件夹下新建<code>md</code>文件，而不是通过<code>hexo new xxx</code>指令；<br>那么直接新建<code>xxx.md</code>再新建<code>xxx</code>文件夹，这种操作的最终效果和使用<code>hexo new xxx</code>指令新建博文的效果一样吗？经过实测，是一样的。</p>
<h2 id="6-2-Typora图像配置"><a href="#6-2-Typora图像配置" class="headerlink" title="6.2 Typora图像配置"></a>6.2 Typora图像配置</h2><p>​		一般来说，大家会现在<code>Typora</code>里写好<code>md</code>格式的博客，然后通过<code>hexo clean</code>、<code>hexo g</code>、<code>hexo s</code>进行一下本地测试，确认无误后再发布到远端。</p>
<p>​		暂且不说<code>hexo</code>博客的图片插入是个问题，我相信当初单纯利用<code>Typora</code>做笔记时，图片文件的管理就让很多人头疼过，<code>Typora</code>官方似乎也意识到这个问题，所以偏好设置中图像是专门的一项，提供了很多选择。</p>
<p>​		我相信大多数同学写<code>md</code>时的图片很多可能是直接截图或者在其他地方copy的，然后在<code>Typora</code>中直接粘贴就ok了。但是这么做之前最好<code>Typora</code>插入图片时采取何种操作配置好，否则<code>md</code>文件和图片相隔十万八千里，后续一旦移动<code>md</code>文件图片就识别不出来，相信大家用过<code>Typora</code>都深有体会。<br>​		所以接下来讲一下<code>Typora</code>如何设置。直接给结论：</p>
<p>​        左上角 <code>文件-&gt;偏好设置-&gt;图像</code></p>
<img src="/2023/12/13/Hexo%20+%20Typora%20+%20Github%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20231224153713398.png" alt="image-20231224153713398" style="zoom: 50%;">

<p>​		框里的路径是：<code>./$&#123;filename&#125;</code>。<code>./</code>表示当前文件夹，<code>$&#123;filename&#125;</code>表示当前文件名。这么设置的好处：</p>
<p>​		1、图片资源文件夹有了；</p>
<p>​		2、而且是同名文件夹！（6.1中的文件夹其实不用手动添加了）</p>
<p>​		这么设置的结果就是：想写篇博客，在<code>source\_posts</code>文件夹下新建<code>xxx.md</code>文件，写着写着需要插一张图，从别处复制，然后在<code>Typora</code>中直接粘贴，<code>bling!</code>图片资源文件夹自动搞定，并不用关心什么文件夹，只管专注于<code>md</code>文件即可。</p>
<h2 id="6-3-插件下载"><a href="#6-3-插件下载" class="headerlink" title="6.3 插件下载"></a>6.3 插件下载</h2><p>这个插件的不同版本可能会有不同的影响，我最终成功解决问题的版本是用如下命令下载的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cnpm install https://github.com/CodeFalling/hexo-asset-image --save</span><br></pre></td></tr></table></figure>

<p>为什么需要这么插件呢？</p>
<p>因为我们虽然在<code>source\_posts</code>文件夹下写了<code>md</code>文件，也有了图片资源文件夹存了图片，但从我们前面<code>Typora</code>中的设置不难知道，实际上<code>md</code>文件中的图片路径都是相对路径<code>（./$&#123;filename&#125;）</code>。而实际网上看到的博文显然不是<code>md</code>文件，而是<code>html</code>文件，从<code>md</code>到<code>html</code>的转变就是<code>Hexo</code>帮我们做的，还记得<code>hexo g</code>命令吗？就是干这个的。转换后的<code>html</code>文件在<code>public</code>目录下，路径是通过日期指示的。</p>
<p><strong>路径转换</strong>就是该插件的作用：根据<code>md</code>图片的相对路径，给出<code>html</code>中图片的绝对路径。</p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>LoRA微调实战</title>
    <url>/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/</url>
    <content><![CDATA[<p>使用LoRA微调LLaMa2，训练LLM在给定上下文无法回答用户问题时拒绝回答的能力，而不是胡说。</p>
<span id="more"></span>

<h2 id="1、实验简介"><a href="#1、实验简介" class="headerlink" title="1、实验简介"></a>1、实验简介</h2><p><strong>选题：《基于  <a href="https://github.com/hiyouga/LLaMA-Factory">https://github.com/hiyouga/LLaMA-Factory</a> 开源项目跑通一个Chat机器人》</strong></p>
<p><strong>选择的是方向1：</strong>尝试对模型进行简单的指令微调，数据集可以是自己构造的、可以是开源的；</p>
<p><strong>Github代码仓库：</strong><a href="https://github.com/LimOkii/nlp_lab">https://github.com/LimOkii/nlp_lab</a></p>
<h3 id="1-1-任务简介"><a href="#1-1-任务简介" class="headerlink" title="1.1 任务简介"></a>1.1 任务简介</h3><p>本次大作业我想微调出一个<code>LLM</code>，使之能够判断给定的语料是否能解答用户问题，不能编造答案。如果根据所有的内容都无法得出明确的结论，需要回复“对不起，根据参考资料无法回答“这些类似的回答。</p>
<p>本次微调的基座采用Meta发布的<code>LLaMa-2-hf-7b-chat</code>版本，训练<code>LLM</code>在给定上下文无法回答用户问题时拒绝回答的能力，而不是胡说。</p>
<p><strong>微调代码参考：</strong><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2">https://github.com/ymcui/Chinese-LLaMA-Alpaca-2</a></p>
<h3 id="1-2-数据集介绍"><a href="#1-2-数据集介绍" class="headerlink" title="1.2  数据集介绍"></a>1.2  数据集介绍</h3><ul>
<li>本次微调采用的数据集是百度发布的<code>WebQA</code></li>
</ul>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">链接: https://pan.baidu.com/s/1pLXEYtd 密码: 6fbf</span><br><span class="line"></span><br><span class="line">文件列表：</span><br><span class="line">WebQA.v1.0/readme.txt</span><br><span class="line">WebQA.v1.0/me_test.ann.json （一个问题只配一段材料，材料中有答案）</span><br><span class="line">WebQA.v1.0/me_test.ir.json （一个问题配多段材料，材料可能有也可能没有答案）</span><br><span class="line">WebQA.v1.0/me_train.json （混合的训练语料）</span><br><span class="line">WebQA.v1.0/me_validation.ann.json （一个问题只配一段材料，材料中有答案）</span><br><span class="line">WebQA.v1.0/me_validation.ir.json （一个问题配多段材料，材料可能有也可能没有答案）</span><br><span class="line"></span><br><span class="line">test跟validation的区别是，理论上来说，validation的分布跟train的分布更加接近。一般而言，validation用来验证模型的精确度，test用来验证模型的迁移能力。ann与ir的区别是，因为ir给每个问题配置了多段材料，可以通过各段材料投票来得到更加可靠的答案；而ann则是一问一材料的形式，是真正考验阅读理解能力的测试集。</span><br></pre></td></tr></table></figure>



<ul>
<li><code>me_train.jsons</code>数据样例如下：</li>
</ul>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;Q_TRN_005637&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;question&quot;</span><span class="punctuation">:</span> <span class="string">&quot;世界上最早的报纸诞生于&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;evidences&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;Q_TRN_005637#00&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;answer&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;no_answer&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;evidence&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1、十月革命胜利,世界上出现了第一个社会主义国家.一个崭新的社会主义报刊体系在苏俄确立形成.&lt;e&gt;2、二战结束后,又有欧、亚、拉美一系列国家脱离了资本主义体系,走社会主义道路,社会主义报业得到很大发展.&lt;e&gt;3、“苏东”剧变后,这些国家的报业结构和性质发生了重大变化.&lt;e&gt;十六、苏联时期报刊体制的主要特征是怎样的?&lt;e&gt;1、苏联的报刊,都属于国家所有,是党和国家机构的重要组成部分；其基本职能是集体的宣传员、集体的鼓动员和集体的组织者.&lt;e&gt;2、苏联的各级报刊绝对服从于各级党委的领导.&lt;e&gt;3、苏联报纸信息来源单一,言论高度集中.&lt;e&gt;4、苏联报刊在建设时期是社会主义建设的工具.&lt;e&gt;十七、发展中国家报业又何共同特点?&lt;e&gt;1、早期报刊、尤其是报业发端较早的国家的早期报刊,大多是殖民者创办的；&lt;e&gt;2、随着反殖民主义反封建斗争的开展,这些国家的民族报刊逐步发展起来,并推动了反殖民主义反封建斗争的进程。</span></span><br><span class="line"><span class="string">        &#125;,</span></span><br><span class="line"><span class="string">        ………</span></span><br><span class="line"><span class="string">        …………</span></span><br><span class="line"><span class="string">        &quot;</span>Q_TRN_005637#<span class="number">03</span><span class="string">&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;</span>answer<span class="string">&quot;: [</span></span><br><span class="line"><span class="string">                &quot;</span>中国<span class="string">&quot;</span></span><br><span class="line"><span class="string">            ],</span></span><br><span class="line"><span class="string">            &quot;</span>evidence<span class="string">&quot;: &quot;</span>北宋末年(公元<span class="number">11</span><span class="punctuation">,</span><span class="number">12</span>世纪)出现的印刷报纸<span class="punctuation">,</span>不仅是中国新闻史上最早的印刷报纸<span class="punctuation">,</span>也是世界新闻史上最早的印刷报纸.中国新闻事业历史的悠久<span class="punctuation">,</span>内容的丰富<span class="punctuation">,</span>是任何西方国家都难以比肩的.&lt;e&gt;中国古代的报纸产生于中国的封建社会时期<span class="punctuation">,</span>是封建地主阶级及其政治代表占统治地位的封建自然经济通过新闻手段的反映.在漫长的封建社会时期<span class="punctuation">,</span>中国古代的报纸<span class="punctuation">,</span>不论是官方的邸报<span class="punctuation">,</span>还是民办的小报和京报<span class="punctuation">,</span>都必然要和当时的封建统治者保持一定的联系.&lt;e&gt;中国古代的邸报有<span class="number">1200</span>年左右的历史.小报有近千年的历史.民间报房出版的邸报<span class="punctuation">,</span>京报有近<span class="number">400</span>年的历史.它们从诞生到结束<span class="punctuation">,</span>持续的时间都不算短<span class="punctuation">,</span>但发展不快<span class="punctuation">,</span>形式内容的变化不大.<span class="string">&quot;</span></span><br><span class="line"><span class="string">        &#125;,</span></span><br><span class="line"><span class="string">        &quot;</span>Q_TRN_005637#<span class="number">04</span><span class="string">&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;</span>answer<span class="string">&quot;: [</span></span><br><span class="line"><span class="string">                &quot;</span>no_answer<span class="string">&quot;</span></span><br><span class="line"><span class="string">            ],</span></span><br><span class="line"><span class="string">            &quot;</span>evidence<span class="string">&quot;: &quot;</span>因此，一般认为，世界上最早的报纸诞生在<span class="number">1609</span>年。<span class="string">&quot;</span></span><br><span class="line"><span class="string">        &#125;,</span></span><br><span class="line"><span class="string">        &quot;</span>Q_TRN_005637#<span class="number">05</span><span class="string">&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;</span>answer<span class="string">&quot;: [</span></span><br><span class="line"><span class="string">                &quot;</span>中国<span class="string">&quot;</span></span><br><span class="line"><span class="string">            ],</span></span><br><span class="line"><span class="string">            &quot;</span>evidence<span class="string">&quot;: &quot;</span>报纸从诞生到今天已经走过了漫长的历史，公元前<span class="number">60</span>年，古罗马政治家恺撒把罗马市以及国家发生的时间书写在白色的木板上，告示市民。这便是世界上最古老的报纸。中国在<span class="number">7</span>世纪，唐朝宫廷内就发行过手写的传阅版，这应该算是中国最早的报纸。<span class="string">&quot;</span></span><br><span class="line"><span class="string">        &#125;,</span></span><br><span class="line"><span class="string">        &quot;</span>Q_TRN_005637#<span class="number">06</span><span class="string">&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;</span>answer<span class="string">&quot;: [</span></span><br><span class="line"><span class="string">                &quot;</span>中国<span class="string">&quot;</span></span><br><span class="line"><span class="string">            ],</span></span><br><span class="line"><span class="string">            &quot;</span>evidence<span class="string">&quot;: &quot;</span>最早的写在纸上的报纸和印刷在纸上的报纸都诞生于中国.唐玄宗开元年间(公元<span class="number">713</span>年-<span class="number">-742</span>年)出现的开元杂报<span class="punctuation">,</span>不仅是中国新闻史上最早的报纸<span class="punctuation">,</span>也是世界新闻史上最早的报纸.<span class="string">&quot;</span></span><br><span class="line"><span class="string">        &#125;,</span></span><br><span class="line"><span class="string">        &quot;</span>Q_TRN_005637#<span class="number">09</span><span class="string">&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;</span>answer<span class="string">&quot;: [</span></span><br><span class="line"><span class="string">                &quot;</span>no_answer<span class="string">&quot;</span></span><br><span class="line"><span class="string">            ],</span></span><br><span class="line"><span class="string">            &quot;</span>evidence<span class="string">&quot;: &quot;</span>答：<span class="number">1566</span>年<span class="punctuation">,</span>世界最早的印刷报纸《威尼斯新闻》诞生于<span class="number">1566</span>年的意大利威尼斯邸报》是我国在世界上发行最早，时间最久的报纸。<span class="string">&quot;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>

<p>​        这个数据集非常适合做给定上下文的回答问题，<code>evidence</code>即是输入给模型的上下文，<code>question</code>则是用户提出的问题，模型需要根据给定的<code>evidence</code>以及<code>question</code>回答<code>no_answer</code>或者是答案。</p>
<h2 id="2、基座模型LLaMa介绍"><a href="#2、基座模型LLaMa介绍" class="headerlink" title="2、基座模型LLaMa介绍"></a>2、基座模型LLaMa介绍</h2><p> 本次微调的基座模型采用Meta发布的<code>LLaMa-2-hf-7b-chat</code>版本</p>
<img src="/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20240101102501900.png" alt="image-20240101102501900" style="zoom:50%;">

<p><code>LLaMa2</code> 和 <code>LLaMa</code> 的模型结构基本一致，共用了 32 个 <code>decoder</code> 层。其中每个 <code>decoder</code> 层如上图右半部分所示，<code>LLaMa2</code> 主要是将 <code>Transformer</code> 中的 <code>Layer Norm</code> 换成了 <code>RMS Norm</code>，<code>Multi-Head Attention</code> 换成了 <code>GQA</code>（&#96;&#96;LLaMa<code>是</code>MQA<code>）, </code>Positional Encoding <code>换成了 </code>Rotary Encoding<code>（</code>RoPE<code> 旋转位置编码），在前馈神经网络（</code>FFN<code>） 使用 </code>SwiGLU<code>激活函数替换了</code>Transformer<code>中的</code>ReLU&#96; 激活函数。</p>
<h2 id="3、实验步骤"><a href="#3、实验步骤" class="headerlink" title="3、实验步骤"></a>3、实验步骤</h2><h3 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h3><p>本次微调代码参考的<code>Chinese-LLaMA-Alpaca-2</code>，指令微调数据格式为<code>Stanford Alpaca</code>：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span><span class="attr">&quot;instruction&quot;</span> <span class="punctuation">:</span> ...<span class="punctuation">,</span></span><br><span class="line">   <span class="attr">&quot;input&quot;</span> <span class="punctuation">:</span> ...<span class="punctuation">,</span></span><br><span class="line">   <span class="attr">&quot;output&quot;</span> <span class="punctuation">:</span> ...<span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  ...</span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>

<p>需要对<code>WebQA</code>数据集做转换，因此编写了脚本 <code>convert_data_to_llama_train.py</code></p>
<p><code>instruction</code>：”请根据给定下文：” +  “evidence” +  ‘\n’  +  “告诉我”  +  “question” + ‘\n’</p>
<p><code>input</code>: “”</p>
<p><code>output</code>：”answer”</p>
<ul>
<li>为了让模型无法回答的输出多样化，如果答案为<code>no_answer</code>,则从以下模板中随机选择一句回答</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 无法回答时，模型给出的回答样例</span></span><br><span class="line">cant_answer_template = [</span><br><span class="line">    <span class="string">&#x27;抱歉，根据您所给的内容，我无法找到有关问题的答案&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;给定的信息中似乎没有提到问题的答案&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;根据提供的内容，我无法找到问题的相关信息&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;根据您提供的上下文，我找不到与问题相关的答案&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;给定的信息中似乎没有与问题有关的信息&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;根据上述内容，我难以找到问题的解答&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;据我所知，问题的答案不在提供的信息中&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;根据上述信息，问题的答案似乎不可得&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;给定的上下文似乎没有包含问题的答案&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;给定的信息中似乎没有与问题有关的线索&#x27;</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<ul>
<li>最终转换后的训练数据样例如下：</li>
</ul>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;请根据给定下文：1、十月革命胜利,世界上出现了第一个社会主义国家.一个崭新的社会主义报刊体系在苏俄确立形成.&lt;e&gt;2、二战结束后,又有欧、亚、拉美一系列国家脱离了资本主义体系,走社会主义道路,社会主义报业得到很大发展.&lt;e&gt;3、“苏东”剧变后,这些国家的报业结构和性质发生了重大变化.&lt;e&gt;十六、苏联时期报刊体制的主要特征是怎样的?&lt;e&gt;1、苏联的报刊,都属于国家所有,是党和国家机构的重要组成部分；其基本职能是集体的宣传员、集体的鼓动员和集体的组织者.&lt;e&gt;2、苏联的各级报刊绝对服从于各级党委的领导.&lt;e&gt;3、苏联报纸信息来源单一,言论高度集中.&lt;e&gt;4、苏联报刊在建设时期是社会主义建设的工具.&lt;e&gt;十七、发展中国家报业又何共同特点?&lt;e&gt;1、早期报刊、尤其是报业发端较早的国家的早期报刊,大多是殖民者创办的；&lt;e&gt;2、随着反殖民主义反封建斗争的开展,这些国家的民族报刊逐步发展起来,并推动了反殖民主义反封建斗争的进程；十八、新闻通讯社是在怎样的背景下诞生的?它的功能与作用如何?\n告诉我世界上最早的报纸诞生于\n&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;给定的上下文似乎没有包含问题的答案&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;请根据给定下文：1566年,世界最早的印刷报纸《威尼斯新闻》诞生于1566年的意大利威尼斯\n告诉我世界上最早的报纸诞生于\n&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;给定的信息中似乎没有与问题有关的信息&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>



<h3 id="3-2-微调训练"><a href="#3-2-微调训练" class="headerlink" title="3.2 微调训练"></a>3.2 微调训练</h3><h4 id="3-2-1-LoRA介绍"><a href="#3-2-1-LoRA介绍" class="headerlink" title="3.2.1 LoRA介绍"></a>3.2.1 LoRA介绍</h4><p>由于大语言模型参数量十分庞大，当将其应用到下游任务时，微调全部参数需要相当高的算力。为了节省成本，研究人员提出了多种参数高效<code>（Parameter Efficient）</code>的微调方法，旨在仅训练少量参数使模型适应到下游任务。本项目使用<code>LoRA(Low-Rank Adaptation of Large Language Models)</code>进行模型微调。<code>LoRA </code>方法 可以在缩减训练参数量和 <code>GPU</code> 显存占用的同时，使训练后的模型具有与全量微调相当的性能。</p>
<p>研究表明，语言模型针对特定任务微调之后，权重矩阵通常具有很低的本征秩 <code>（Intrinsic Rank）</code>。研究人员认为参数更新量即便投影到较小的子空间中，也不会影响学习的有效性。因此，提出固定预训练模型参数不变，在原本权重矩阵旁路添加低秩矩阵的乘积作为可训练参数，用以模拟参数的变化量。具体来说，假设预训练权重为${w_0\ \epsilon \ \mathbb{R}^{d<em>k}}$，可训练参数为${\varDelta W\ &#x3D;\ BA}$，其中${B\ \epsilon \ \mathbb{R}^{d</em>r} }$，${A\ \epsilon \ \mathbb{R}^{r*d}}$，初始化时，矩阵 ${A}$ 通过高斯函数初始化，矩阵${B}$ 为零初始化，使得训练开始之前旁路对原模型不造成影响，即参数改变量为 0。对于该权重的输入 ${x}$ 来说，输出为式${h\ &#x3D;\ W_0x+∆W\ x\ &#x3D;W_0x+BAx}$，<code>LoRA</code>算法结构方法如图：</p>
<img src="/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20240101152138111.png" alt="image-20240101152138111" style="zoom:50%;">



<p>除 <code>LoRA</code> 之外，也其他高效微调方法，如微调适配器<code>（Adapter）</code>或前缀微调<code>（Prefix Tuning）</code>。 适配器方法分别对 <code>Transformer </code>层中的自注意力模块与多层感知<code>（MLP）</code>模块，在其与其之后的残差连接之间添加适配器层<code>（Adapter layer）</code>作为可训练参数，该方法及其变体会增加网络的深度，从而在模型推理时带来额外的时间开销。当没有使用模型或数据并行时，这种开销会较为明显。而对于使用 <code>LoRA </code>的模型来说，由于可以将原权重与训练后权重合并，即 ${W\ &#x3D;\ W_0\ +\ BA}$， 因此在推理时不存在额外的开销。前缀微调是指在输入序列前缀添加连续可微的软提示作为可训练参数。由于模型可接受的最大输入长度有限，随着软提示的参数量增多，实际输入序列的最大长度也会相应减小，影响模型性能。这使得前缀微调的模型性能并非随着可训练参数量单调上升。 在文献的实验中，使用 <code>LoRA</code> 方法训练的 <code>GPT-2</code>、<code>GPT-3</code>模型在相近数量的可训练参数下， 性能均优于或相当于使用上述两种微调方法。</p>
<h4 id="3-2-2-LoRA微调"><a href="#3-2-2-LoRA微调" class="headerlink" title="3.2.2 LoRA微调"></a>3.2.2 LoRA微调</h4><p>数据共<code>40w+</code>条，其中训练数据<code>313910</code>条，其余是验证数据，在单卡<code>A6000 48G显存</code>显卡上采用LoRA方式微调。</p>
<img src="/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20240101104514987.png" alt="image-20240101104514987" style="zoom:50%;">

<p>可以看到原版<code>LLaMa2</code>是<code>7b</code>的权重,使用<code>LoRA</code>方式微调，训练参数仅为<code>0.3b</code>，为初始权重的<code>4%</code>左右，大大减少了需要训练的参数量。</p>
<p>在单卡<code>A6000 48G显存</code>训练一个<code>epoch</code>，约<code>57</code>个小时(包括训练时间和评估时间)，最终的<code>loss</code>从一开始的<code>7</code>左右降到了<code>0.1</code>上下。</p>
<h4 id="3-2-3-权重合并"><a href="#3-2-3-权重合并" class="headerlink" title="3.2.3 权重合并"></a>3.2.3 权重合并</h4><p>手动将<code>LoRA</code>与原版<code>Llama-2</code>合并得到完整模型的流程</p>
<p>确保机器有足够的内存加载完整模型（例如<code>7B</code>模型需要<code>13-15G</code>）以进行合并模型操作</p>
<p><strong>Step 1: 获取原版Llama-2-hf模型</strong></p>
<p><code>HF</code>格式模型相关文件（可以不用下载<code>safetensors</code>格式模型权重）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">config.json</span><br><span class="line">generation_config.json</span><br><span class="line">pytorch_model-00001-of-00002.bin</span><br><span class="line">pytorch_model-00002-of-00002.bin</span><br><span class="line">pytorch_model.bin.index.json</span><br><span class="line">special_tokens_map.json</span><br><span class="line">tokenizer_config.json</span><br><span class="line">tokenizer.json</span><br><span class="line">tokenizer.model</span><br></pre></td></tr></table></figure>

<p><strong>Step 2: 合并LoRA权重，生成全量模型权重</strong></p>
<p>这一步骤会合并<code>LoRA</code>权重，生成全量模型权重。此处可以选择输出<code>PyTorch</code>版本权重（<code>.pth</code>文件）或者输出<code>HuggingFace</code>版本权重（<code>.bin</code>文件）。执行以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ python scripts/merge_llama2_with_chinese_lora_low_mem.py \</span><br><span class="line">    --base_model path_to_original_llama2_hf_dir \</span><br><span class="line">    --lora_model path_to_chinese_llama2_or_alpaca2_lora \</span><br><span class="line">    --output_type huggingface \</span><br><span class="line">    --output_dir path_to_output_dir </span><br><span class="line">    --verbose</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li><code>--base_model</code>：存放<code>HF</code>格式的<code>Llama-2</code>模型权重和配置文件的目录</li>
<li><code>--lora_model</code>：中文<code>LLaMA-2/Alpaca-2 LoRA</code>解压后文件所在目录，也可使用🤗<code>Model Hub</code>模型调用名称（会自动下载）</li>
<li><code>--output_type</code>：指定输出格式，可为<code>pth</code>或<code>huggingface</code>。若不指定，默认为<code>huggingface</code></li>
<li><code>--output_dir</code>：指定保存全量模型权重的目录，默认为<code>./</code></li>
<li>（可选）<code>--verbose</code>：显示合并过程中的详细信息</li>
</ul>
<p><img src="/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20240101161620803.png" alt="image-20240101161620803"></p>
<h2 id="4、实验结果展示"><a href="#4、实验结果展示" class="headerlink" title="4、实验结果展示"></a>4、实验结果展示</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = <span class="string">&quot;/data0/luyifei/cant_ans_merge_weight/&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model)</span><br><span class="line">pipeline = transformers.pipeline(</span><br><span class="line">    <span class="string">&quot;conversational&quot;</span>,</span><br><span class="line">    model=model,</span><br><span class="line">    torch_dtype=torch.float16,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line">)</span><br><span class="line">question = <span class="string">&quot;请根据给定下文：在返回江陵途中，写下了这首诗，抒发了诗人愉悦的心情。\n告诉我李白写过一首诗，对飞舟过峡的动态美景作了绝妙的描述，千古流传，这首诗的题目是什么?&quot;</span></span><br><span class="line">conversation = Conversation(question)</span><br><span class="line">sequences = pipeline(</span><br><span class="line">    conversation,</span><br><span class="line">    do_sample=<span class="literal">True</span>,</span><br><span class="line">    top_k=<span class="number">10</span>,</span><br><span class="line">    num_return_sequences=<span class="number">1</span>,</span><br><span class="line">    eos_token_id=tokenizer.eos_token_id,</span><br><span class="line">    max_length=<span class="number">500</span>,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;问题1是&#x27;</span>,question1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型的回复是：&#x27;</span>sequences.generated_responses[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>加载合并后的权重，3个测试样例如下：</p>
<p><img src="/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20240101174138733.png" alt="image-20240101174138733"></p>
<ul>
<li><p>例子1和例子3回答正确</p>
</li>
<li><p>例子2回答错误</p>
</li>
</ul>
<p>例子1中，给定的上下文中没有关于这首诗的题目，因此模型无法回答该问题。</p>
<p>例子2中，给定的上下文中给出了李白的出生地为碎叶城，但是模型却回复无法回答该问题。</p>
<p>例子3中，给定的上下文中告知b-2轰炸机是美国空军研制，模型也能正确回复答案<code>美国</code></p>
<h2 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h2><p>​		使用<code>LoRA</code>方式微调<code>LLaMa</code>，能使大模型一定程度上根据给定的上下文来回答问题。在给定上下文不包含问题的答案时能输出”对不起，我无法回答该问题”等回复，若给定上下文包含问题的答案，模型也能输出正确答案。</p>
<p>​		但是当我尝试更多样例测试时，发现模型更容易偏向输出无法回答的回复，即使给定上下文中有明确的问题答案。我总结的分析原因如下：</p>
<p>​		微调大型模型时，模型可能会倾向于输出一种相对保守的策略，即更倾向于回答无法回答的响应。这可能是因为微调过程中的数据集中，有更多的例子涉及到模型无法从给定上下文中得知答案的情况，导致模型更容易学习到这种“保守”的回答。</p>
<p>有几个可能的原因导致这种现象：</p>
<ol>
<li><strong>数据分布不均衡：</strong> 可能时微调数据中无法回答的例子相对较多，模型可能会更容易学习到输出类似于“无法回答”的响应。</li>
<li><strong>Loss 函数设计：</strong> 微调过程中使用的损失函数可能也影响了模型的学习方向。如果损失函数更倾向于对无法回答的情况进行惩罚，模型可能更倾向于产生这样的输出。</li>
<li><strong>训练数据中的噪声：</strong> 如果微调数据中包含了噪声或错误的标签，模型可能会过度拟合这些错误的标签，导致更多的“无法回答”响应。</li>
</ol>
<p>​	下一步尝试的改进方向：</p>
<p>​	1、<strong>检查数据质量：</strong> 仔细检查微调数据集，确保标签和上下文对应正确，避免包含噪声或错误的信息。</p>
<p>​    2、<strong>平衡数据集：</strong> 确保微调的数据集中有足够的例子涉及到模型可以回答的情况，以及无法回答的情况，以避免数据分布不均衡。</p>
]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>LoRA</tag>
      </tags>
  </entry>
  <entry>
    <title>Parameter Efficient Fine-Tuning(PEFT)系列论文总结(一)</title>
    <url>/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>本篇主要介绍早期的<code>PEFT</code>方法，包括<code>Adapter</code>适配器方法、<code>PET</code>、<code>Prefix Tuning</code>以及<code>Prompt Tuning</code>。</p>
<span id="more"></span>

<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>当前主流微调方法分为：<code>Fine-tune</code>和<code>PEFT</code>。</p>
<p><code>Fine-tune</code>，也叫全参微调。在<code>LLM</code>出现之前，<code>Bert</code>系列微调模型一直用的这种方法，即模型的全部参数权重参与更新以适配领域数据(有硬件条件的话自然是最好的选择)。</p>
<p><code>PEFT</code>, 包括<code>Prefix Tuning</code>、<code>P-Tuning V1/V2</code>、<code>LoRA</code>、<code>AdaLoRA</code>、<code>QLoRA</code>等方法，即部分模型参数参与微调。这种方式训练快，显存占用少，但是效果可能跟<code>FT（fine-tune）</code>比会稍有损失。</p>
<h2 id="二、Adapter适配器方法"><a href="#二、Adapter适配器方法" class="headerlink" title="二、Adapter适配器方法"></a>二、Adapter适配器方法</h2><p>谷歌的研究人员于2019年在论文<code>《Parameter-Efficient Transfer Learning for NLP》</code>提出针对 <code>BERT </code>的 <code>PEFT</code> 微调方式，拉开了 <code>PEFT </code>研究的序幕。他们指出：</p>
<ul>
<li>在面对特定的下游任务时，如果进行 <code>Full-fintuning</code>（即预训练模型中的所有参数都进行微调），过于低效</li>
<li>而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果</li>
</ul>
<p>于是他们设计了如下图所示的 <code>Adapter</code> 结构，作为全模型微调的一种替代方案:</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226185027290.png" alt="image-20231226185027290" style="zoom:67%;">

<p>在预训练模型每一层(或某些层)中添加<code>Adapter</code>模块(如上图左侧结构所示)，微调时冻结预训练模型主体，由<code>Adapter</code>模块学习特定下游任务的知识。每个<code>Adapter</code>模块由两个前馈子层组成，第一个前馈子层将<code>Transformer</code>块的输出作为输入，将原始输入维度<code>d</code>投影到<code>m</code>，通过控制<code>m</code>的大小来限制<code>Adapte</code>r模块的参数量，通常情况下<code>m&lt;&lt;d</code>。在输出阶段，通过第二个前馈子层还原输入维度，将<code>m</code>重新投影到<code>d</code>，作为<code>Adapter</code>模块的输出(如上图右侧结构)。</p>
<p>可以看到每一个<code>Adapter Layer</code>需要训练的参数，包括偏置的话是: <code>2md + m + d</code></p>
<p>通过添加<code>Adapter</code>模块来产生一个易于扩展的下游模型，每当出现新的下游任务，通过添加<code>Adapter</code>模块来避免全模型微调与灾难性遗忘的问题。<code>Adapter</code>方法不需要微调预训练模型的全部参数，通过引入少量针对特定任务的参数，来存储有关该任务的知识，降低对模型微调的算力要求。</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226192458146.png" alt="image-20231226192458146" style="zoom:67%;">

<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240101220641347.png" alt="image-20240101220641347" style="zoom:80%;">

<p>从实验结果来看，该方法能够在只额外对增加的<code>3.6%</code>参数规模（相比原来预训练模型的参数量）的情况下取得和<code>Full-finetuning</code>接近的效果（<code>GLUE</code>指标在<code>0.4%</code>以内）</p>
<h2 id="三、Pattern-Exploiting-Training-PET"><a href="#三、Pattern-Exploiting-Training-PET" class="headerlink" title="三、Pattern-Exploiting Training(PET)"></a>三、Pattern-Exploiting Training(PET)</h2><p>想要更好的理解下文将讲的<code>Prefix Tuning/P-Tuning</code>，便不得不提<code>Pattern-Exploiting Training(PET)</code>，所谓<code>PET</code>，主要的思想是借助由自然语言构成的模版(英文常称<code>Pattern</code>或<code>Prompt</code>)，将下游任务也转化为一个完形填空任务，这样就可以用<code>BERT</code>的<code>MLM</code>模型来进行预测了。</p>
<p>比如下图中通过条件前缀来实现情感分类和主题分类的例子:</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/957788384.png" alt="img" style="zoom:80%;">

<p>当然，这种方案也不是只有<code>MLM</code>模型可行，用<code>GPT</code>这样的单向语言模型其实也很简单：</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/2581387139.png" alt="img" style="zoom:80%;">

<p>不过由于语言模型是从左往右解码的，因此预测部分只能放在句末了(但还可以往补充前缀说明，只不过预测部分放在最后)</p>
<p>这种人为构造提示模板，就是在输入上加<code>Prompt</code>文本，再对输出进行映射。但这种方式怎么想都不是很优雅，无法避免人工的介入。即使有方法可以批量挖掘，但也有些复杂（有这个功夫能标不少高质量语料），而且模型毕竟是黑盒，对离散文本输入的鲁棒性很差。</p>
<h2 id="四、Prefix-Tuning"><a href="#四、Prefix-Tuning" class="headerlink" title="四、Prefix Tuning"></a>四、Prefix Tuning</h2><p>在<code>Prefix Tuning</code>之前的工作主要是人工设计离散的<code>template</code>或者自动化搜索离散template，问题在于最终的性能对人工设计的<code>template</code>的特别敏感：加一个词或者少一个词，或者变动位置，都会造成很大的变化，所以这种离散化的<code>token</code>的搜索出来的结果可能并不是最优的，下图给出的是一个例子：<br><img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240106145327787.png" alt="image-20240106145327787"></p>
<p><strong>论文摘要：</strong>微调是利用大型预训练语言模型执行下游任务的实际方法。然而，它修改了所有语言模型参数，因此需要为每个任务存储一个完整的副本。在本文中，我们提出了一种名为“前缀调优”的轻量级微调替代方法，用于自然语言生成任务。这种方法保持语言模型参数不变，但优化了一个小型的连续任务特定向量(称为前缀)。前缀调优从提示方法中获得灵感，允许后续标记关注这个前缀，就像它们是”虚拟标记”。我们将前缀调优应用于<code>GPT-2</code>进行表格到文本的生成，以及使用BART进行摘要生成。我们发现，通过仅学习<code>0.1%</code>的参数，前缀调优在完整数据集中获得了与微调相当的性能，在低数据设置中表现更好，并且能够更好地推广到训练中未见的主题。</p>
<p><code>Prefix Tuning</code>是<code>PEFT</code>方法之一，<code>Prefix Tuning</code>之前的工作主要是人工设计模板或者自动化搜索模板，也是<code>prompt</code>范式的第一阶段，就是在输入上加上<code>prompt</code>文本，再对输出进行映射。这种离散模板对模型的鲁棒性很差。所以后续的研究都将离散的方式转成连续。<code>Prefix Tuning</code>在模型输入前添加一个连续的且任务特定的向量序列称之为<code>prefix</code>，固定<code>PLM(预训练模型)</code>的所有参数，只更新优化特定任务的<code>prefix</code>。</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226200610499.png" alt="image-20231226200610499" style="zoom:67%;">

<h3 id="4-1-适配不同任务的prefix构造形式"><a href="#4-1-适配不同任务的prefix构造形式" class="headerlink" title="4.1 适配不同任务的prefix构造形式"></a>4.1 适配不同任务的prefix构造形式</h3><p>针对不同的模型结构，需要构造不同的 <code>Prefix</code>。</p>
<ul>
<li>针对自回归架构模型：在句子前面添加前缀，得到 <code>z = [PREFIX; x; y]</code>，合适的上文能够在固定 <code>LM</code> 的情况下去引导生成下文（比如：<code>GPT3</code>的上下文学习）。</li>
<li>针对编码器-解码器架构模型：<code>Encoder</code>和<code>Decoder</code>都增加了前缀，得到 <code>z = [PREFIX; x; PREFIX&#39;; y]</code>。</li>
<li><code>Encoder</code>端增加前缀是为了引导输入部分的编码，<code>Decoder</code> 端增加前缀是为了引导后续<code>token</code>的生成。</li>
</ul>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231228111353270.png" alt="image-20231228111353270" style="zoom:67%;">

<h3 id="4-2-对virtual-token的编码方式"><a href="#4-2-对virtual-token的编码方式" class="headerlink" title="4.2 对virtual token的编码方式"></a>4.2 对virtual token的编码方式</h3><p>​		同时，为了防止直接更新 <code>Prefix</code> 的参数导致训练不稳定和性能下降的情况，在 <code>Prefix</code> 层前面加了 <code>MLP</code> 结构，训练完成后，只保留 <code>Prefix</code> 的参数。</p>
<p>​		除此之外，通过消融实验证实，只调整<code>embedding</code>层的表现力不够，将导致性能显著下降，因此，在每层<code>Transformer</code>的输入部分都加了<code>prompt</code>的参数，改动较大。</p>
<h2 id="四、Prompt-Tuning"><a href="#四、Prompt-Tuning" class="headerlink" title="四、Prompt Tuning"></a>四、Prompt Tuning</h2><p>2021年4月，<code>Google Research</code>通过此篇论文《<a href="https://arxiv.org/pdf/2104.08691">The Power of Scale for Parameter-Efficient Prompt Tuning</a>》提出了<code>Prompt Tuning</code>，论文中指出，该方法可以看作是<code>Prefix Tuning</code>的简化版本。</p>
<ul>
<li><p><code>Prefix Tuning</code>在每层<code>Transformer</code>的输入部分都加了<code>prompt</code>的参数，相比之下，<code>Prompt Tuning</code>使用单个提示表示，该表示前置于嵌入式输入。除了需要更少的参数外，所提出方法允许<code>Transformer</code>更新中间层任务表示。</p>
</li>
<li><p>此外，<code>Prefix tuning</code>也依赖于前缀的重新参数化来稳定学习，这在训练期间增加了大量参数，而<code>Prefix tuning</code>的配置不需要这种重新参数化，并且在<code>SuperGLUE</code>任务和模型尺寸上都是鲁棒的。</p>
</li>
<li><p>它冻结整个预训练模型，只允许每个下游任务在输入文本前添加额外的k个可调<code>tokens</code>(意味着它给每个任务都定义了自己的<code>Prompt</code>，在输入层加入<code>prompt tokens</code>)</p>
</li>
</ul>
<p>具体而言，如下图所示：</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240107102116261.png" alt="image-20240107102116261" style="zoom:80%;">

<ul>
<li><p><code>Model Tuning</code>需要为每个下游任务生成整个预训练模型的任务特定副本，并且推理必须分批执行</p>
</li>
<li><p><code>Prompt Tuning</code>只需要为每个任务存储一个小的特定于任务的提示，并使用原始的预训练模型支持混合任务推理</p>
</li>
</ul>
<p>且通过实验发现，随着预训练模型参数量的增加，<code>Prompt Tuning</code>的方法会逼近全参数微调的结果</p>
<img src="/2023/12/24/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240107103047631.png" alt="image-20240107103047631" style="zoom:80%;">
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>Parameter Efficient Fine-Tuning(PEFT)系列论文总结(二)</title>
    <url>/2023/12/27/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/</url>
    <content><![CDATA[<p>承接上篇Parameter Efficient Fine-Tuning(PEFT)系列论文总结(一)，本篇主要介绍P-Tuning系列的微调方法。</p>
<span id="more"></span>

<h2 id="一、P-Tuning-v1"><a href="#一、P-Tuning-v1" class="headerlink" title="一、P-Tuning v1"></a>一、P-Tuning v1</h2><p>清华大学的研究者于2021年3月通过此篇论文《<a href="https://arxiv.org/pdf/2103.10385">GPT Understands, Too</a>》提出<code>P-Tuning</code></p>
<p>文章的提出为了解决这样一个问题，如下图给出：</p>
<p>大模型的<code>Prompt</code>构造方式严重影响下游任务的效果。比如：<code>GPT-3</code>采用人工构造的模版来做上下文学习<code>(in-context learning)</code>，但人工设计的模版的变化特别敏感，加一个词或者少一个词，或者变动位置都会造成比较大的变化。</p>
<img src="/2023/12/27/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/image-20240106145327787.png" alt="image-20240106145327787" style="zoom: 67%;">

<p>之前的工作都是这种离散化的<code>token</code>，搜索出来的结果可能并不是最优的，导致性能不稳定。</p>
<p>![image-20240106153147765](Parameter Efficient Fine-Tuning(PEFT)系列论文总结(二)&#x2F;image-20240106153147765.png)</p>
<p>基于此，作者提出了<code>P-Tuning</code>，设计了一种连续可微的<code>virtual token</code>（类似<code>Prefix-Tuning</code>）。<code>P-Tuning</code>成功地实现了模版的自动构建，且借助<code>P-tuning</code>，<code>GPT</code>在<code>SuperGLUE</code>上的成绩首次超过了同等级别的<code>BERT</code>模型，这颠覆了在那年之前“<code>GPT</code>不擅长<code>NLU</code>”的结论，也是该论文命名的缘由</p>
<img src="/2023/12/27/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/image-20240106150023656.png" alt="image-20240106150023656" style="zoom:80%;">

<p><strong>对于上面可微的理解：</strong></p>
<p>原文是<code>continuous</code>连续，这里可微就是可导，应该是反向传播的时候要求导数，所以可以BP优化学习&#x3D;可微。</p>
<p><strong>P-tuning和Prefix Tuning类似</strong>，也放弃了“模版由自然语言构成”这一常规要求，从而将模版的构建转化为连续参数优化问题</p>
<p>下图是一个<code>prompt search</code>针对<code>The capital of Britain is [MASK]</code>(英国的首都是哪个城市)的例子<br>即给定上下文(蓝色区域，“英国”)和目标(红色区域，“[MASK]”)，橙色区域指的是提示符号<code>prompt tokens</code></p>
<img src="/2023/12/27/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/image-20240107104210128.png" alt="image-20240107104210128" style="zoom:80%;">

<ul>
<li><p>在(a)中，提示生成器只收到离散的奖励<br><em>In (a), the prompt generator only receives discrete rewards</em></p>
</li>
<li><p>在(b)中，伪<code>prompt</code>和<code>prompt encoder</code>可以以可微的方式进行优化，有时，在(b)中添加少量与任务相关的<code>anchor tokens</code>(如<code>capital</code>)将带来进一步的改进<br><em>in (b) the pseudo prompts and prompt encoder can be optimized in a differentiable way. Sometimes, adding few task-related anchor tokens(such as “capital” in (b)) will bring further improvement</em></p>
<p>(<code>ps</code>：经过预训练的LM的词嵌入已经变得高度离散，如果随机初始化<code>virtual token</code>，容易优化到局部最优值，而这些<code>virtual token</code>理论是应该有关联的。因此，作者通过实验发现用一个<code>prompt encoder</code>来编码会收敛更快，效果更好。即用一个<code>LSTM+MLP</code>去编码这些<code>virtual token</code>以后，再输入到模型)</p>
</li>
</ul>
<p>换言之，<code>P-tuning</code>做法是用一些伪<code>prompt</code>代替这些显式的<code>prompt</code>(说白了，将自然语言提示的<code>token</code>，替换为可训练的嵌入)<br>具体的做法是可以用预训练词表中的<code>unused token</code>作为伪<code>prompt</code>「<code>BERT</code>的<code>vocab</code>里有<code>unused 1 ~ unused99</code>，就是为了方便增加词汇的」，然后通过训练去更新这些<code>token</code>的参数<br>也就是，<code>P-tuning</code>的<code> Prompt</code>不是显式的，不是我们可以看得懂的字符，而是一些隐式的、经过训练的、模型认为最好的<code>prompt token</code></p>
<p><strong>Prefix Tuning和P-Tuning v1的区别：</strong></p>
<p>1、<code>prefix tuning</code>在所有<code>transformer layer</code>都加入了<code>prompt</code>，而<code>P-Tuning</code>只在输入层加</p>
<p>2、<code>P-Tuning</code>的<code>virtual token</code>的位置也不一定是前缀，插入的位置是可选的</p>
<p>3、作者是用一个<code>prompt encoder</code>来编码收敛更快，效果更好。也就是说，用一个<code>LSTM+MLP</code>去编码这些<code>virtual token</code>以后，再输入到模型</p>
<p>苏剑林说：“在P-Tuning中，如果我们不将新插入的token视为“模版”，是将它视为模型的一部分，那么实际上P-Tuning也是一种类似Adapter的做法，同样是固定原模型的权重，然后插入一些新的可优化参数，同样是只优化这些新参数，只不过这时候新参数插入的是Embedding层，因此，从这个角度看，P-Tuning与Adapter有颇多异曲同工之处”</p>
<h2 id="二、P-Tuning-v2"><a href="#二、P-Tuning-v2" class="headerlink" title="二、P-Tuning v2"></a>二、P-Tuning v2</h2><p>之前的<code>Prompt Tuning</code>和<code>P-Tuning</code>等方法存在两个主要的问题：</p>
<p>第一，缺乏模型参数规模和任务通用性。</p>
<ul>
<li>缺乏规模通用性：<code>Prompt Tuning</code>论文中表明当模型规模超过100亿个参数时，提示优化可以与全量微调相媲美。但是对于那些较小的模型（从100M到1B），提示优化和全量微调的表现有很大差异，这大大限制了提示优化的适用性。</li>
<li>缺乏任务普遍性：尽管<code>Prompt Tuning</code>和<code>P-tuning</code>在一些 <code>NLU </code>基准测试中表现出优势，但提示调优对硬序列标记任务（即序列标注）的有效性尚未得到验证。</li>
</ul>
<p>第二，缺少深度提示优化，在<code>Prompt Tuning</code>和<code>P-tuning</code>中，连续提示只被插入<code>transformer</code>第一层的输入<code>embedding</code>序列中，在接下来的<code>transformer</code>层中，插入连续提示的位置的<code>embedding</code>是由之前的<code>transformer</code>层计算出来的，这可能导致两个可能的优化挑战：</p>
<ul>
<li>由于序列长度的限制，可调参数的数量是有限的</li>
<li>输入<code>embedding</code>对模型预测只有相对间接的影响</li>
</ul>
<p>考虑到这些问题，作者提出了<code>P-Tuning v2</code>，它利用深度提示优化（如：<code>Prefix Tuning</code>），对<code>Prompt Tuning</code>和<code>P-Tuning</code>进行改进，作为一个跨规模和<code>NLU</code>任务的通用解决方案</p>
<p><code>P-Tuning v2</code>（论文： <strong>P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</strong>），该方法在<strong>每一层</strong>都加入了<code>Prompts tokens</code>作为输入，而不是仅仅加在输入层，这带来两个方面的好处：</p>
<ul>
<li>更多可学习的参数（从<code>P-Tuning</code>和<code>Prompt Tuning</code>的0.01%增加到0.1%-3%），同时也足够参数高效。</li>
<li>加入到更深层结构中的<code>Promp</code>能给模型预测带来更直接的影响</li>
</ul>
<img src="/2023/12/27/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/image-20240107135447839.png" alt="image-20240107135447839" style="zoom:80%;">

<p>具体做法基本同<code>Prefix Tuning</code>，可以看作是将<strong>文本生成</strong>的<code>Prefix Tuning</code>技术适配到<code>NLU</code>任务中，然后做了一些改进：</p>
<ul>
<li><strong>移除重参数化的编码器</strong>。以前的方法利用重参数化功能来提高训练速度和鲁棒性（如：<code>Prefix Tuning</code>中的<code>MLP</code>、<code>P-Tuning</code>中的<code>LSTM</code>））。在 <code>P-Tuning v2</code> 中，作者发现重参数化的改进很小，尤其是对于较小的模型，同时还会影响模型的表现。</li>
<li><strong>针对不同任务采用不同的提示长度</strong>。提示长度在提示优化方法的超参数搜索中起着核心作用。在实验中，我们发现不同的理解任务通常用不同的提示长度来实现其最佳性能，这与<code>Prefix-Tuning</code>中的发现一致，不同的文本生成任务可能有不同的最佳提示长度。</li>
<li><strong>引入多任务学习</strong>。先在多任务的<code>Prompt</code>上进行预训练，然后再适配下游任务。多任务学习对我们的方法来说是可选的，但可能是相当有帮助的。一方面，连续提示的随机惯性给优化带来了困难，这可以通过更多的训练数据或与任务相关的无监督预训练来缓解；另一方面，连续提示是跨任务和数据集的特定任务知识的完美载体。我们的实验表明，在一些困难的序列任务中，多任务学习可以作为<code>P-Tuning v2</code>的有益补充。</li>
<li><strong>回归传统的分类标签范式，而不是映射器</strong>。标签词映射器（Label Word Verbalizer）一直是提示优化的核心组成部分，它将<code>one-hot</code>类标签变成有意义的词，以利用预训练语言模型头。尽管它在<code>few-shot</code>设置中具有潜在的必要性，但在全数据监督设置中，<code>Verbalizer</code>并不是必须的。它阻碍了提示调优在我们需要无实际意义的标签和句子嵌入的场景中的应用。因此，<code>P-Tuning v2</code>回归传统的<code>CLS</code>标签分类范式，采用随机初始化的分类头（Classification Head）应用于<code>tokens</code>之上，以增强通用性，可以适配到序列标注任务。</li>
</ul>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformers 理解</title>
    <url>/2023/06/27/Transformers/</url>
    <content><![CDATA[<p>本章是Transformers精讲，并配备哈佛版的基于Pytorch的实现代码</p>
<span id="more"></span>

<h2 id="一、宏观角度"><a href="#一、宏观角度" class="headerlink" title="一、宏观角度"></a>一、宏观角度</h2><p>1、首先将该模型视为一个黑匣子。 在机器翻译应用程序中，它将采用一种语言的句子，并以另一种语言输出其翻译。</p>
<p><img src="/2023/06/27/Transformers/image-20240113204413171.png" alt="image-20240113204413171"></p>
<p>2、继续瓦解，可以看到一个编码组件、一个解码组件以及它们之间的连接。</p>
<p><img src="/2023/06/27/Transformers/image-20240113204543371.png" alt="image-20240113204543371"></p>
<p>3、编码组件是一堆<code>Encoder</code>（论文将其中六个编码器堆叠在一起 - 6 没有什么神奇之处，绝对可以尝试其他排列）。 解码组件是相同数量的<code>Decoder</code>的堆栈。</p>
<p><img src="/2023/06/27/Transformers/image-20240113204719720.png" alt="image-20240113204719720"></p>
<p>4、这些编码器在结构上都是相同的（但它们不共享权重）。 每一层又分为两个子层：</p>
<p><img src="/2023/06/27/Transformers/image-20240113204801613.png" alt="image-20240113204801613"><code>Encoder</code>的输入首先流经<code>Self-Attention</code>，该层帮助编码器在对特定单词进行编码时查看输入句子中的其他单词。 <code>Self-Attention</code>将在后续小节详细介绍。</p>
<p><code>Self-Attention</code>的输出被馈送到前馈神经网络。 完全相同的前馈网络独立应用于每个位置。</p>
<p>5、<code>Decoder</code>具有这两个层，但它们之间是一个注意力层，帮助解码器关注输入句子的相关部分（类似于 <code>seq2seq</code> 模型中注意力的作用）。</p>
<p><img src="/2023/06/27/Transformers/image-20240113205106009.png" alt="image-20240113205106009"></p>
<p>6、有了宏观的感受，再来看一下原论文中的图</p>
<img src="/2023/06/27/Transformers/image-20240113205906204.png" alt="image-20240113205906204" style="zoom:67%;">

<p>拆解完再回过来看是不是更清晰，接下来我将逐一介绍其中的各个模块，并配有<code>Pytorch</code>代码实现</p>
<h2 id="二、Self-Attention"><a href="#二、Self-Attention" class="headerlink" title="二、Self-Attention"></a>二、Self-Attention</h2><h3 id="2-1-理论部分"><a href="#2-1-理论部分" class="headerlink" title="2.1 理论部分"></a>2.1 理论部分</h3><p>其实按照模型流程应该先介绍输入部分</p>
<p>输入部分包括：<strong>词向量嵌入 +  位置向量嵌入</strong>（对应维度直接相加）</p>
<p>但是，先讲完<code>Self-Attention</code>，你就会明白为什么光有词向量嵌入还不够，还需要位置向量嵌入</p>
<p><strong>正式开始Self-Attention</strong></p>
<p><img src="/2023/06/27/Transformers/image-20240113211345136.png" alt="image-20240113211345136"></p>
<p>假设我们已经得到了模型的输入，每个单词都嵌入到大小为 512 的向量中。我将用这些简单的框表示这些向量</p>
<p>将单词嵌入到输入序列中后，每个单词都会流经<code>Encoder</code>的两个子层。</p>
<p><img src="/2023/06/27/Transformers/image-20240113211618648.png" alt="image-20240113211618648"></p>
<p>接下来，我将示例切换为较短的句子，去查看编码器的每个子层中发生的情况</p>
<p>正如已经提到的，<code>Encoder</code>接收向量列表作为输入。 它通过将这些向量传递到<code>Self-Attention</code>层，然后传递到前馈神经网络，然后将输出向上发送到下一个<code>Encoder</code>来处理该列表。</p>
<p><img src="/2023/06/27/Transformers/image-20240113211916839.png" alt="image-20240113211916839"></p>
<p><code>Self-Attention</code>即自注意力机制，感动陌生很正常，因为当时正是在这篇论文中提出的</p>
<p>下面我将介绍到底什么叫自注意力机制</p>
<p>假设以下句子是要翻译的输入句子：</p>
<p>”<code>The animal didn&#39;t cross the street because it was too tired</code>”</p>
<p>这句话中的<code>it</code>指的是什么？ 指的是街道还是动物？ 这对人类来说是一个简单的问题，但对算法来说就不那么简单了。</p>
<p>当模型处理<code>it</code>这个词时，自注意力使其能够将<code>it</code>与“动物”联系起来。</p>
<p>当模型处理每个单词（输入序列中的每个位置）时，自注意力允许它查看输入序列中的其他位置以寻找有助于更好地编码该单词的线索。</p>
<p>你肯定还是不明白它是怎么做的，接下来是细节部分</p>
<p>我们首先看看如何使用向量计算自注意力，然后继续看看它是如何实际实现的——使用矩阵。</p>
<p>计算自注意力的第一步是从每个编码器的输入向量（在本例中为每个单词的嵌入）创建三个向量。 因此，对于每个单词，我们创建一个查询向量、一个键向量和一个值向量。 这些向量是通过将<strong>嵌入</strong>乘以我们在训练过程中训练的<strong>三个矩阵</strong>来创建的。</p>
<p><img src="/2023/06/27/Transformers/image-20240113212618490.png" alt="image-20240113212618490"></p>
<p>将 <code>x1 </code>乘以 <code>Wq</code> 权重矩阵会产生 <code>q1</code>，即与该单词关联的“查询”向量。 我们最终为输入句子中的每个单词创建一个“查询”、一个“键”和一个“值”投影。</p>
<p>这里注意一下维度, 在论文中的<code>Embedding</code>维度<code>d_model = 512</code>，给出的<code>Key</code>维度和<code>Value</code>维度均为64</p>
<p>即<code>d_k = d_v = d_model / h = 64</code>，那么对应<code>QKV</code>的矩阵<code>Wq</code>、<code>Wk</code>、<code>Wv</code>大小都应该是（512，64）</p>
<p>这样就能根据输入得到一个查询向量<code>q1</code>，一组键值对<code>&lt;k1,v1&gt;</code></p>
<p>有了<code>QKV</code>, 接下来需要按照<code>Attention</code>的流程计算<code>q1</code>和<code>k1</code>的<code>Score</code></p>
<p> 根据论文中提到的<strong>缩放点积注意力</strong>(Scaled Dot-Product Attention):</p>
<img src="/2023/06/27/Transformers/image-20240113213524039.png" alt="image-20240113213524039" style="zoom: 80%;">

<p>先进行点积, 再进行缩放, 计算完<code>q1</code>与句中所有单词的<code>k1,k2……kn</code>的得分(这里采用点积得到)后, 再对<code>Score</code>除以根号下<code>d_k</code>, 完成缩放, 最后再通过<code>Softmax</code>得到<code>Attention</code>权重, 加权求和结果称为<code>z1</code></p>
<p><img src="/2023/06/27/Transformers/image-20240113213705835.png" alt="image-20240113213705835"></p>
<p>上面的讨论全部都是针对一个单词的, 但是在实际的运算中, 由于<code>Encoder</code>是线性<code>Stack</code>起来的, 所以其实<code>Encoder</code>的训练是可以并行的, 即<strong>多个单词做完Embedding后作为一个矩阵并行计算</strong>, 假设输入矩阵<code>X</code>，通过<code>Wq</code>、<code>Wk</code>、<code>Wv</code>计算后可以得到<code>Q、K、V</code>：</p>
<img src="/2023/06/27/Transformers/image-20240114201121264.png" alt="image-20240114201121264" style="zoom:50%;">

<p>最后，由于我们处理的是矩阵，我们可以将上述步骤压缩为一个公式来计算自注意力层的输出：</p>
<p>​                                                                                        $$ Attention\left( Q,K,V \right) \ &#x3D;\ Soft\max \left( \frac{QK^T}{\sqrt[]{d_k}} \right) V $$ </p>
<img src="/2023/06/27/Transformers/image-20240114201259276.png" alt="image-20240114201259276" style="zoom: 67%;">



<p><strong>这里除以根号下 d_k 的解释</strong>：</p>
<p>当<code>d_k</code>非常大时, 求得的内积可能会非常大, 如果不进行缩放, 不同的内积大小可能差异会非常大, <code>Softmax</code>在指数运算可能将梯度推到特别小, 导致梯度消失</p>
<p>当 <code>d_k</code>较大时，很有可能存在某个 <code>key</code>，其与<code>query</code>计算出来的对齐分数远大于其他的<code>key</code>与该 <code>query</code>算出的对齐分数。这时，<code>softmax</code> 函数对另外的<code>qk</code>偏导数都趋于 0.</p>
<p>这样结果就是，<code>softmax</code>函数梯度过低（趋于零），使得模型误差反向传播经过<code>softmax</code> 函数后无法继续传播到模型前面部分的参数上，造成这些参数无法得到更新，最终影响模型的训练效率</p>
<h3 id="2-2-代码实现"><a href="#2-2-代码实现" class="headerlink" title="2.2 代码实现"></a>2.2 代码实现</h3><p><strong>Pytorch代码实现如下</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>))/math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>query</code>，<code>key</code>，和 <code>value</code> 的维度是 <code>(batch_size, seq_len, d_model)</code>，其中 <code>seq_len</code> 是输入序列的长度，<code>d_model</code> 是模型的隐藏单元数</p>
</li>
<li><p>在线性映射的步骤中，<code>l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)</code> 使用了多头 (<code>self.h</code>) 和 <code>d_k</code>，它将 <code>query</code>，<code>key</code>，和 <code>value</code> 映射到了 <code>(batch_size, h, seq_len, d_k)</code> 的维度</p>
</li>
<li><p><code>torch.matmul(query, key.transpose(-2, -1))</code> 计算注意力分数，其中 <code>query</code> 和 <code>key</code> 经过了 <code>transpose</code> 操作以匹配维度，最终得到的分数维度是 <code>(batch_size, h, seq_len, seq_len)</code>，在最后两个维度的行上做了<code>softmax</code></p>
</li>
<li><p>在 <code>mask</code> 步骤中，如果有掩码，就使用 <code>scores.masked_fill(mask == 0, -1e9)</code> 将不应考虑的位置的分数设置为一个极小的值 <code>-1e9</code></p>
</li>
<li><p>通过 <code>F.softmax(scores, dim=-1)</code> 对分数进行<code>softmax</code> 操作，得到注意力权重 <code>p_attn</code>，其维度为 <code>(batch_size, h, seq_len, seq_len)</code></p>
</li>
<li><p><code>F.softmax(scores, dim=-1)</code>如果是一个二维的张量，<code>dim=0</code>表示在列上做<code>softmax</code>，<code>dim=1</code>表示在行上做<code>softmax</code></p>
</li>
<li><p>最后通过 <code>torch.matmul(p_attn, value)</code> 得到经过注意力权重调节后的值，其维度为 <code>(batch_size, h, seq_len, d_k)</code></p>
<p><code>batch_size</code> 是每个 batch 的大小，<code>seq_len</code> 是序列的长度，<code>h</code> 是头数，<code>d_k</code> 是每个头的隐藏单元数。在多头注意力机制中，通过对头数进行拼接，最后的输出维度为 <code>(batch_size, seq_len, h * d_k)</code></p>
<h3 id="2-3-多头注意力机制"><a href="#2-3-多头注意力机制" class="headerlink" title="2.3 多头注意力机制"></a>2.3 多头注意力机制</h3><p>多头的思路和<code>CNN</code>中的多个卷积核起到的作用明显是一致的. 所谓”多头”, 放在卷积神经网络里就是卷积层多个卷积核的特征提取过程, 在这里就是进行多次注意力的提取, 就像多个卷积核一样, 多次<strong>不同的初始化矩阵</strong>经过训练可能会有多种<strong>不同的特征,</strong> 每个头用于将输入嵌入投影到不同的表示子空间中，更有利于<strong>不同角度</strong>的特征抽取和信息提取。</p>
<p><img src="/2023/06/27/Transformers/image-20240116133617842.png" alt="image-20240116133617842"></p>
<p>通过多头注意力，为每个头维护单独的 <code>Q/K/V</code> 权重矩阵，从而产生不同的 <code>Q/K/V</code> 矩阵。 正如我们之前所做的那样，我们将 <code>X</code> 乘以 <code>WQ/WK/WV</code> 矩阵以生成 <code>Q/K/V</code> 矩阵。</p>
<p><img src="/2023/06/27/Transformers/image-20240116133811713.png" alt="image-20240116133811713"></p>
<p>如果进行与上面概述相同的自注意力计算，只是使用不同的权重矩阵进行八次不同的计算，我们最终会得到八个不同的 <code>Z </code>矩阵</p>
<p>但是接下来的前馈层不需要八个矩阵——它需要一个矩阵（每个单词一个向量）</p>
<p> 所以我们需要一种方法将这八个压缩成一个矩阵</p>
<p>该怎么做呢？ 将矩阵连接起来，然后将它们乘以一个附加的权重矩阵 <code>Wo</code></p>
<p><img src="/2023/06/27/Transformers/image-20240116134104328.png" alt="image-20240116134104328">这几乎就是多头自注意力的全部内容。 这是相当多的矩阵。 </p>
<p>用下图进行汇总</p>
<p><img src="/2023/06/27/Transformers/image-20240116134218698.png" alt="image-20240116134218698"></p>
<p><strong>Pytorch代码实现如下</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="comment"># 判断d - model % h是否有余数，如果有就报错</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># d-model一般为512（序列符号的embedding长度），h是头数一般为8</span></span><br><span class="line">        self.h = h</span><br><span class="line">        <span class="comment"># 两者相除得到d_k的长度，即query、key矩阵中的列数</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        <span class="comment"># 这里定义的4个线性层, 相当于Wq、Wk、Wv、Wo四个投影矩阵</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span></span><br><span class="line">        <span class="comment"># zip中的(query, key, value)相当于原始的输入, 即 词嵌入 + 位置嵌入</span></span><br><span class="line">        <span class="comment"># l(x) 相当于原始嵌入输入过了Wq、Wk、Wv三个映射矩阵得到query, key, value</span></span><br><span class="line">        <span class="comment"># 然后再reshape到 nbatches * head * seq_len * d_k(64)</span></span><br><span class="line">        query, key, value = [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear.</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="三、Positional-Encoding"><a href="#三、Positional-Encoding" class="headerlink" title="三、Positional Encoding"></a>三、Positional Encoding</h2><h3 id="3-1-理论部分"><a href="#3-1-理论部分" class="headerlink" title="3.1 理论部分"></a>3.1 理论部分</h3><p>正式因为<code>Transformer</code>采用了纯粹的<code>Attention</code>结构, 不像<code>RNN</code>一样能够通过时间步来反映句子中单词的前后关系, 即不能得知<strong>位置信息</strong>。要知道, 在<code>NLP</code>任务中, <strong>语序</strong>是一个相当重要的属性, 所以必须要通过某种方式让<code>Transformer</code>得知单词的位置, 作者通过<strong>位置编码</strong>在每次进入<code>Encoder</code>和<code>Decoder</code>前将位置信息写入。这样来看, 与其叫位置编码, 不如叫<strong>位置嵌入</strong>。</p>
<p>位置编码可以直接与<code>Embedding</code>的向量相加:</p>
<p><img src="/2023/06/27/Transformers/image-20240114162517162.png" alt="image-20240114162517162"></p>
<p>那这个位置编码是怎么得到的呢？</p>
<p>作者的做法非常有意思, 对不同的单词位置, 不同的<code>Embedding</code>维度, 它的编码都是<strong>唯一</strong>的, 应用正弦和余弦函数也方便<code>Transformer</code>学到位置的特征. 如果将当前单词位置记为<code>pos</code>, 而词向量的某个维度记为<code>i</code>, 那么位置编码的方法为:</p>
<p><img src="/2023/06/27/Transformers/image-20240114162811559.png" alt="image-20240114162811559"></p>
<p>如果我们假设嵌入的维数为<code>4</code>，则实际的位置编码将如下所示：</p>
<p><img src="/2023/06/27/Transformers/image-20240114162837902.png" alt="image-20240114162837902"></p>
<p>根据上述<code>PE</code>的位置编码公式，在这个式子中, 编码周期不受单词位置影响, 仅仅与模型开始设计的<code>d_model</code><br> 和<code>Embedding</code>的不同维度<code>i</code>相关</p>
<p>对于不同的<code>i</code>，根据三角函数的周期公式<code>T = 2Π / w</code>，<code>i</code>的范围是[0，256]</p>
<p>可以得到<code>PE</code>的的周期变化范围是<code>[2Π，10000 * 2Π ]</code></p>
<p>这样看，同一位置上的词语, 对于不同的<code>Embedding</code>维度, 都得到不同的编码, 并且随着<code>i</code>的增大, 位置编码的值的变化就越来越慢. 这种编码对于不同维度的<code>Embedding</code>来说是<strong>唯一</strong>的, 因此模型能够学习到关于<code>Embedding</code>的位置信息。</p>
<p>为什么会选择如上公式呢？作者表示：</p>
<p><img src="/2023/06/27/Transformers/image-20240114195410034.png" alt="image-20240114195410034"></p>
<p>已知三角函数公式如下：</p>
<p><img src="/2023/06/27/Transformers/image-20240114195423850.png" alt="image-20240114195423850"></p>
<p>偏移<code>k</code>后，得到的<code>PE</code>如下：</p>
<p><img src="/2023/06/27/Transformers/image-20240114195522326.png" alt="image-20240114195522326"></p>
<p>作者希望借助上述绝对位置的编码公式，让模型能够学习到相对位置信息</p>
<h3 id="3-2-代码实现"><a href="#3-2-代码实现" class="headerlink" title="3.2 代码实现"></a>3.2 代码实现</h3><p><strong>Pytorch代码实现如下</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        <span class="comment"># pe (5000 * 512)</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        <span class="comment"># position (5000 * 1)</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># div_term (256 * 1)</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        <span class="comment"># 偶数列</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        <span class="comment"># 奇数列</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        <span class="comment"># [1, max_len, d_model]</span></span><br><span class="line">        <span class="comment"># pe (1 * 5000 * 512)</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 输入的最终编码 = word_embedding + positional_embedding</span></span><br><span class="line">        x = x + self.pe[:, :x.size(<span class="number">1</span>)].detach()</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>



<h2 id="四、Residuals-LN-FFN"><a href="#四、Residuals-LN-FFN" class="headerlink" title="四、Residuals + LN + FFN"></a>四、Residuals + LN + FFN</h2><p>在继续之前，需要提及<code>Encoder</code>架构中的一个细节，即每个编码器中的每个子层（自注意力，<code>ffnn</code>）周围都有一个残差连接，并且后面是层归一化步骤。</p>
<h3 id="4-1-残差连接"><a href="#4-1-残差连接" class="headerlink" title="4.1 残差连接"></a>4.1 残差连接</h3><p><img src="/2023/06/27/Transformers/image-20240116135601496.png" alt="image-20240116135601496"></p>
<p>如果要可视化与自注意力相关的向量和层归一化操作，如下图：</p>
<p><img src="/2023/06/27/Transformers/image-20240116135615871.png" alt="image-20240116135615871"></p>
<p>这也适用于<code>Decoder</code>的子层。 如果我们考虑一个由 <code>2</code> 个堆叠编码器和解码器组成的 <code>Transformer</code>，它看起来会是这样的：</p>
<p><img src="/2023/06/27/Transformers/image-20240116135752823.png" alt="image-20240116135752823"></p>
<h3 id="4-2-层归一化"><a href="#4-2-层归一化" class="headerlink" title="4.2 层归一化"></a>4.2 层归一化</h3><p><code>Layer Norm</code>也是一种类似于<code>Batch Norm</code>的归一化方式, 同样能起到加快收敛的作用, 在<strong>NLP任务</strong>中比较常用</p>
<p><code>Batch Norm</code>中, 记录下一个<code>Batch</code>中每维<code>Feature</code>的均值和方差, 并进行放缩和平移, 即对<strong>不同样本的同一个通道特征</strong>进行归一化</p>
<p>在<code>Layer Norm</code>中, 只是换了一个维度, 我们对<strong>同一个样本的不同通道</strong>进行归一化</p>
<p><img src="/2023/06/27/Transformers/layernorm.png" alt="img"></p>
<p><strong>BN和LN的区别：</strong></p>
<p><strong>主要区别在于 normalization的方向不同！</strong></p>
<p><code>Batch</code>顾名思义是对一个<code>batch</code>进行操作。假设我们有 10行 3列 的数据，即我们的<code>batchsize = 10</code>，每一行数据有三个特征，假设这三个特征是**[身高、体重、年龄]<strong>。那么<code>BN</code>是针对每一列（特征）进行缩放，例如算出</strong>[身高]**的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种“列缩放”。</p>
<p>而<code>layer</code>方向相反，它针对的是每一行进行缩放。即只看一笔数据，算出这笔所有特征的均值与方差再缩放。这是一种“行缩放”。</p>
<p>细心的你已经看出来，<code>layer normalization</code>对所有的特征进行缩放，这显得很没道理。我们算出一行这**[身高、体重、年龄]**三个特征的均值方差并对其进行缩放，事实上会因为特征的量纲不同而产生很大的影响。但是<code>BN</code>则没有这个影响，因为<code>BN</code>是对一列进行缩放，一列的量纲单位都是相同的。</p>
<p>那么我们为什么还要使用<code>LN</code>呢？因为<code>NLP</code>领域中，<code>LN</code>更为合适。</p>
<p>如果我们将一批文本组成一个<code>batch</code>，那么<code>BN</code>的操作方向是，对每句话的<strong>第一个</strong>词进行操作。但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而<code>BN</code>是<strong>针对每个位置</strong>进行缩放，这<strong>不符合NLP的规律</strong>。</p>
<p>而<code>LN</code>则是针对一句话进行缩放的，且<strong>LN一般用在第三维度</strong>，如<code>[batchsize, seq_len, dims]</code>中的<code>dims</code>，一般为词向量的维度，或者是<code>RNN</code>的输出维度等等，这一维度各个特征的量纲应该相同。因此也不会遇到上面因为特征的量纲不同而导致的缩放问题。</p>
<p><img src="/2023/06/27/Transformers/image-20240116150026767.png" alt="image-20240116150026767"></p>
<p><code>BN</code> 感觉是对样本内部特征的缩放，<code>LN </code>是样本直接之间所有特征的缩放。为啥<code>BN</code>不适合<code>NLP</code> 是因为NLP模型训练里的每次输入的句子都是多个句子，并且长度不一，那么 针对每一句的缩放才更加合理，才能表达每个句子之间代表不同的语义表示，这样让模型更加能捕捉句子之间的上下语义关系。如果要用<code>BN</code>，它首先要面临的长度不一的问题。有时候<code>batch size</code>越小的<code>bn</code>效果更不好</p>
<h3 id="4-3-代码实现"><a href="#4-3-代码实现" class="headerlink" title="4.3 代码实现"></a>4.3 代码实现</h3><p><strong>Pytorch代码实现如下</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        <span class="comment"># 层归一化后, 引入这两个可学习参数</span></span><br><span class="line">        <span class="comment"># Layer Normalization 能够更灵活地适应不同的数据分布</span></span><br><span class="line">        <span class="comment"># 练过程中，这两个参数会通过反向传播进行更新，以最优化模型的性能</span></span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">       <span class="comment"># mean, std的计算在最后一个维度（emb_dim）上进行的</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 对 x - mean 和 (std + self.eps) 的运算仍然是逐元素的，维度与输入张量 x 一致</span></span><br><span class="line">        <span class="comment"># 最终返回的 self.a_2 * (x - mean) / (std + self.eps) + self.b_2 </span></span><br><span class="line">		<span class="comment"># 的形状为(batch_size, seq_len, emb_dim)</span></span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encoder输出和最后进入FFN之前的Residual+LN</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>



<h2 id="五、Decoder"><a href="#五、Decoder" class="headerlink" title="五、Decoder"></a>五、Decoder</h2><h3 id="5-1-理论部分"><a href="#5-1-理论部分" class="headerlink" title="5.1 理论部分"></a>5.1 理论部分</h3><p>现在我们已经涵盖了<code>Encoder</code>方面的大部分概念，我们基本上也知道了解码器的组件是如何工作的</p>
<p>但让我们看看它们是如何协同工作的</p>
<p><code>Encoder</code>首先处理输入序列。 然后，顶部<code>Decoder</code>的输出被转换为一组注意力向量<code>K</code>和<code> V</code>。这些向量将由每个<code>Decoder</code>在其<code>Encoder-Decoder Attention</code>层中使用，这有助于<code>Decoder</code>关注输入序列中的适当位置：</p>
<p><img src="/2023/06/27/Transformers/transformerdecode1.gif" alt="img"></p>
<p>以下步骤重复该过程，直到到达特殊符号，指示<code>Decoder</code>已完成其输出。 每个步骤的输出在下一个时间步骤中被馈送到底部<code>Decoder</code>，并且<code>Decoder</code>像编码器一样冒泡其解码结果</p>
<p><img src="/2023/06/27/Transformers/transformerdecode2.gif" alt="img"></p>
<p>我们将位置编码嵌入并添加到这些<code>Decoder</code>输入中以指示每个单词的位置，这和处理<code>Encoder</code>的输入时一样</p>
<p><strong>但是我们要知道<code>Decoder</code>中的<code>Attention</code>层的运行方式与<code>Encoder</code>中的运行方式略有不同：</strong></p>
<p>在<code>Decoder</code>中，<code>Attention</code>层只允许关注输出序列中较早的位置。 这是通过在自注意力计算中的 <code>softmax </code>步骤之前屏蔽未来位置（将它们设置为 -inf）来完成的，因为在推理的时候肯定不能看到后面的结果</p>
<p><strong>“Encoder-Decoder Attention”层的工作方式与多头自注意力类似，只不过它从其下面的层创建查询矩阵，并从Encoder最后一层的输出中获取键和值矩阵</strong></p>
<p>若仍然沿用传统<code>Seq2Seq+RNN</code>的思路, Decoder是一个<strong>顺序操作</strong>的结构, 我们代入一个场景来看看。假设我们要执行<strong>机器翻译</strong>任务, 要将<code>我 是 学生</code>翻译为<code>I am Student</code>, 假设所有参数与论文中提到的参数一样, <code>batch size</code>视为1. 根据前面已知的知识, <code>Encoder</code>堆叠后的输入和<code>Embedding</code>的大小是相同的</p>
<p>在这里有三个词语, <code>Embedding</code>且通过<code>Encoder</code>后的编码大小为<code>(3,512)</code>。下面对<code>Decoder</code>进行训练:</p>
<ol>
<li>将起始符<code>&lt;start&gt;</code> 作为初始Decoder输入, 经过Decoder处理和分类得到输出<code>I</code>.</li>
<li>将<code>&lt;start&gt; I</code>作为Decoder输入, 经过Decoder处理和分类得到输出<code>am</code>.</li>
<li>将<code>&lt;start&gt; I am</code>作为Decoder输入, 经过Decoder处理和分类得到输出<code>Student</code>.</li>
<li>将<code>&lt;start&gt; I am student</code>作为Decoder输入, 经过Decoder处理和分类得到结束符<code>&lt;end&gt;</code>.</li>
</ol>
<p>这种预测的方式也称为<strong>自回归</strong></p>
<p>如果想做到<strong>并行</strong>训练, 需要将上面的过程转化为一个这样的矩阵直接作为<code>Decoder</code>的输入</p>
<p>因为在<strong>训练时已知任务标签</strong>, 所以可以产生类似的效果，这种方法被称为<code>Teacher Forcing</code></p>
<p>在论文的图中, <code>Mask</code>操作顺序被放在<code>Q</code>和<code>K</code>计算并缩放后, <code>Softmax</code>计算前。如果继续计算下去, 不做<code>Mask</code>, 与<code>V</code>相乘后得到<code>Attention</code>, 所有时间步信息全部都被泄露给<code>Decoder</code>, 必须用<code>Mask</code>将当前预测的单词信息和之后的单词信息全部遮住。</p>
<p>遮住的方法非常简单, 首先不能使用<code>0</code>进行遮盖, 因为<code>Softmax</code>中用零填充会产生错误, <code>e^0=1</code>. 所以必须要用<code>−∞</code>来填充那些不能被看见的部分. 我们直接生成一个下三角全为<code>0</code>, 上三角全部为<strong>负无穷</strong>的矩阵, 与原数据相加就能完成遮盖的效果</p>
<p><img src="/2023/06/27/Transformers/transformer24.png" alt="img"></p>
<p>做<code>Softmax</code>时, 所有的负无穷全变成了<code>0</code>, 不再干扰计算:</p>
<p><img src="/2023/06/27/Transformers/transformer25.png" alt="img"></p>
<p>其实<code>Mask</code>在对句子的<strong>无效部分<pad>填充</pad></strong>时, 也是用同样方法将所有句子补齐, 无效部分用负无穷填充的</p>
<p><strong>PS:</strong><code>Decoder</code>仍然依赖与先前输出结果作为输入, 所以在正式使用时不能实现并行预测, 但在训练的时结果是已知的, 可以实现并行训练</p>
<h3 id="5-2-代码实现"><a href="#5-2-代码实现" class="headerlink" title="5.2 代码实现"></a>5.2 代码实现</h3><p><strong>Pytorch代码实现如下</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Decoder is made of self-attn, src-attn, and feed forward (defined below)&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (right) for connections.&quot;</span></span><br><span class="line">        m = memory</span><br><span class="line">        <span class="comment"># 第一个attention是自注意力，Q，K，V 都是 x</span></span><br><span class="line">        <span class="comment"># 第二个attention的Q是上一层自注意力汇聚输出的x，K和V都是encoder编码的memory</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>

<h2 id="六、The-Final-Linear-and-Softmax-Layer"><a href="#六、The-Final-Linear-and-Softmax-Layer" class="headerlink" title="六、The Final Linear and Softmax Layer"></a>六、The Final Linear and Softmax Layer</h2><p><code>Decoder</code>堆栈输出浮点数向量。 我们如何把它变成一个词？ </p>
<p>这就是最后一个<code>Linear</code>层的工作，后面是 <code>Softmax</code> 层</p>
<p>线性层是一个简单的全连接神经网络，它将<code>Decoder</code>堆栈产生的向量投影到一个更大的向量中，称为<code>logits</code>向量</p>
<p>假设我们的模型知道从训练数据集中学习的 10000 个独特的英语单词（我们模型的“输出词汇”）</p>
<p> 这将使 <code>logits</code> 向量有 10000 个单元格宽——每个单元格对应一个唯一单词的分数。 这就是我们解释线性层模型输出的方式。</p>
<p>然后，<code>Softmax</code> 层将这些分数转换为概率（全部为正，全部加起来为 1.0）。 选择概率最高的单元格，并生成与其关联的单词作为该时间步的输出。</p>
<p><img src="/2023/06/27/Transformers/image-20240117145150356.png" alt="image-20240117145150356"></p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>Transformers</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode 哈希表篇</title>
    <url>/2024/01/14/%E5%93%88%E5%B8%8C%E8%A1%A8%E7%AF%87/</url>
    <content><![CDATA[<p>Leetcode刷题记录——哈希表篇</p>
<span id="more"></span>

<h2 id="Leetcode242-有效的字母异位词"><a href="#Leetcode242-有效的字母异位词" class="headerlink" title="Leetcode242 有效的字母异位词"></a>Leetcode242 有效的字母异位词</h2><p>给定两个字符串 <code>s</code> 和 <code>t</code> ，编写一个函数来判断 <code>t</code> 是否是 <code>s</code> 的字母异位词。</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isAnagram</span>(<span class="params">self, s: <span class="built_in">str</span>, t: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># 首先定义一个record数组, 初始化全为0</span></span><br><span class="line">        record = [<span class="number">0</span>] * <span class="number">26</span></span><br><span class="line">        <span class="comment"># 遍历s字符串, 因为都是小写字母, 所以用相对位置即可</span></span><br><span class="line">        <span class="comment"># 对于s字符串, 出现的字母下标位置+1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">            record[<span class="built_in">ord</span>(i) - <span class="built_in">ord</span>(<span class="string">&quot;a&quot;</span>)] += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 对于t字符串, 出现的字母下标位置-1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> t:</span><br><span class="line">            record[<span class="built_in">ord</span>(i) - <span class="built_in">ord</span>(<span class="string">&quot;a&quot;</span>)] -= <span class="number">1</span></span><br><span class="line">        <span class="comment"># 如果, t是s的字母异位词, 那么record数组1的地方应该刚好被抵消, 全为0</span></span><br><span class="line">        <span class="comment"># 否则t不是s的字母异位词</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> record:</span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>



<h2 id="Leetcode349-两个数组的交集"><a href="#Leetcode349-两个数组的交集" class="headerlink" title="Leetcode349 两个数组的交集"></a>Leetcode349 两个数组的交集</h2><p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">intersection</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], nums2: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        count1 = [<span class="number">0</span>] * <span class="number">1000</span></span><br><span class="line">        count2 = [<span class="number">0</span>] * <span class="number">1000</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums1)):</span><br><span class="line">            count1[nums1[i]] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums2)):</span><br><span class="line">            count2[nums2[i]] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(count1)):</span><br><span class="line">            <span class="keyword">if</span> count1[i] * count2[i] &gt; <span class="number">0</span>:</span><br><span class="line">                res.append(i)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>



<h2 id="Leetcode202-快乐数"><a href="#Leetcode202-快乐数" class="headerlink" title="Leetcode202 快乐数"></a>Leetcode202 快乐数</h2><p>题目中说了会 <strong>无限循环</strong>，那么也就是说<strong>求和的过程中，sum会重复出现，这对解题很重要</strong></p>
<p><strong>当我们遇到了要快速判断一个元素是否出现集合里的时候，就要考虑哈希法了。</strong></p>
<p>所以这道题目使用哈希法，来判断这个<code>sum</code>是否重复出现，如果重复了就是<code>return False</code>， 否则一直找到<code>sum</code>为1为止。</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isHappy</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        res = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">while</span> n != <span class="number">1</span>:</span><br><span class="line">            <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">str</span>(n):</span><br><span class="line">                <span class="built_in">sum</span> += <span class="built_in">int</span>(i) ** <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">sum</span> <span class="keyword">in</span> res:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            res.add(<span class="built_in">sum</span>)</span><br><span class="line">            n = <span class="built_in">sum</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>



<h2 id="Leetcode1-两数之和"><a href="#Leetcode1-两数之和" class="headerlink" title="Leetcode1 两数之和"></a>Leetcode1 两数之和</h2><p>简单的方式是直接两层<code>for</code>循环遍历所有可能的情况，但复杂度是O(n^2)</p>
<p>用<code>map</code>，也就是字典来解题</p>
<p>首先遍历<code>nums</code>，判断<code>target - value</code>是否出现在字典，若字典里有，则返回相应的下标</p>
<p>若没有，则将<code>value-index</code>这对键值对加入字典</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="built_in">map</span> = &#123;&#125;</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> index, value <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            <span class="keyword">if</span> target - value <span class="keyword">in</span> <span class="built_in">map</span>:</span><br><span class="line">                res.append(index)</span><br><span class="line">                res.append(<span class="built_in">map</span>[target-value])</span><br><span class="line">            <span class="keyword">if</span> value <span class="keyword">not</span> <span class="keyword">in</span> <span class="built_in">map</span>:</span><br><span class="line">                <span class="built_in">map</span>[value] = index</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>





<h2 id="Leetcode383-赎金信"><a href="#Leetcode383-赎金信" class="headerlink" title="Leetcode383 赎金信"></a>Leetcode383 赎金信</h2><p>题目中说：<code>magazine</code> 中的每个字符只能在 <code>ransomNote</code> 中使用一次</p>
<p>那么就考虑哈希表的思路去做</p>
<p>首先遍历<code>magazine</code>  ，将其中的字符以及个数添加到<code>map</code>中</p>
<p>再遍历<code>ransomNote</code>，如果其中字符存在<code>map</code>中，且<code>map[i] &gt; 0</code>，则对应<code> -1</code>，否则返回<code>False</code></p>
<p>若循环顺利结束，说明条件成立，返回<code>True</code> </p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">canConstruct</span>(<span class="params">self, ransomNote: <span class="built_in">str</span>, magazine: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="built_in">map</span> = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> magazine:</span><br><span class="line">            <span class="built_in">map</span>[i] = <span class="built_in">map</span>.get(i, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> ransomNote:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> <span class="built_in">map</span> <span class="keyword">and</span> <span class="built_in">map</span>[i] &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">map</span>[i] -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>



<h2 id="Leetcode454-四数相加II"><a href="#Leetcode454-四数相加II" class="headerlink" title="Leetcode454 四数相加II"></a>Leetcode454 四数相加II</h2><p>首先定义一个<code>map</code>，<code>key</code>是<code>a+b</code>的值，<code>value</code>是其出现的次数</p>
<p>再遍历<code>c+d</code>，若相加<code>=0</code>，则<code>count</code>加上对应的<code>value</code></p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fourSumCount</span>(<span class="params">self, nums1: <span class="type">List</span>[<span class="built_in">int</span>], nums2: <span class="type">List</span>[<span class="built_in">int</span>], nums3: <span class="type">List</span>[<span class="built_in">int</span>], nums4: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="built_in">map</span> = &#123;&#125;</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> nums1:</span><br><span class="line">            <span class="keyword">for</span> b <span class="keyword">in</span> nums2:</span><br><span class="line">                <span class="built_in">map</span>[a+b] = <span class="built_in">map</span>.get(a+b, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> nums3:</span><br><span class="line">            <span class="keyword">for</span> d <span class="keyword">in</span> nums4:</span><br><span class="line">                <span class="keyword">if</span> -(c+d) <span class="keyword">in</span> <span class="built_in">map</span> <span class="keyword">and</span> <span class="built_in">map</span>[-(c+d)] &gt; <span class="number">0</span>:</span><br><span class="line">                    count += <span class="built_in">map</span>[-(c+d)]</span><br><span class="line">        <span class="keyword">return</span> count</span><br></pre></td></tr></table></figure>



<h2 id="Leetcode15-三数之和"><a href="#Leetcode15-三数之和" class="headerlink" title="Leetcode15 三数之和"></a>Leetcode15 三数之和</h2><p>首先将数组排序，然后有一层<code>for</code>循环，<code>i</code>从下标0的地方开始</p>
<p>同时定一个下标<code>left</code> 定义在<code>i+1</code>的位置上，定义下标<code>right</code>在数组结尾的位置上。</p>
<p>依然还是在数组中找到 <code>abc </code>使得<code>a + b +c =0</code></p>
<p>我们这里相当于 <code>a = nums[i]</code>，<code>b = nums[left]</code>，<code>c = nums[right]</code></p>
<p>接下来如何移动<code>left</code>和<code>right</code>呢， 如果<code>nums[i] + nums[left] + nums[right] &gt; 0 </code>就说明 此时三数之和大了，因为数组是排序后了，所以<code>right</code>下标就应该向左移动，这样才能让三数之和小一些。</p>
<p>如果 <code>nums[i] + nums[left] + nums[right] &lt; 0</code> 说明此时三数之和小了，<code>left</code> 就向右移动，才能让三数之和大一些，直到<code>left</code>与<code>right</code>相遇为止。</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">threeSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        res = []</span><br><span class="line">        nums.sort()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="comment"># 因为是排序后的, 如果nums[i], 那就没必要继续了</span></span><br><span class="line">            <span class="keyword">if</span> nums[i] &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> res</span><br><span class="line">            <span class="comment"># 每次i往后一位时，需要判断与之前是否相同,因为会重复</span></span><br><span class="line">            <span class="comment"># 看参考样例[-1,0,1,2,-1,-4]</span></span><br><span class="line">            <span class="keyword">if</span> i&gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i-<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            left = i + <span class="number">1</span></span><br><span class="line">            right = <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> left &lt; right:</span><br><span class="line">                <span class="built_in">sum</span> = nums[i] + nums[left] + nums[right]</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">sum</span> &gt; <span class="number">0</span>:</span><br><span class="line">                    right -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> <span class="built_in">sum</span> &lt; <span class="number">0</span>:</span><br><span class="line">                    left += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    res.append([nums[i], nums[left], nums[right]])</span><br><span class="line">                    <span class="comment"># 当前位置匹配成功过,跳过相同元素避免重复</span></span><br><span class="line">                    <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[right] == nums[right-<span class="number">1</span>]:</span><br><span class="line">                        right -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[left] == nums[left+<span class="number">1</span>]:</span><br><span class="line">                        left += <span class="number">1</span></span><br><span class="line">                    right -= <span class="number">1</span></span><br><span class="line">                    left += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>



<h2 id="Leetcode18-四数之和"><a href="#Leetcode18-四数之和" class="headerlink" title="Leetcode18 四数之和"></a>Leetcode18 四数之和</h2><p>四数之和的双指针解法是两层<code>for</code>循环<code>nums[i] + nums[j]</code>为确定值，依然是循环内有<code>left</code>和<code>right</code>下标作为双指针，找出<code>nums[i] + nums[j] + nums[left] + nums[right] == target</code>的情况，三数之和的时间复杂度是<code>O(n^2)</code>，四数之和的时间复杂度是<code>O(n^3)</code> 。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fourSum</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        nums.sort()</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i-<span class="number">1</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">                <span class="keyword">if</span> j &gt; i+<span class="number">1</span> <span class="keyword">and</span> nums[j] == nums[j-<span class="number">1</span>]:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                left = j + <span class="number">1</span></span><br><span class="line">                right = <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">                <span class="keyword">while</span> left &lt; right:</span><br><span class="line">                    <span class="built_in">sum</span> = nums[i] + nums[j] + nums[left] + nums[right]</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">sum</span> &gt; target:</span><br><span class="line">                        right -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">elif</span> <span class="built_in">sum</span> &lt; target:</span><br><span class="line">                        left += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        res.append([nums[i], nums[j], nums[left], nums[right]])</span><br><span class="line">                        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[left] == nums[left+<span class="number">1</span>]:</span><br><span class="line">                            left += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[right] == nums[right-<span class="number">1</span>]:</span><br><span class="line">                            right -= <span class="number">1</span></span><br><span class="line">                        left += <span class="number">1</span></span><br><span class="line">                        right -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Leetcode 刷题</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>哈希表</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode 字符串篇</title>
    <url>/2024/01/18/%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%AF%87/</url>
    <content><![CDATA[<p>Leetcode刷题记录——字符串篇</p>
<span id="more"></span>

<h2 id="Leetcode344-反转字符串"><a href="#Leetcode344-反转字符串" class="headerlink" title="Leetcode344 反转字符串"></a>Leetcode344 反转字符串</h2><p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseString</span>(<span class="params">self, s: <span class="type">List</span>[<span class="built_in">str</span>]</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Do not return anything, modify s in-place instead.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="built_in">len</span>(s) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            <span class="comment"># s[left], s[right] = s[right], s[left]</span></span><br><span class="line">            <span class="comment"># Python 的一项语法糖，它可以在不借助临时变量的情况下直接交换两个变量的值</span></span><br><span class="line">            temp = s[left]</span><br><span class="line">            s[left] = s[right]</span><br><span class="line">            s[right] = temp</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">            right -= <span class="number">1</span></span><br></pre></td></tr></table></figure>

<h2 id="Leetcode541-反转字符串Ⅱ"><a href="#Leetcode541-反转字符串Ⅱ" class="headerlink" title="Leetcode541 反转字符串Ⅱ"></a>Leetcode541 反转字符串Ⅱ</h2><p>每隔<code>2k</code>个字符，反转前<code>k</code>个，后<code>k</code>个不动</p>
<p>当循环结束时，判断剩余字符数量的区间，做不同的反转操作</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseStr</span>(<span class="params">self, s: <span class="built_in">str</span>, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        length = <span class="built_in">len</span>(s)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">while</span> i &lt; length <span class="keyword">and</span> i+<span class="number">2</span>*k &lt;= length:</span><br><span class="line">            temp = s[i:i+k][::-<span class="number">1</span>]</span><br><span class="line">            res.append(temp)</span><br><span class="line">            res.append(s[i+k:i+<span class="number">2</span>*k])</span><br><span class="line">            i = i + <span class="number">2</span>*k</span><br><span class="line">        <span class="keyword">if</span> length - i &lt; k:</span><br><span class="line">            temp = s[i:][::-<span class="number">1</span>]</span><br><span class="line">            res.append(temp)</span><br><span class="line">        <span class="keyword">if</span> length - i &lt; <span class="number">2</span>*k <span class="keyword">and</span> length - i &gt;= k:</span><br><span class="line">            temp = s[i:i+k][::-<span class="number">1</span>]</span><br><span class="line">            res.append(temp)</span><br><span class="line">            res.append(s[i+k:])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(res)</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode151-翻转字符串里的单词"><a href="#Leetcode151-翻转字符串里的单词" class="headerlink" title="Leetcode151 翻转字符串里的单词"></a>Leetcode151 翻转字符串里的单词</h2><p>首先用一个<code>split</code>函数分割字符串</p>
<p>再用双指针，同样的反转字符串的操作</p>
<p>最后用 <code>&#39; &#39;.join()</code>合并就可以了</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseWords(self, s: str) -&gt; str:</span><br><span class="line">        words = s.split()</span><br><span class="line">        left = 0</span><br><span class="line">        right = len(words) - 1</span><br><span class="line">        while left &lt; right:</span><br><span class="line">            words[left], words[right] = words[right], words[left]</span><br><span class="line">            left += 1</span><br><span class="line">            right -= 1</span><br><span class="line">        return &#x27; &#x27;.join(words)</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode28-找出字符串中第一个匹配项的下标"><a href="#Leetcode28-找出字符串中第一个匹配项的下标" class="headerlink" title="Leetcode28 找出字符串中第一个匹配项的下标"></a>Leetcode28 找出字符串中第一个匹配项的下标</h2><p><code>KMP</code>思想</p>
<p>主要是求出根据子串求<code>next</code>数组</p>
<p>讲起来太抽象，可以看代码随想录</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">strStr</span>(<span class="params">self, haystack: <span class="built_in">str</span>, needle: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="built_in">next</span> = [<span class="number">0</span>] * <span class="built_in">len</span>(needle)</span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(needle)):</span><br><span class="line">            <span class="keyword">while</span> j &gt; <span class="number">0</span> <span class="keyword">and</span> needle[i] != needle[j]:</span><br><span class="line">                j = <span class="built_in">next</span>[j-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> needle[i] == needle[j]:</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">            <span class="built_in">next</span>[i] = j</span><br><span class="line">        </span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(haystack)):</span><br><span class="line">            <span class="keyword">while</span> j &gt; <span class="number">0</span> <span class="keyword">and</span> haystack[i] != needle[j]:</span><br><span class="line">                j = <span class="built_in">next</span>[j-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> haystack[i] == needle[j]:</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> j == <span class="built_in">len</span>(needle):</span><br><span class="line">                <span class="keyword">return</span> i-j+<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">        </span><br></pre></td></tr></table></figure>

<h2 id="Leetcode459-重复的子字符串"><a href="#Leetcode459-重复的子字符串" class="headerlink" title="Leetcode459 重复的子字符串"></a>Leetcode459 重复的子字符串</h2><p>如果这个字符串包含重复的子串</p>
<p>假设是字符串<code>abcabc</code>，记为<code>s</code></p>
<p>那么<code>s</code>一定包含在<code>s+s</code>内</p>
<p>所以判断字符串<code>s</code>是否由重复子串组成，只要两个<code>s</code>拼接在一起，里面还出现一个<code>s</code>的话，就说明是由重复子串组成</p>
<p>我们在判断<code> s + s</code>拼接的字符串里是否出现一个<code>s</code>的的时候，<strong>要刨除 s + s 的首字符和尾字符</strong>，这样避免在<code>s+s</code>中搜索出原来的<code>s</code>，我们要搜索的是中间拼接出来的<code>s</code></p>
<p>还是回到了<code>KMP</code>算法</p>
<p>只不过这道题的模式串变成了<code>(s+s)[1:-1]</code><strong>（拼接再去掉首位字符）</strong></p>
<p>子串变成了<code>s</code></p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def repeatedSubstringPattern(self, s: str) -&gt; bool:</span><br><span class="line">        next = [0] * len(s)</span><br><span class="line">        j = 0</span><br><span class="line">        needle = (s+s)[1:-1]</span><br><span class="line">        for i in range(1, len(s)):</span><br><span class="line">            while j &gt; 0 and s[j] != s[i]:</span><br><span class="line">                j = next[j-1]</span><br><span class="line">            if s[j] == s[i]:</span><br><span class="line">                j += 1</span><br><span class="line">            next[i] = j</span><br><span class="line">        </span><br><span class="line">        j = 0</span><br><span class="line">        needle = (s+s)[1:-1]</span><br><span class="line">        for i in range(len(needle)):</span><br><span class="line">            while j &gt; 0 and needle[i] != s[j]:</span><br><span class="line">                j = next[j-1]</span><br><span class="line">            if needle[i] == s[j]:</span><br><span class="line">                j += 1</span><br><span class="line">            if j == len(s):</span><br><span class="line">                return True</span><br><span class="line">        return False</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Leetcode 刷题</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>字符串</tag>
      </tags>
  </entry>
  <entry>
    <title>Parameter Efficient Fine-Tuning(PEFT)系列论文总结(三)</title>
    <url>/2023/12/29/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%B8%89)/</url>
    <content><![CDATA[<p>承接上篇Parameter Efficient Fine-Tuning(PEFT)系列论文总结(二)，本篇主要介绍LoRA及其各种变体的微调方法。</p>
<span id="more"></span>

<h2 id="一、LoRA"><a href="#一、LoRA" class="headerlink" title="一、LoRA"></a>一、LoRA</h2><p>研究表明，语言模型针对特定任务微调之后，权重矩阵通常具有很低的本征秩 <code>（Intrinsic Rank）</code>。研究人员认为参数更新量即便投影到较小的子空间中，也不会影响学习的有效性。因此，提出固定预训练模型参数不变，在原本权重矩阵旁路添加低秩矩阵的乘积作为可训练参数，用以模拟参数的变化量。具体来说，假设预训练权重为${w_0\ \epsilon \ \mathbb{R}^{d<em>k}}$，可训练参数为${\varDelta W\ &#x3D;\ BA}$，其中${B\ \epsilon \ \mathbb{R}^{d</em>r} }$，${A\ \epsilon \ \mathbb{R}^{r*d}}$，初始化时，矩阵 ${A}$ 通过高斯函数初始化，矩阵${B}$ 为零初始化，使得训练开始之前旁路对原模型不造成影响，即参数改变量为 0。对于该权重的输入 ${x}$ 来说，输出为式${h\ &#x3D;\ W_0x+∆W\ x\ &#x3D;W_0x+BAx}$，<code>LoRA</code>算法结构方法如图：</p>
<img src="/2023/12/29/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%B8%89)/image-20240110192331072.png" alt="image-20240110192331072" style="zoom:80%;">

<p>简言之，<code>LoRA</code>的核心思想是用一种低秩的方式来调整这些参数矩阵。在数学上，低秩意味着一个矩阵可以用两个较小的矩阵相乘来近似。</p>
<p><strong>LoRA实现步骤如下：</strong></p>
<p>1、选择目标层</p>
<p>首先，在预训练神经网络模型中选择要应用<code>LoRA</code>的目标层。这些层通常是与特定任务相关的，如自注意力机制中的查询<code>Q</code>和键<code>K</code>矩阵</p>
<p>值得注意的是，原则上，可以将<code>LoRA</code>应用于神经网络中权矩阵的任何子集，以减少可训练参数的数量。在<code>Transformer</code>体系结构中，<code>Self-Attetion</code>模块(<code>Wq</code>、<code>Wk</code>、<code>Wv</code>、<code>Wo</code>)中有四个权重矩阵，<code>MLP</code>模块中有两个权重矩阵。我们将<code>Wq</code>(或<code>Wk</code>，<code>Wv</code>)作为维度的单个矩阵，尽管输出维度通常被切分为注意力头。<br><em>(In principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce th enumber of trainable parameters. In the Transformer architecture, there are four weight matrices in the self-attention module (Wq, Wk, Wv, Wo) and two in the MLP module. We treat Wq (or Wk, Wv)as a single matrix of dimension , even though the output dimension is usually sliced into attention heads)</em></p>
<p>不过，为了简单和参数效率，将研究限制为仅适应下游任务的注意力权重，并冻结<code>MLP</code>模块(因此它们不接受下游任务的训练)</p>
<p>(We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency)<br>2、初始化映射矩阵和逆映射矩阵</p>
<p>为目标层创建两个较小的矩阵<code>A</code>和<code>B</code>，然后进行变换</p>
<p><code>A</code>是映射矩阵(一般用随机高斯分布初始化，维度上是降维）</p>
<p><code>B</code>是逆映射矩阵(用0矩阵初始化)，维度上是升维</p>
<p>之后做参数变换：将目标层的原始参数矩阵W通过映射矩阵<code>A</code>和逆映射矩阵<code>B</code>进行变换，计算公式为：<code>W&#39; = W + A * B</code>，这里<code>W&#39;</code>是变换后的参数矩阵</p>
<p>3、微调模型</p>
<p>使用新的参数矩阵<code>W&#39;</code>替换目标层的原始参数矩阵<code>W</code>，然后在特定任务的训练数据上对模型进行微调</p>
<p>4、梯度更新</p>
<p>在微调过程中，计算损失函数关于映射矩阵<code>A</code>和逆映射矩阵<code>B</code>的梯度，并使用优化算法(如<code>Adam</code>、<code>SGD</code>等)对<code>A</code>和<code>B</code>进行更新<br>注意，在更新过程中，原始参数矩阵<code>W</code>保持不变</p>
<p>说白了，<strong>训练的时候固定原始PLM的参数，只训练降维矩阵A与升维矩阵B</strong></p>
<p>且当需要切换到另一个下游任务时，可以通过减去<code>BA</code>然后添加不同的<code>B&#39;A&#39;</code>来恢复<code>W</code>，这是一个内存开销很小的快速操作*(When we need to switch to another downstream task, we can recover W0 by subtracting BA and then adding a different B0A0, a quick operation with very little memory overhead )*</p>
<h2 id="二、AdaLoRA"><a href="#二、AdaLoRA" class="headerlink" title="二、AdaLoRA"></a>二、AdaLoRA</h2><p>从上文介绍的<code>LoRA</code>可以看出它的局限性：在一个模型的所有使用适配器的模块都使用了同一个<code>r</code>，但是无论是不同深度的参数，还是同一个深度不同模块的参数，它们在模型中的重要性都是不同的。例如下图的这两个例子，作者通过只对特定的模块进行微调，得出了不同参数的重要性的可视化结果。从中我们可以看出，自注意机制的全连接层要比计算 <code>Wq</code>，<code>Wk</code> ，<code>Wv</code>的权值重要，而更深层的参数要比更浅层的参数重要。</p>
<img src="/2023/12/29/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%B8%89)/image-20240110202455667.png" alt="image-20240110202455667" style="zoom:80%;">

<p><strong>AdaLoRA的动机</strong></p>
<p>因为在一个模型中，不同模块拥有着不同的贡献，那么在使用<code>LoRA</code>时如果我们能够根据它们重要性的不同为不同的模块分配不同的秩，那么将会带来很多好处。首先，我们为重要性更低的模块分配更小的秩，那么将有效的减少模型的计算量。其次，如果我们能够为更重要的特征分配更大的秩，那么将能够更有效的捕捉特征的细节信息。</p>
<p>具体的过程数学要求有点高，先不看了</p>
<p> <strong>总结</strong></p>
<p><code>AdaLoRA</code>通过将所有添加了适配器的模块的秩的值看做了一组超参，然后通过模型剪枝的思想对<code>LoRA</code>的秩进行了自适应的计算。同时为了剪枝后模型效果的稳定，<code>AdaLoRA</code>使用<code>SVD</code>的三元组替代了<code>LoRA</code>的二元组，充分利用了<code>SVD</code>奇异矩阵的正交性和奇异向量的绝对值和特征重要性的相关性设计剪枝策略。</p>
<h2 id="三、QLoRA"><a href="#三、QLoRA" class="headerlink" title="三、QLoRA"></a>三、QLoRA</h2><p>讲<code>QLoRA</code>之前，首先得明白什么是<strong>模型量化</strong></p>
<p><strong>模型量化</strong>（Quantization）也被叫做模型的低精度表示，指的是在不大幅降低模型效果的前提下使用更低的精度来表示模型中的参数，从而缩减模型的体积和训练模型时占用的显存。量化的本质是函数映射，根据量化过程是否线性我们可以把量化分为<strong>线性量化</strong>和<strong>非线性量化</strong>。</p>
<p>模型量化的核心工作就是在尽量保证模型准确率的前提下优化模型的推理速度和模型体积。</p>
<p>与量化对应的是反量化（<code>Dequantization</code>），反量化指的是将模型的低精度恢复为高精度的过程，主要用于减少量化造成的精度损失</p>
<p>简单来讲，模型量化是将浮点数值转化为定点数值，同时尽可能减少计算精度损失的方法。<br>模型量化是一种压缩网络参数的方式，它将神经网络的参数(<code>weight</code>)、激活值(<code>activation</code>)等原本用浮点表示的量值换用定点(整型)表示，在计算过程中，再将定点数据反量化回浮点数据，得到结果。</p>
<p>我们可以对模型参数(<code>weight</code>)、激活值(<code>activation</code>)或者梯度(<code>gradient</code>)做量化。通常而言，模型的参数分布较为稳定，因此对参数 <code>weight</code> 做量化较为容易(比如，<code>QLoRA</code>便是对<code>weight</code>做量化)<br>至于模型的激活值往往存在异常值，直接对其做量化，会降低有效的量化格点数，导致精度损失严重，因此，激活值的量化需要更复杂的处理方法(如<code>SmoothQuant</code>)</p>
<img src="/2023/12/29/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%B8%89)/image-20240112135538529.png" alt="image-20240112135538529" style="zoom:80%;">



<p><strong>QLoRA的工作有三个</strong></p>
<p>1、结合了分位数量化和分块量化的<strong>4位标准浮点数量化</strong>（4-bit NormalFloat Quantization）</p>
<p>2、对模型进行两次量化的<strong>双重量化</strong>（Double Quantization），它的第二次量化只作用在第一次量化产生的量化常数上，可以进一步节约显存占用</p>
<p>3、<strong>分页优化</strong>（Paged Optimizer），使用<code>CPU</code>内存代替<code>GPU</code>显存保存部分梯度参数</p>
<p>留到之后理解更深了再更新</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode 数组篇</title>
    <url>/2024/01/06/%E6%95%B0%E7%BB%84%E7%AF%87/</url>
    <content><![CDATA[<p>Leetcode刷题记录——数组篇</p>
<span id="more"></span>

<h2 id="Leetcode704-二分查找"><a href="#Leetcode704-二分查找" class="headerlink" title="Leetcode704  二分查找"></a>Leetcode704  二分查找</h2><p><strong>使用二分查找的前提条件：</strong></p>
<ul>
<li>数组有序</li>
<li>数组内无重复元素（因为一旦有重复元素，使用二分查找法返回的元素下标可能不是唯一的）</li>
</ul>
<p>二分查找涉及的很多的边界条件，逻辑比较简单，但就是写不好。例如到底是 <code>while(left &lt; right)</code> 还是 <code>while(left &lt;= right)</code>，到底是<code>right = middle</code>呢，还是要<code>right = middle - 1</code>？</p>
<p>因此要遵循<strong>循环不变量原则</strong>，就是在while寻找中每一次边界的处理都要坚持根据区间的定义来操作</p>
<p>区间的定义一般为两种，左闭右闭即<code>[left, right]</code>，或者左闭右开即<code>[left, right)</code>，本题采用左闭右闭的写法</p>
<p>区间的定义这就决定了二分法的代码应该如何写，<strong>因为定义target在[left, right]区间，所以有如下两点：</strong></p>
<ul>
<li><code>while (left &lt;= right)</code> 要使用 <code>&lt;=</code> ，因为<code>left == right</code>是有意义的，在<code>[left, right]</code>区间是合法的，所以使用 <code>&lt;=</code></li>
<li><code>if (nums[middle] &gt; target)</code> <code>right</code> 要赋值为 <code>middle - 1</code>，因为当前这个<code>nums[middle]</code>一定不是<code>target</code>，那么接下来要查找的左区间结束下标位置就是 <code>middle - 1</code></li>
</ul>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">            middle = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> target &lt; nums[middle]:</span><br><span class="line">                right = middle -<span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> target &gt; nums[middle]:</span><br><span class="line">                left = middle + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> middle</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>





<h2 id="Leetcode27-移除元素"><a href="#Leetcode27-移除元素" class="headerlink" title="Leetcode27  移除元素"></a>Leetcode27  移除元素</h2><p>双指针法（快慢指针法）： <strong>通过一个快指针和慢指针在一个for循环下完成两个for循环的工作。</strong></p>
<p>定义快慢指针</p>
<ul>
<li>快指针：寻找新数组的元素 ，新数组就是不含有目标元素的数组</li>
<li>慢指针：指向更新 新数组下标的位置</li>
</ul>
<p>也就是快指针去寻找符合条件的元素，慢指针用来更新需要返回的数组，这样在一次遍历中，即可完成数组的更新</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeElement</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], val: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        slow = <span class="number">0</span></span><br><span class="line">        fast = <span class="number">0</span></span><br><span class="line">        length = <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> fast &lt; length:</span><br><span class="line">            <span class="keyword">if</span> nums[fast] != val:</span><br><span class="line">                nums[slow] = nums[fast]</span><br><span class="line">                slow = slow + <span class="number">1</span></span><br><span class="line">            fast = fast + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> slow</span><br></pre></td></tr></table></figure>





<h2 id="Leetcode977-有序数组的平方"><a href="#Leetcode977-有序数组的平方" class="headerlink" title="Leetcode977 有序数组的平方"></a>Leetcode977 有序数组的平方</h2><p>数组其实是有序的， 只不过负数平方之后可能成为最大数了。</p>
<p>那么数组平方的最大值就在数组的两端，不是最左边就是最右边，不可能是中间。</p>
<p>此时可以考虑双指针法了，<code>head</code>指向起始位置，<code>tail</code>指向终止位置。</p>
<p>定义一个新数组<code>res</code>，和<code>nums</code>数组一样的大小，让<code>index</code>指向<code>res</code>数组终止位置。</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sortedSquares</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        length = <span class="built_in">len</span>(nums)</span><br><span class="line">        head = <span class="number">0</span></span><br><span class="line">        tail = length - <span class="number">1</span></span><br><span class="line">        index = length - <span class="number">1</span></span><br><span class="line">        res = [<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)] * <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> head &lt;= tail:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">abs</span>(nums[head]) &gt; <span class="built_in">abs</span>(nums[tail]):</span><br><span class="line">                res[index] = nums[head] * nums[head]</span><br><span class="line">                index -= <span class="number">1</span></span><br><span class="line">                head += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res[index] = nums[tail] * nums[tail]</span><br><span class="line">                index -= <span class="number">1</span></span><br><span class="line">                tail -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>





<h2 id="Leetcode209-长度最小的子数组"><a href="#Leetcode209-长度最小的子数组" class="headerlink" title="Leetcode209 长度最小的子数组"></a>Leetcode209 长度最小的子数组</h2><p>首先想到的暴力法，思路很简单，两个<code>for</code>循环，找到所有的连续子组合，如果该组合<code>&gt;=target</code>则记录长度，最终返回长度最小的记录即可</p>
<p>这道题可以用<strong>滑动窗口</strong>的思想来解决：</p>
<p>在暴力解法中，是一个<code>for</code>循环为滑动窗口的起始位置，一个<code>for</code>循环为滑动窗口的终止位置，用两个<code>for</code>循环 完成了一个不断搜索区间的过程</p>
<p>那么滑动窗口如何用一个<code>for</code>循环来完成这个操作呢？</p>
<p>首先要思考，如果用一个<code>for</code>循环，那么应该表示滑动窗口的起始位置，还是终止位置？</p>
<p>如果只用一个<code>for</code>循环来表示滑动窗口的起始位置，那么如何遍历剩下的终止位置？</p>
<p>此时难免再次陷入暴力解法的怪圈</p>
<p>所以 如果只用一个<code>for</code>循环，那么这个循环的索引，一定是表示滑动窗口的终止位置</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minSubArrayLen</span>(<span class="params">self, target: <span class="built_in">int</span>, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line">        end = <span class="number">0</span></span><br><span class="line">        length = <span class="built_in">len</span>(nums)</span><br><span class="line">        max_len = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> end <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">            <span class="built_in">sum</span> = <span class="built_in">sum</span> + nums[end]</span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">sum</span> &gt;= target:</span><br><span class="line">                <span class="comment"># 如果此时窗口内的和&gt;=target,那么试着剔除窗口的第一个元素，看是否还成立</span></span><br><span class="line">                <span class="comment"># 但操作前,需要先记录此时成立情况下的窗口长度</span></span><br><span class="line">                res = end - start + <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> res &lt; max_len:</span><br><span class="line">                    max_len = res</span><br><span class="line">                <span class="built_in">sum</span> = <span class="built_in">sum</span> - nums[start]</span><br><span class="line">                start += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> max_len == inf:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> max_len</span><br></pre></td></tr></table></figure>





<h2 id="Leetcode59-螺旋矩阵II"><a href="#Leetcode59-螺旋矩阵II" class="headerlink" title="Leetcode59 螺旋矩阵II"></a>Leetcode59 螺旋矩阵II</h2><p>这道题考察对代码的掌控能力，仍然会遇到复杂的边界问题，坚持<strong>循环不变量原则</strong></p>
<p>模拟顺时针画矩阵的过程:</p>
<ul>
<li>填充上行从左到右</li>
<li>填充右列从上到下</li>
<li>填充下行从右到左</li>
<li>填充左列从下到上</li>
</ul>
<p>要处理好边界问题，就要坚持<strong>循环不变量原则</strong></p>
<p>本题采用<strong>左闭右开</strong>的写法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generateMatrix</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="comment"># 先创建 n * n 的全0矩阵</span></span><br><span class="line">        nums = [[<span class="number">0</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        x = <span class="number">0</span></span><br><span class="line">        y = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 偶数n能走完整圈,奇数n会留中间一个空</span></span><br><span class="line">        loop = n // <span class="number">2</span></span><br><span class="line">        <span class="comment"># n为奇数时, 矩阵的中间位置为nums[mid][mid]</span></span><br><span class="line">        mid = n // <span class="number">2</span></span><br><span class="line">        <span class="comment"># 用来给矩阵中每一个空格赋值</span></span><br><span class="line">        count = <span class="number">1</span></span><br><span class="line">        offset = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> loop:</span><br><span class="line">            <span class="comment"># 从左到右</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x, n - offset):</span><br><span class="line">                nums[x][i] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 从上到下</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(y, n - offset):</span><br><span class="line">                nums[i][n - offset] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 从右到左</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - offset, x, -<span class="number">1</span>):</span><br><span class="line">                nums[n - offset][i] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 从下到上</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - offset, y, -<span class="number">1</span>):</span><br><span class="line">                nums[i][y] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            x += <span class="number">1</span></span><br><span class="line">            y += <span class="number">1</span></span><br><span class="line">            offset += <span class="number">1</span></span><br><span class="line">            loop -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n % <span class="number">2</span> != <span class="number">0</span>:</span><br><span class="line">            nums[mid][mid] = count</span><br><span class="line">        <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Leetcode 刷题</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title>Leetcode 链表篇</title>
    <url>/2024/01/10/%E9%93%BE%E8%A1%A8%E7%AF%87/</url>
    <content><![CDATA[<p>Leetcode刷题记录——链表篇</p>
<span id="more"></span>

<h2 id="Leetcode203-移除链表"><a href="#Leetcode203-移除链表" class="headerlink" title="Leetcode203 移除链表"></a>Leetcode203 移除链表</h2><p>如果对于一个链表，头节点的操作和其他节点的操作需要2个逻辑的话，建立一个虚拟头节点往往会大大降低难度</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeElements</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode], val: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        dummy_head = ListNode(<span class="built_in">next</span> = head)</span><br><span class="line">        current = dummy_head</span><br><span class="line">        <span class="keyword">while</span> current.<span class="built_in">next</span>:</span><br><span class="line">            <span class="keyword">if</span> current.<span class="built_in">next</span>.val == val:</span><br><span class="line">                current.<span class="built_in">next</span> = current.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                current = current.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> dummy_head.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>



<h2 id="Leetcode707-设计链表"><a href="#Leetcode707-设计链表" class="headerlink" title="Leetcode707 设计链表"></a>Leetcode707 设计链表</h2><p>特定位置插入删除的操作，要考虑好头尾结点的特殊情况就行</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ListNode</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val=<span class="number">0</span>, <span class="built_in">next</span>=<span class="literal">None</span></span>):</span><br><span class="line">        self.val = val</span><br><span class="line">        self.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinkedList</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.dummy_node = ListNode()</span><br><span class="line">        self.size = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, index: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> index &lt; <span class="number">0</span> <span class="keyword">or</span> index &gt;= self.size:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">        current = self.dummy_node.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(index):</span><br><span class="line">            current = current.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> current.val</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addAtHead</span>(<span class="params">self, val: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        new_node = ListNode(val, <span class="literal">None</span>)</span><br><span class="line">        new_node.<span class="built_in">next</span> = self.dummy_node.<span class="built_in">next</span></span><br><span class="line">        self.dummy_node.<span class="built_in">next</span> = new_node</span><br><span class="line">        self.size += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addAtTail</span>(<span class="params">self, val: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        new_node = ListNode(val, <span class="literal">None</span>)</span><br><span class="line">        current = self.dummy_node</span><br><span class="line">        <span class="keyword">while</span> current.<span class="built_in">next</span>:</span><br><span class="line">            current = current.<span class="built_in">next</span></span><br><span class="line">        current.<span class="built_in">next</span> = new_node</span><br><span class="line">        self.size += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addAtIndex</span>(<span class="params">self, index: <span class="built_in">int</span>, val: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> index &lt; <span class="number">0</span> <span class="keyword">or</span> index &gt; self.size:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        new_node = ListNode(val, <span class="literal">None</span>)</span><br><span class="line">        current = self.dummy_node</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(index):</span><br><span class="line">            current = current.<span class="built_in">next</span></span><br><span class="line">        new_node.<span class="built_in">next</span> = current.<span class="built_in">next</span></span><br><span class="line">        current.<span class="built_in">next</span> = new_node</span><br><span class="line">        self.size += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">deleteAtIndex</span>(<span class="params">self, index: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> index &lt; <span class="number">0</span> <span class="keyword">or</span> index &gt;= self.size:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        current = self.dummy_node</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(index):</span><br><span class="line">            current = current.<span class="built_in">next</span></span><br><span class="line">        current.<span class="built_in">next</span> = current.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">        self.size -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Your MyLinkedList object will be instantiated and called as such:</span></span><br><span class="line"><span class="comment"># obj = MyLinkedList()</span></span><br><span class="line"><span class="comment"># param_1 = obj.get(index)</span></span><br><span class="line"><span class="comment"># obj.addAtHead(val)</span></span><br><span class="line"><span class="comment"># obj.addAtTail(val)</span></span><br><span class="line"><span class="comment"># obj.addAtIndex(index,val)</span></span><br><span class="line"><span class="comment"># obj.deleteAtIndex(index)</span></span><br></pre></td></tr></table></figure>



<h2 id="Leetcode206-反转链表"><a href="#Leetcode206-反转链表" class="headerlink" title="Leetcode206 反转链表"></a>Leetcode206 反转链表</h2><p>首先定义一个<code>current</code>指针，指向头结点，<code>pre</code>指向<code>None</code></p>
<p>从头开始反转时，需要先定义一个<code>temp</code>指针保留<code>current.next</code></p>
<p>接着将<code>current.next</code>指向<code>pre</code>，再移动<code>pre</code>到<code>current</code>的位置，<code>current</code>再移到<code>temp</code>的位置</p>
<p>这是一次反转的操作</p>
<p>只要当前<code>current</code>非空，就可以一直进行</p>
<p>最后返回的<code>pre</code>就是头节点</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverseList</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        pre = <span class="literal">None</span></span><br><span class="line">        current = head</span><br><span class="line">        <span class="keyword">while</span> current:</span><br><span class="line">            temp = current.<span class="built_in">next</span></span><br><span class="line">            current.<span class="built_in">next</span> = pre</span><br><span class="line">            pre = current</span><br><span class="line">            current = temp</span><br><span class="line">        <span class="keyword">return</span> pre</span><br></pre></td></tr></table></figure>





<h2 id="Leetcode24-两两交换链表中的节点"><a href="#Leetcode24-两两交换链表中的节点" class="headerlink" title="Leetcode24 两两交换链表中的节点"></a>Leetcode24 两两交换链表中的节点</h2><p>卡了好久没找出错误，思维只定在了第一轮，还得是<code>gpt</code></p>
<p>问题在于你每次交换节点时，都将 <code>dummy_node.next</code> 指向了当前的 <code>temp</code> 节点，这样会导致 <code>dummy_node.next</code> 不断指向链表中的同一个节点，而不是每次都更新。在进行两两交换时，<code>dummy_node.next</code> 应该指向新的头节点，而不是固定地指向 <code>temp</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">swapPairs</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        <span class="keyword">if</span> head <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> head.<span class="built_in">next</span> <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line">        dummy_node = ListNode(<span class="built_in">next</span>=head)</span><br><span class="line">        cur = head</span><br><span class="line">        <span class="keyword">while</span> cur <span class="keyword">and</span> cur.<span class="built_in">next</span>:</span><br><span class="line">            <span class="comment"># next_node = cur.next</span></span><br><span class="line">            temp = cur.<span class="built_in">next</span></span><br><span class="line">            next_head = cur.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">            dummy_node.<span class="built_in">next</span> = temp</span><br><span class="line">            temp.<span class="built_in">next</span> = cur</span><br><span class="line">            cur.<span class="built_in">next</span> = next_head</span><br><span class="line">            cur= next_head</span><br><span class="line">        <span class="keyword">return</span> dummy_node.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>

<p>画图模拟过程就行</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">swapPairs</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        <span class="keyword">if</span> head <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> head.<span class="built_in">next</span> <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line">        dummy_node = ListNode(<span class="built_in">next</span>=head)</span><br><span class="line">        cur = dummy_node</span><br><span class="line">        <span class="keyword">while</span> cur.<span class="built_in">next</span> <span class="keyword">and</span> cur.<span class="built_in">next</span>.<span class="built_in">next</span>:</span><br><span class="line">            next_head = cur.<span class="built_in">next</span>.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">            temp = cur.<span class="built_in">next</span></span><br><span class="line">            cur.<span class="built_in">next</span> = temp.<span class="built_in">next</span></span><br><span class="line">            cur.<span class="built_in">next</span>.<span class="built_in">next</span> = temp</span><br><span class="line">            temp.<span class="built_in">next</span> = next_head</span><br><span class="line">            cur = cur.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> dummy_node.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>

<p>另有递归的写法，留到下次再回来做的时候想</p>
<h2 id="Leetcode19-删除链表的倒数第n个节点"><a href="#Leetcode19-删除链表的倒数第n个节点" class="headerlink" title="Leetcode19 删除链表的倒数第n个节点"></a>Leetcode19 删除链表的倒数第n个节点</h2><p>双指针的经典应用</p>
<p>如果要删除倒数第<code>n</code>个节点，让<code>fast</code>移动<code>n</code>步，然后让<code>fast</code>和<code>slow</code>同时移动，直到<code>fast</code>指向链表末尾。</p>
<p>删掉<code>slow.next</code>所指向的节点就可以了。</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, next=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.next = next</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeNthFromEnd</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode], n: <span class="built_in">int</span></span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        dummy_node = ListNode(<span class="built_in">next</span>=head)</span><br><span class="line">        <span class="comment"># 创建两个指针，慢指针和快指针，并将它们初始化为虚拟节点</span></span><br><span class="line">        fast = dummy_node</span><br><span class="line">        slow = dummy_node</span><br><span class="line">        <span class="comment"># 快指针比慢指针快 n 步</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            fast = fast.<span class="built_in">next</span></span><br><span class="line">        <span class="comment"># 当快指针走到链表尾节点时,slow刚好在要删除的那个节点的前一个节点,slow.next = slow.next.next就可以达到删除的目的了</span></span><br><span class="line">        <span class="keyword">while</span> fast.<span class="built_in">next</span>:</span><br><span class="line">            fast = fast.<span class="built_in">next</span></span><br><span class="line">            slow = slow.<span class="built_in">next</span></span><br><span class="line">        slow.<span class="built_in">next</span> = slow.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> dummy_node.<span class="built_in">next</span></span><br></pre></td></tr></table></figure>



<h2 id="Leetcode160-链表相交"><a href="#Leetcode160-链表相交" class="headerlink" title="Leetcode160 链表相交"></a>Leetcode160 链表相交</h2><p>两链表若相交，则相交之后的长度是相同的，相交前的长度不同</p>
<p>首先遍历两链表，分别求出它们的长度<code>lenA</code>，<code> lenB</code></p>
<p>再求出它们的长度差<code>dif</code>，先让长的那个链表移动<code>dif</code>步，则两链表就在同一起跑线了</p>
<p>接着遍历到尾节点的同时判断<code>curA == curB</code></p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getIntersectionNode</span>(<span class="params">self, headA: ListNode, headB: ListNode</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        lenA = <span class="number">0</span> </span><br><span class="line">        lenB = <span class="number">0</span></span><br><span class="line">        curA = headA</span><br><span class="line">        curB = headB</span><br><span class="line">        <span class="keyword">while</span> curA:</span><br><span class="line">            curA = curA.<span class="built_in">next</span></span><br><span class="line">            lenA += <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> curB:</span><br><span class="line">            curB = curB.<span class="built_in">next</span></span><br><span class="line">            lenB += <span class="number">1</span></span><br><span class="line">        curA = headA</span><br><span class="line">        curB = headB</span><br><span class="line">        <span class="keyword">if</span> lenA &gt; lenB:</span><br><span class="line">            dif = lenA -lenB</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(dif):</span><br><span class="line">                curA = curA.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dif = lenB - lenA</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(dif):</span><br><span class="line">                curB = curB.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">while</span> curA:</span><br><span class="line">            <span class="keyword">if</span> curA == curB:</span><br><span class="line">                <span class="keyword">return</span> curA</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                curA = curA.<span class="built_in">next</span></span><br><span class="line">                curB = curB.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>



<h2 id="Leetcode142-环形链表Ⅱ"><a href="#Leetcode142-环形链表Ⅱ" class="headerlink" title="Leetcode142 环形链表Ⅱ"></a>Leetcode142 环形链表Ⅱ</h2><ul>
<li>首先判断链表中是否有环</li>
</ul>
<p>定义<code>slow</code>、<code>fast</code>指针，首先指向<code>head</code></p>
<p><code>slow</code>一次移动一步，<code>fast</code>一次移动两步</p>
<p><strong>若链表有环</strong>，<code>fast</code>一定比<code>slow</code>先进环</p>
<p>等到<code>slow</code>和<code>fast</code>都进环后，由于<code>fast</code>比<code>slow</code>每次多移动一步，并且在环内，这就变成了一个追及问题</p>
<p>也就是说，不论这个环多大，<code>fast</code>总能追上<code>slow</code></p>
<ul>
<li>然后需要找到环的入口</li>
</ul>
<p>假设从头结点到环形入口节点 的节点数为<code>x</code>。 环形入口节点到 <code>fast</code>指针与<code>slow</code>指针相遇节点 节点数为<code>y</code>。 从相遇节点再到环形入口节点节点数为 <code>z</code></p>
<p>相遇时：<code> slow</code>指针走过的节点数为: <code>x + y</code>， fast指针走过的节点数：<code>x + y + n (y + z)</code>，<code>n</code>为<code>fast</code>指针在环内走了<code>n</code>圈才遇到<code>slow</code>指针， <code>（y+z）</code>为 一圈内节点的个数<code>A</code></p>
<p>因为<code>fast</code>指针是一步走两个节点，<code>slow</code>指针一步走一个节点， 所以 <code>fast</code>指针走过的节点数 &#x3D; <code>slow</code>指针走过的节点数 * 2：</p>
<p><code>(x + y) * 2 = x + y + n (y + z)</code></p>
<p>两边消掉一个（x+y）: <code>x + y = n (y + z)</code></p>
<p>因为要找环形的入口，那么要求的是<code>x</code>，因为<code>x</code>表示头结点到 环形入口节点的的距离</p>
<p>所以要求<code>x</code> ，将<code>x</code>单独放在左面：<code>x = n (y + z) - y</code> </p>
<p>再从<code>n(y+z)</code>中提出一个<code>（y+z）</code>来，整理公式之后为如下公式：<code>x = (n - 1) (y + z) + z</code> 注意这里<code>n</code>一定是大于等于1的，因为 <code>fast</code>指针至少要多走一圈才能相遇<code>slow</code>指针</p>
<p>这个公式说明什么呢？</p>
<p>先拿<code>n为1</code>的情况来举例，意味着<code>fast</code>指针在环形里转了一圈之后，就遇到了<code>slow</code>指针了。</p>
<p>当 <code>n为1</code>的时候，公式就化解为 <code>x = z</code>，</p>
<p>这就意味着，<strong>从头结点出发一个指针，从相遇节点 也出发一个指针，这两个指针每次只走一个节点， 那么当这两个指针相遇的时候就是 环形入口的节点</strong>。</p>
<p>也就是在相遇节点处，定义一个指针<code>index1</code>，在头结点处定一个指针<code>index2</code></p>
<p>让<code>index1</code>和<code>index2</code>同时移动，每次移动一个节点， 那么他们相遇的地方就是 环形入口的节点</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">detectCycle</span>(<span class="params">self, head: <span class="type">Optional</span>[ListNode]</span>) -&gt; <span class="type">Optional</span>[ListNode]:</span><br><span class="line">        slow = head</span><br><span class="line">        fast = head</span><br><span class="line">        <span class="comment"># 为什么不写 while fast.next and fast.next.next</span></span><br><span class="line">        <span class="comment"># 会出现头结点为[]时, 会报错Nonetype has attribute next, null没有next了</span></span><br><span class="line">        <span class="comment"># ps:而且判断条件是按顺序来的, 写在前面的先判断, 如果前面错了就不往后判断了</span></span><br><span class="line">        <span class="keyword">while</span> fast <span class="keyword">and</span> fast.<span class="built_in">next</span>:</span><br><span class="line">            slow = slow.<span class="built_in">next</span></span><br><span class="line">            fast = fast.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">if</span> slow == fast:</span><br><span class="line">                slow = head</span><br><span class="line">                <span class="keyword">while</span> slow != fast:</span><br><span class="line">                    slow = slow.<span class="built_in">next</span></span><br><span class="line">                    fast = fast.<span class="built_in">next</span></span><br><span class="line">                <span class="keyword">return</span> slow</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Leetcode 刷题</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>链表</tag>
      </tags>
  </entry>
</search>
