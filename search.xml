<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hexo + Typora + Github 博客搭建</title>
    <url>/2023/12/13/Hexo%20+%20Typora%20+%20Github%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<p>Hexo 是一个高效简洁的静态博客框架，支持 Markdown 写作语法，插件丰富，主题优雅，部署方便。目前已成为多数人博客建站的选择，本博客采用Hexo搭建，Markdown编辑软件为Typora并且部署在Github Page上。</p>
<span id="more"></span>

<h1 id="1、Hexo-环境准备"><a href="#1、Hexo-环境准备" class="headerlink" title="1、Hexo 环境准备"></a>1、Hexo 环境准备</h1><p><strong>Hexo 依赖于 <a href="https://nodejs.org/zh-cn/">Node.js</a> 和 <a href="https://git-scm.com/download/">git</a>，所以在安装 Hexo 之前先确保已安装了这两项应用。本教程不再赘述这两项应用的安装教程，可自行查看网上的其他教程</strong></p>
<p>在命令行中通过 npm 来安装 Hexo：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure>

<p><code>-g</code> 表示全局安装，会将 Hexo 命令加入环境变量中，以使其在 cmd 下有效。</p>
<p>新建博客目录，然后在该路径下执行初始化命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo init</span><br></pre></td></tr></table></figure>

<p>执行完毕后，将会生成以下文件结构：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── node_modules       //依赖安装目录</span><br><span class="line">├── scaffolds          //模板文件夹，新建的文章将会从此目录下的文件中继承格式</span><br><span class="line">|   ├── draft.md         //草稿模板</span><br><span class="line">|   ├── page.md          //页面模板</span><br><span class="line">|   └── post.md          //文章模板</span><br><span class="line">├── source             //资源文件夹，用于放置图片、数据、文章等资源</span><br><span class="line">|   └── _posts           //文章目录</span><br><span class="line">├── themes             //主题文件夹</span><br><span class="line">|   └── landscape        //默认主题</span><br><span class="line">├── .gitignore         //指定不纳入git版本控制的文件</span><br><span class="line">├── _config.yml        //站点配置文件</span><br><span class="line">├── db.json            </span><br><span class="line">├── package.json</span><br><span class="line">└── package-lock.json</span><br></pre></td></tr></table></figure>

<p>在根目录下执行如下命令启动 hexo 的内置 Web 服务器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>该命令将会调用 Markdown 引擎解析项目中的博客内容生成网页资源，资源将会存于内存中，所以用户执行完命令之后在项目文件夹中是找不到相关的 Web 资源目录的。该命令还会启动一个简易的 Web 服务器用于提供对内存中网页资源的访问（工作机制类似于 webpack-dev-server），Web 服务器默认监听 4000 端口，用户可在浏览器中通过地址 <code>localhost:4000</code> 访问博客。</p>
<p>此外，可以通过添加命令行参数来支持高级用法：</p>
<ul>
<li>当 4000 端口已被其他应用占用时，可以添加 <code>-p</code> &#x2F; <code>--port</code> 参数来设置 Web 服务监听的端口号，如<code>hexo s -p 8000</code></li>
</ul>
<h1 id="2、Hexo-新建文章"><a href="#2、Hexo-新建文章" class="headerlink" title="2、Hexo 新建文章"></a>2、Hexo 新建文章</h1><p>1、在Hexo根目录下，使用<code>git</code>输入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo new &quot;New&quot;</span><br></pre></td></tr></table></figure>

<p>就创建了一篇新的博文，博文名字为<code>New</code></p>
<p>2、在Hexo目录下的<code>source</code>&#x2F;<code>_posts</code>文件夹内，就会出现刚刚创建的博文：<code>New.md</code></p>
<p><strong>注：对于已经写好的.md文件，可以直接复制到<code>source</code>&#x2F;<code>_posts</code>文件夹内</strong></p>
<h1 id="3、Hexo-博客引擎编译"><a href="#3、Hexo-博客引擎编译" class="headerlink" title="3、Hexo 博客引擎编译"></a>3、Hexo 博客引擎编译</h1><p>在写完博文后（<code>.md</code>文件），回到Hexo根目录，使用<code>git</code>输入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo g</span><br></pre></td></tr></table></figure>

<p>即可编译完成，如果有错误，可以再尝试输入：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ hexo clean</span><br><span class="line">$ hexo g</span><br></pre></td></tr></table></figure>

<p><strong>Hexo更多的基础配置可查看官方文档</strong></p>
<h1 id="4、Hexo-使用主题"><a href="#4、Hexo-使用主题" class="headerlink" title="4、Hexo 使用主题"></a>4、Hexo 使用主题</h1><p>Hexo 中切换主题的方式非常简单，只需要将主题文件拷贝至根目录下的 <code>themes</code> 文件夹中， 然后修改 <code>_config.yml</code> 文件中的 <code>theme</code> 字段即可。本博客使用的是Next主题，Next 作为一款符合广大程序员审美的主题，还是有着较高的出场率的。</p>
<p>在根目录下执行以下命令下载主题文件：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git clone https://github.com/theme-next/hexo-theme-next.git themes/next</span><br></pre></td></tr></table></figure>

<p>也可以在 <a href="https://github.com/theme-next/hexo-theme-next/releases">NexT 版本发布页面</a> 手动下载然后解压到根目录下的 <code>theme</code> 文件夹下，并将文件夹命名为 <code>next</code> 。</p>
<p>打开根目录下的站点配置文件，将 <code>theme</code> 字段的值修改为 <code>next</code>。</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">next</span></span><br></pre></td></tr></table></figure>

<p>这个时候刷新浏览器页面并不会发生变化，需要重启服务器并刷新才能使主题生效。</p>
<p>如果重启服务器仍无效，尝试使用 <code>hexo clean</code> 清除缓存</p>
<p>Next 默认主题风格为 <code>Muse</code>，用户可以在主题配置文件中修改 <code>scheme</code> 字段以选择自己喜欢的主题风格：</p>
<p>本博客采用的<code>Pisces</code></p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Schemes</span></span><br><span class="line"><span class="comment">#scheme: Muse</span></span><br><span class="line"><span class="comment">#scheme: Mist</span></span><br><span class="line"><span class="attr">scheme:</span> <span class="string">Pisces</span></span><br><span class="line"><span class="comment">#scheme: Gemini</span></span><br></pre></td></tr></table></figure>



<h1 id="5、Github-部署"><a href="#5、Github-部署" class="headerlink" title="5、Github 部署"></a>5、Github 部署</h1><h2 id="5-1-连接Github"><a href="#5-1-连接Github" class="headerlink" title="5.1 连接Github"></a>5.1 连接Github</h2><ul>
<li>博客项目根目录   右键  –&gt; <code>Git Bash Here</code>  <strong>设置用户名和邮箱</strong></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ git config --global user.name &quot;GitHub的用户名&quot;</span><br><span class="line">$ git config --global user.email &quot;GitHub的邮箱&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li>创建 SSH 密匙</li>
</ul>
<p>​	输入 <code>ssh-keygen -t rsa -C &quot;GitHub 邮箱&quot;</code>，然后一路回车</p>
<ul>
<li>添加密钥</li>
</ul>
<p>​	进入 <code>C:\Users\用户名\.ssh</code> 目录（要勾选显示“隐藏的项目”），用记事本打开公钥 <code>id_rsa.pub</code> 文件并复制里面的内容</p>
<p>​	登陆 <code>GitHub</code> ，进入 <code>Settings</code> 页面，选择左边栏的 <code>SSH and GPG keys</code>，点击 <code>New SSH key</code></p>
<p>​	<code>Title</code> 随便取个名字，粘贴复制的 <code>id_rsa.pub</code> 内容到 <code>Key</code> 中，点击 <code>Add SSH key</code> 完成添加。</p>
<img src="/2023/12/13/Hexo%20+%20Typora%20+%20Github%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20231224114351461.png" alt="image-20231224114351461" style="zoom: 50%;">

<ul>
<li>验证连接</li>
</ul>
<p>​	打开 <code>Git Bash</code>，输入 <code>ssh -T git@github.com</code> 出现 <code>“Are you sure……”</code>，输入 <code>yes</code> 回车确认。</p>
<p>​	显示 <code>“Hi xxx! You&#39;ve successfully……”</code> 即连接成功。</p>
<h2 id="5-2-创建Github仓库"><a href="#5-2-创建Github仓库" class="headerlink" title="5.2 创建Github仓库"></a>5.2 创建Github仓库</h2><p><code>GitHub</code> 主页右上角加号 –&gt; <code>New repository</code>：</p>
<ul>
<li><code>Repository name</code> 中输入 <code>用户名.github.io</code></li>
<li>勾选 <code>“Initialize this repository with a README”</code></li>
<li><code>Description </code>选填</li>
</ul>
<img src="/2023/12/13/Hexo%20+%20Typora%20+%20Github%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20231224115909546.png" alt="image-20231224115909546" style="zoom: 67%;">

<ul>
<li>创建后默认自动启用 <code>HTTPS</code>，博客地址为：<code>https://用户名.github.io</code></li>
</ul>
<h2 id="5-3-部署到Github"><a href="#5-3-部署到Github" class="headerlink" title="5.3 部署到Github"></a>5.3 部署到Github</h2><ul>
<li><p>本地博客测试成功后，就是上传到 <code>GitHub</code> 进行部署，使其能够在网络上访问</p>
</li>
<li><p>首先安装 <code>hexo-deployer-git</code>：</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>

<p>然后修改项目根目录下的<code>_config.yml</code>文件末尾的 <code>Deployment </code>部分，修改成如下：</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repository:</span> <span class="string">git@github.com:用户名/用户名.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">main</span></span><br></pre></td></tr></table></figure>

<p>​	完成后项目根目录<code>Git bash</code>中运行 <code>hexo d</code> 将网站上传部署到 <code>GitHub Pages</code>。</p>
<p>​	这时访问我们的 <code>https://用户名.github.io</code> 就可以看到 <code>Hexo</code> 网站了</p>
<h1 id="6、Typora-编写博客"><a href="#6、Typora-编写博客" class="headerlink" title="6、Typora 编写博客"></a>6、Typora 编写博客</h1><p>用<code>Typora</code>编写<code>md</code>文件，利用<code>Hexo</code>解析时存在图片无法显示的问题</p>
<p>关于这个配置使得<code>md文件-&gt;hexo生成-&gt;所见即所得</code>,我花费了1个晚上的时间才搞明白，主要是别的博客质量太次了，要么照搬，要么方法矛盾，<code>bug</code>了好久</p>
<p>直接上方法</p>
<h2 id="6-1-config-yml配置"><a href="#6-1-config-yml配置" class="headerlink" title="6.1 _config.yml配置"></a>6.1 _config.yml配置</h2><ul>
<li>将项目根目录下的<code>_config.yml</code> 文件中的<code>post_asset_folder</code> 选项设为 <code>true</code></li>
</ul>
<p>​		该操作的作用就是在使用<code>hexo new xxx</code>指令新建博文时，在相同路径下同步创建一个<code>xxx</code>文件夹，而<code>xxx</code>文件夹的作用就是用来存放图片资源；<br>​		就我个人而言，我偏好于直接在<code>source\_posts</code>文件夹下新建<code>md</code>文件，而不是通过<code>hexo new xxx</code>指令；<br>那么直接新建<code>xxx.md</code>再新建<code>xxx</code>文件夹，这种操作的最终效果和使用<code>hexo new xxx</code>指令新建博文的效果一样吗？经过实测，是一样的。</p>
<h2 id="6-2-Typora图像配置"><a href="#6-2-Typora图像配置" class="headerlink" title="6.2 Typora图像配置"></a>6.2 Typora图像配置</h2><p>​		一般来说，大家会现在<code>Typora</code>里写好<code>md</code>格式的博客，然后通过<code>hexo clean</code>、<code>hexo g</code>、<code>hexo s</code>进行一下本地测试，确认无误后再发布到远端。</p>
<p>​		暂且不说<code>hexo</code>博客的图片插入是个问题，我相信当初单纯利用<code>Typora</code>做笔记时，图片文件的管理就让很多人头疼过，<code>Typora</code>官方似乎也意识到这个问题，所以偏好设置中图像是专门的一项，提供了很多选择。</p>
<p>​		我相信大多数同学写<code>md</code>时的图片很多可能是直接截图或者在其他地方copy的，然后在<code>Typora</code>中直接粘贴就ok了。但是这么做之前最好<code>Typora</code>插入图片时采取何种操作配置好，否则<code>md</code>文件和图片相隔十万八千里，后续一旦移动<code>md</code>文件图片就识别不出来，相信大家用过<code>Typora</code>都深有体会。<br>​		所以接下来讲一下<code>Typora</code>如何设置。直接给结论：</p>
<p>​        左上角 <code>文件-&gt;偏好设置-&gt;图像</code></p>
<img src="/2023/12/13/Hexo%20+%20Typora%20+%20Github%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/image-20231224153713398.png" alt="image-20231224153713398" style="zoom: 50%;">

<p>​		框里的路径是：<code>./$&#123;filename&#125;</code>。<code>./</code>表示当前文件夹，<code>$&#123;filename&#125;</code>表示当前文件名。这么设置的好处：</p>
<p>​		1、图片资源文件夹有了；</p>
<p>​		2、而且是同名文件夹！（6.1中的文件夹其实不用手动添加了）</p>
<p>​		这么设置的结果就是：想写篇博客，在<code>source\_posts</code>文件夹下新建<code>xxx.md</code>文件，写着写着需要插一张图，从别处复制，然后在<code>Typora</code>中直接粘贴，<code>bling!</code>图片资源文件夹自动搞定，并不用关心什么文件夹，只管专注于<code>md</code>文件即可。</p>
<h2 id="6-3-插件下载"><a href="#6-3-插件下载" class="headerlink" title="6.3 插件下载"></a>6.3 插件下载</h2><p>这个插件的不同版本可能会有不同的影响，我最终成功解决问题的版本是用如下命令下载的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cnpm install https://github.com/CodeFalling/hexo-asset-image --save</span><br></pre></td></tr></table></figure>

<p>为什么需要这么插件呢？</p>
<p>因为我们虽然在<code>source\_posts</code>文件夹下写了<code>md</code>文件，也有了图片资源文件夹存了图片，但从我们前面<code>Typora</code>中的设置不难知道，实际上<code>md</code>文件中的图片路径都是相对路径<code>（./$&#123;filename&#125;）</code>。而实际网上看到的博文显然不是<code>md</code>文件，而是<code>html</code>文件，从<code>md</code>到<code>html</code>的转变就是<code>Hexo</code>帮我们做的，还记得<code>hexo g</code>命令吗？就是干这个的。转换后的<code>html</code>文件在<code>public</code>目录下，路径是通过日期指示的。</p>
<p><strong>路径转换</strong>就是该插件的作用：根据<code>md</code>图片的相对路径，给出<code>html</code>中图片的绝对路径。</p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>Huggingface 服务器端镜像下载</title>
    <url>/2023/12/18/Huggingface%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E9%95%9C%E5%83%8F%E4%B8%8B%E8%BD%BD/</url>
    <content><![CDATA[<p>使用<code>Huggingface</code> 官方提供的 <a href="https://hf-mirror.com/docs/huggingface_hub/guides/download#download-from-the-cli">*<em><code>huggingface-cli</code>*</em> </a>命令行工具在服务器端镜像下载权重或文件。</p>
<span id="more"></span>

<h2 id="1、安装依赖"><a href="#1、安装依赖" class="headerlink" title="1、安装依赖"></a>1、安装依赖</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ pip install -U huggingface_hub</span><br></pre></td></tr></table></figure>



<h2 id="2、基本命令示例"><a href="#2、基本命令示例" class="headerlink" title="2、基本命令示例"></a>2、基本命令示例</h2><ul>
<li>在.bashrc文件中添加下面这条镜像配置</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ export HF_ENDPOINT=https://hf-mirror.com</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ huggingface-cli download --resume-download --local-dir-use-symlinks False THUDM/chatglm2-6b --local-dir chatglm2-6b</span><br></pre></td></tr></table></figure>

<ul>
<li><code>download</code>: Hugging Face CLI 的下载命令。</li>
<li><code>--resume-download</code>: 如果下载中断，该选项会尝试继续下载而不是重新开始。</li>
<li><code>--local-dir-use-symlinks False</code>: 该选项指示不使用符号链接。符号链接是一种链接到其他文件或目录的特殊类型的文件，这里指示不使用它们。</li>
<li><code>THUDM/chatglm2-6b</code>: 模型的名称或模型 ID。在这里，它下载的是 <code>THUDM/chatglm2-6b</code>。</li>
<li><code>--local-dir chatglm2-6b</code>: 该选项指定本地保存模型的目录名称，即 <code>chatglm2-6b</code>。</li>
</ul>
<h2 id="3、下载需要登录的模型（Gated-Model）"><a href="#3、下载需要登录的模型（Gated-Model）" class="headerlink" title="3、下载需要登录的模型（Gated Model）"></a>3、下载需要登录的模型（Gated Model）</h2><p>请添加<code>--token hf_***</code>参数，其中<code>hf_***</code>是 <em>access token</em>，请在<a href="https://huggingface.co/settings/tokens">huggingface官网这里</a>获取。示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ huggingface-cli download --token hf_*** --resume-download --local-dir-use-symlinks False meta-llama/Llama-2-7b-hf --local-dir Llama-2-7b-hf</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>服务器命令</category>
      </categories>
      <tags>
        <tag>Huggingface</tag>
      </tags>
  </entry>
  <entry>
    <title>LoRA微调实战</title>
    <url>/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/</url>
    <content><![CDATA[<p>使用LoRA微调LLaMa2，训练LLM在给定上下文无法回答用户问题时拒绝回答的能力，而不是胡说。</p>
<span id="more"></span>

<h2 id="1、实验简介"><a href="#1、实验简介" class="headerlink" title="1、实验简介"></a>1、实验简介</h2><p><strong>选题：《基于  <a href="https://github.com/hiyouga/LLaMA-Factory">https://github.com/hiyouga/LLaMA-Factory</a> 开源项目跑通一个Chat机器人》</strong></p>
<p><strong>选择的是方向1：</strong>尝试对模型进行简单的指令微调，数据集可以是自己构造的、可以是开源的；</p>
<p><strong>Github代码仓库：</strong><a href="https://github.com/LimOkii/nlp_lab">https://github.com/LimOkii/nlp_lab</a></p>
<h3 id="1-1-任务简介"><a href="#1-1-任务简介" class="headerlink" title="1.1 任务简介"></a>1.1 任务简介</h3><p>本次大作业我想微调出一个<code>LLM</code>，使之能够判断给定的语料是否能解答用户问题，不能编造答案。如果根据所有的内容都无法得出明确的结论，需要回复“对不起，根据参考资料无法回答“这些类似的回答。</p>
<p>本次微调的基座采用Meta发布的<code>LLaMa-2-hf-7b-chat</code>版本，训练<code>LLM</code>在给定上下文无法回答用户问题时拒绝回答的能力，而不是胡说。</p>
<p><strong>微调代码参考：</strong><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2">https://github.com/ymcui/Chinese-LLaMA-Alpaca-2</a></p>
<h3 id="1-2-数据集介绍"><a href="#1-2-数据集介绍" class="headerlink" title="1.2  数据集介绍"></a>1.2  数据集介绍</h3><ul>
<li>本次微调采用的数据集是百度发布的<code>WebQA</code></li>
</ul>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">链接: https://pan.baidu.com/s/1pLXEYtd 密码: 6fbf</span><br><span class="line"></span><br><span class="line">文件列表：</span><br><span class="line">WebQA.v1.0/readme.txt</span><br><span class="line">WebQA.v1.0/me_test.ann.json （一个问题只配一段材料，材料中有答案）</span><br><span class="line">WebQA.v1.0/me_test.ir.json （一个问题配多段材料，材料可能有也可能没有答案）</span><br><span class="line">WebQA.v1.0/me_train.json （混合的训练语料）</span><br><span class="line">WebQA.v1.0/me_validation.ann.json （一个问题只配一段材料，材料中有答案）</span><br><span class="line">WebQA.v1.0/me_validation.ir.json （一个问题配多段材料，材料可能有也可能没有答案）</span><br><span class="line"></span><br><span class="line">test跟validation的区别是，理论上来说，validation的分布跟train的分布更加接近。一般而言，validation用来验证模型的精确度，test用来验证模型的迁移能力。ann与ir的区别是，因为ir给每个问题配置了多段材料，可以通过各段材料投票来得到更加可靠的答案；而ann则是一问一材料的形式，是真正考验阅读理解能力的测试集。</span><br></pre></td></tr></table></figure>



<ul>
<li><code>me_train.jsons</code>数据样例如下：</li>
</ul>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="attr">&quot;Q_TRN_005637&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;question&quot;</span><span class="punctuation">:</span> <span class="string">&quot;世界上最早的报纸诞生于&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;evidences&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;Q_TRN_005637#00&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;answer&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;no_answer&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;evidence&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1、十月革命胜利,世界上出现了第一个社会主义国家.一个崭新的社会主义报刊体系在苏俄确立形成.&lt;e&gt;2、二战结束后,又有欧、亚、拉美一系列国家脱离了资本主义体系,走社会主义道路,社会主义报业得到很大发展.&lt;e&gt;3、“苏东”剧变后,这些国家的报业结构和性质发生了重大变化.&lt;e&gt;十六、苏联时期报刊体制的主要特征是怎样的?&lt;e&gt;1、苏联的报刊,都属于国家所有,是党和国家机构的重要组成部分；其基本职能是集体的宣传员、集体的鼓动员和集体的组织者.&lt;e&gt;2、苏联的各级报刊绝对服从于各级党委的领导.&lt;e&gt;3、苏联报纸信息来源单一,言论高度集中.&lt;e&gt;4、苏联报刊在建设时期是社会主义建设的工具.&lt;e&gt;十七、发展中国家报业又何共同特点?&lt;e&gt;1、早期报刊、尤其是报业发端较早的国家的早期报刊,大多是殖民者创办的；&lt;e&gt;2、随着反殖民主义反封建斗争的开展,这些国家的民族报刊逐步发展起来,并推动了反殖民主义反封建斗争的进程。</span></span><br><span class="line"><span class="string">        &#125;,</span></span><br><span class="line"><span class="string">        ………</span></span><br><span class="line"><span class="string">        …………</span></span><br><span class="line"><span class="string">        &quot;</span>Q_TRN_005637#<span class="number">03</span><span class="string">&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;</span>answer<span class="string">&quot;: [</span></span><br><span class="line"><span class="string">                &quot;</span>中国<span class="string">&quot;</span></span><br><span class="line"><span class="string">            ],</span></span><br><span class="line"><span class="string">            &quot;</span>evidence<span class="string">&quot;: &quot;</span>北宋末年(公元<span class="number">11</span><span class="punctuation">,</span><span class="number">12</span>世纪)出现的印刷报纸<span class="punctuation">,</span>不仅是中国新闻史上最早的印刷报纸<span class="punctuation">,</span>也是世界新闻史上最早的印刷报纸.中国新闻事业历史的悠久<span class="punctuation">,</span>内容的丰富<span class="punctuation">,</span>是任何西方国家都难以比肩的.&lt;e&gt;中国古代的报纸产生于中国的封建社会时期<span class="punctuation">,</span>是封建地主阶级及其政治代表占统治地位的封建自然经济通过新闻手段的反映.在漫长的封建社会时期<span class="punctuation">,</span>中国古代的报纸<span class="punctuation">,</span>不论是官方的邸报<span class="punctuation">,</span>还是民办的小报和京报<span class="punctuation">,</span>都必然要和当时的封建统治者保持一定的联系.&lt;e&gt;中国古代的邸报有<span class="number">1200</span>年左右的历史.小报有近千年的历史.民间报房出版的邸报<span class="punctuation">,</span>京报有近<span class="number">400</span>年的历史.它们从诞生到结束<span class="punctuation">,</span>持续的时间都不算短<span class="punctuation">,</span>但发展不快<span class="punctuation">,</span>形式内容的变化不大.<span class="string">&quot;</span></span><br><span class="line"><span class="string">        &#125;,</span></span><br><span class="line"><span class="string">        &quot;</span>Q_TRN_005637#<span class="number">04</span><span class="string">&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;</span>answer<span class="string">&quot;: [</span></span><br><span class="line"><span class="string">                &quot;</span>no_answer<span class="string">&quot;</span></span><br><span class="line"><span class="string">            ],</span></span><br><span class="line"><span class="string">            &quot;</span>evidence<span class="string">&quot;: &quot;</span>因此，一般认为，世界上最早的报纸诞生在<span class="number">1609</span>年。<span class="string">&quot;</span></span><br><span class="line"><span class="string">        &#125;,</span></span><br><span class="line"><span class="string">        &quot;</span>Q_TRN_005637#<span class="number">05</span><span class="string">&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;</span>answer<span class="string">&quot;: [</span></span><br><span class="line"><span class="string">                &quot;</span>中国<span class="string">&quot;</span></span><br><span class="line"><span class="string">            ],</span></span><br><span class="line"><span class="string">            &quot;</span>evidence<span class="string">&quot;: &quot;</span>报纸从诞生到今天已经走过了漫长的历史，公元前<span class="number">60</span>年，古罗马政治家恺撒把罗马市以及国家发生的时间书写在白色的木板上，告示市民。这便是世界上最古老的报纸。中国在<span class="number">7</span>世纪，唐朝宫廷内就发行过手写的传阅版，这应该算是中国最早的报纸。<span class="string">&quot;</span></span><br><span class="line"><span class="string">        &#125;,</span></span><br><span class="line"><span class="string">        &quot;</span>Q_TRN_005637#<span class="number">06</span><span class="string">&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;</span>answer<span class="string">&quot;: [</span></span><br><span class="line"><span class="string">                &quot;</span>中国<span class="string">&quot;</span></span><br><span class="line"><span class="string">            ],</span></span><br><span class="line"><span class="string">            &quot;</span>evidence<span class="string">&quot;: &quot;</span>最早的写在纸上的报纸和印刷在纸上的报纸都诞生于中国.唐玄宗开元年间(公元<span class="number">713</span>年-<span class="number">-742</span>年)出现的开元杂报<span class="punctuation">,</span>不仅是中国新闻史上最早的报纸<span class="punctuation">,</span>也是世界新闻史上最早的报纸.<span class="string">&quot;</span></span><br><span class="line"><span class="string">        &#125;,</span></span><br><span class="line"><span class="string">        &quot;</span>Q_TRN_005637#<span class="number">09</span><span class="string">&quot;: &#123;</span></span><br><span class="line"><span class="string">            &quot;</span>answer<span class="string">&quot;: [</span></span><br><span class="line"><span class="string">                &quot;</span>no_answer<span class="string">&quot;</span></span><br><span class="line"><span class="string">            ],</span></span><br><span class="line"><span class="string">            &quot;</span>evidence<span class="string">&quot;: &quot;</span>答：<span class="number">1566</span>年<span class="punctuation">,</span>世界最早的印刷报纸《威尼斯新闻》诞生于<span class="number">1566</span>年的意大利威尼斯邸报》是我国在世界上发行最早，时间最久的报纸。<span class="string">&quot;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>

<p>​        这个数据集非常适合做给定上下文的回答问题，<code>evidence</code>即是输入给模型的上下文，<code>question</code>则是用户提出的问题，模型需要根据给定的<code>evidence</code>以及<code>question</code>回答<code>no_answer</code>或者是答案。</p>
<h2 id="2、基座模型LLaMa介绍"><a href="#2、基座模型LLaMa介绍" class="headerlink" title="2、基座模型LLaMa介绍"></a>2、基座模型LLaMa介绍</h2><p> 本次微调的基座模型采用Meta发布的<code>LLaMa-2-hf-7b-chat</code>版本</p>
<img src="/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20240101102501900.png" alt="image-20240101102501900" style="zoom:50%;">

<p><code>LLaMa2</code> 和 <code>LLaMa</code> 的模型结构基本一致，共用了 32 个 <code>decoder</code> 层。其中每个 <code>decoder</code> 层如上图右半部分所示，<code>LLaMa2</code> 主要是将 <code>Transformer</code> 中的 <code>Layer Norm</code> 换成了 <code>RMS Norm</code>，<code>Multi-Head Attention</code> 换成了 <code>GQA</code>（&#96;&#96;LLaMa<code>是</code>MQA<code>）, </code>Positional Encoding <code>换成了 </code>Rotary Encoding<code>（</code>RoPE<code> 旋转位置编码），在前馈神经网络（</code>FFN<code>） 使用 </code>SwiGLU<code>激活函数替换了</code>Transformer<code>中的</code>ReLU&#96; 激活函数。</p>
<h2 id="3、实验步骤"><a href="#3、实验步骤" class="headerlink" title="3、实验步骤"></a>3、实验步骤</h2><h3 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h3><p>本次微调代码参考的<code>Chinese-LLaMA-Alpaca-2</code>，指令微调数据格式为<code>Stanford Alpaca</code>：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span><span class="attr">&quot;instruction&quot;</span> <span class="punctuation">:</span> ...<span class="punctuation">,</span></span><br><span class="line">   <span class="attr">&quot;input&quot;</span> <span class="punctuation">:</span> ...<span class="punctuation">,</span></span><br><span class="line">   <span class="attr">&quot;output&quot;</span> <span class="punctuation">:</span> ...<span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  ...</span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>

<p>需要对<code>WebQA</code>数据集做转换，因此编写了脚本 <code>convert_data_to_llama_train.py</code></p>
<p><code>instruction</code>：”请根据给定下文：” +  “evidence” +  ‘\n’  +  “告诉我”  +  “question” + ‘\n’</p>
<p><code>input</code>: “”</p>
<p><code>output</code>：”answer”</p>
<ul>
<li>为了让模型无法回答的输出多样化，如果答案为<code>no_answer</code>,则从以下模板中随机选择一句回答</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 无法回答时，模型给出的回答样例</span></span><br><span class="line">cant_answer_template = [</span><br><span class="line">    <span class="string">&#x27;抱歉，根据您所给的内容，我无法找到有关问题的答案&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;给定的信息中似乎没有提到问题的答案&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;根据提供的内容，我无法找到问题的相关信息&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;根据您提供的上下文，我找不到与问题相关的答案&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;给定的信息中似乎没有与问题有关的信息&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;根据上述内容，我难以找到问题的解答&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;据我所知，问题的答案不在提供的信息中&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;根据上述信息，问题的答案似乎不可得&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;给定的上下文似乎没有包含问题的答案&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;给定的信息中似乎没有与问题有关的线索&#x27;</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<ul>
<li>最终转换后的训练数据样例如下：</li>
</ul>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;请根据给定下文：1、十月革命胜利,世界上出现了第一个社会主义国家.一个崭新的社会主义报刊体系在苏俄确立形成.&lt;e&gt;2、二战结束后,又有欧、亚、拉美一系列国家脱离了资本主义体系,走社会主义道路,社会主义报业得到很大发展.&lt;e&gt;3、“苏东”剧变后,这些国家的报业结构和性质发生了重大变化.&lt;e&gt;十六、苏联时期报刊体制的主要特征是怎样的?&lt;e&gt;1、苏联的报刊,都属于国家所有,是党和国家机构的重要组成部分；其基本职能是集体的宣传员、集体的鼓动员和集体的组织者.&lt;e&gt;2、苏联的各级报刊绝对服从于各级党委的领导.&lt;e&gt;3、苏联报纸信息来源单一,言论高度集中.&lt;e&gt;4、苏联报刊在建设时期是社会主义建设的工具.&lt;e&gt;十七、发展中国家报业又何共同特点?&lt;e&gt;1、早期报刊、尤其是报业发端较早的国家的早期报刊,大多是殖民者创办的；&lt;e&gt;2、随着反殖民主义反封建斗争的开展,这些国家的民族报刊逐步发展起来,并推动了反殖民主义反封建斗争的进程；十八、新闻通讯社是在怎样的背景下诞生的?它的功能与作用如何?\n告诉我世界上最早的报纸诞生于\n&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;给定的上下文似乎没有包含问题的答案&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;请根据给定下文：1566年,世界最早的印刷报纸《威尼斯新闻》诞生于1566年的意大利威尼斯\n告诉我世界上最早的报纸诞生于\n&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;给定的信息中似乎没有与问题有关的信息&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>



<h3 id="3-2-微调训练"><a href="#3-2-微调训练" class="headerlink" title="3.2 微调训练"></a>3.2 微调训练</h3><h4 id="3-2-1-LoRA介绍"><a href="#3-2-1-LoRA介绍" class="headerlink" title="3.2.1 LoRA介绍"></a>3.2.1 LoRA介绍</h4><p>由于大语言模型参数量十分庞大，当将其应用到下游任务时，微调全部参数需要相当高的算力。为了节省成本，研究人员提出了多种参数高效<code>（Parameter Efficient）</code>的微调方法，旨在仅训练少量参数使模型适应到下游任务。本项目使用<code>LoRA(Low-Rank Adaptation of Large Language Models)</code>进行模型微调。<code>LoRA </code>方法 可以在缩减训练参数量和 <code>GPU</code> 显存占用的同时，使训练后的模型具有与全量微调相当的性能。</p>
<p>研究表明，语言模型针对特定任务微调之后，权重矩阵通常具有很低的本征秩 <code>（Intrinsic Rank）</code>。研究人员认为参数更新量即便投影到较小的子空间中，也不会影响学习的有效性。因此，提出固定预训练模型参数不变，在原本权重矩阵旁路添加低秩矩阵的乘积作为可训练参数，用以模拟参数的变化量。具体来说，假设预训练权重为${w_0\ \epsilon \ \mathbb{R}^{d<em>k}}$，可训练参数为${\varDelta W\ &#x3D;\ BA}$，其中${B\ \epsilon \ \mathbb{R}^{d</em>r} }$，${A\ \epsilon \ \mathbb{R}^{r*d}}$，初始化时，矩阵 ${A}$ 通过高斯函数初始化，矩阵${B}$ 为零初始化，使得训练开始之前旁路对原模型不造成影响，即参数改变量为 0。对于该权重的输入 ${x}$ 来说，输出为式${h\ &#x3D;\ W_0x+∆W\ x\ &#x3D;W_0x+BAx}$，<code>LoRA</code>算法结构方法如图：</p>
<img src="/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20240101152138111.png" alt="image-20240101152138111" style="zoom:50%;">



<p>除 <code>LoRA</code> 之外，也其他高效微调方法，如微调适配器<code>（Adapter）</code>或前缀微调<code>（Prefix Tuning）</code>。 适配器方法分别对 <code>Transformer </code>层中的自注意力模块与多层感知<code>（MLP）</code>模块，在其与其之后的残差连接之间添加适配器层<code>（Adapter layer）</code>作为可训练参数，该方法及其变体会增加网络的深度，从而在模型推理时带来额外的时间开销。当没有使用模型或数据并行时，这种开销会较为明显。而对于使用 <code>LoRA </code>的模型来说，由于可以将原权重与训练后权重合并，即 ${W\ &#x3D;\ W_0\ +\ BA}$， 因此在推理时不存在额外的开销。前缀微调是指在输入序列前缀添加连续可微的软提示作为可训练参数。由于模型可接受的最大输入长度有限，随着软提示的参数量增多，实际输入序列的最大长度也会相应减小，影响模型性能。这使得前缀微调的模型性能并非随着可训练参数量单调上升。 在文献的实验中，使用 <code>LoRA</code> 方法训练的 <code>GPT-2</code>、<code>GPT-3</code>模型在相近数量的可训练参数下， 性能均优于或相当于使用上述两种微调方法。</p>
<h4 id="3-2-2-LoRA微调"><a href="#3-2-2-LoRA微调" class="headerlink" title="3.2.2 LoRA微调"></a>3.2.2 LoRA微调</h4><p>数据共<code>40w+</code>条，其中训练数据<code>313910</code>条，其余是验证数据，在单卡<code>A6000 48G显存</code>显卡上采用LoRA方式微调。</p>
<img src="/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20240101104514987.png" alt="image-20240101104514987" style="zoom:50%;">

<p>可以看到原版<code>LLaMa2</code>是<code>7b</code>的权重,使用<code>LoRA</code>方式微调，训练参数仅为<code>0.3b</code>，为初始权重的<code>4%</code>左右，大大减少了需要训练的参数量。</p>
<p>在单卡<code>A6000 48G显存</code>训练一个<code>epoch</code>，约<code>57</code>个小时(包括训练时间和评估时间)，最终的<code>loss</code>从一开始的<code>7</code>左右降到了<code>0.1</code>上下。</p>
<h4 id="3-2-3-权重合并"><a href="#3-2-3-权重合并" class="headerlink" title="3.2.3 权重合并"></a>3.2.3 权重合并</h4><p>手动将<code>LoRA</code>与原版<code>Llama-2</code>合并得到完整模型的流程</p>
<p>确保机器有足够的内存加载完整模型（例如<code>7B</code>模型需要<code>13-15G</code>）以进行合并模型操作</p>
<p><strong>Step 1: 获取原版Llama-2-hf模型</strong></p>
<p><code>HF</code>格式模型相关文件（可以不用下载<code>safetensors</code>格式模型权重）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">config.json</span><br><span class="line">generation_config.json</span><br><span class="line">pytorch_model-00001-of-00002.bin</span><br><span class="line">pytorch_model-00002-of-00002.bin</span><br><span class="line">pytorch_model.bin.index.json</span><br><span class="line">special_tokens_map.json</span><br><span class="line">tokenizer_config.json</span><br><span class="line">tokenizer.json</span><br><span class="line">tokenizer.model</span><br></pre></td></tr></table></figure>

<p><strong>Step 2: 合并LoRA权重，生成全量模型权重</strong></p>
<p>这一步骤会合并<code>LoRA</code>权重，生成全量模型权重。此处可以选择输出<code>PyTorch</code>版本权重（<code>.pth</code>文件）或者输出<code>HuggingFace</code>版本权重（<code>.bin</code>文件）。执行以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ python scripts/merge_llama2_with_chinese_lora_low_mem.py \</span><br><span class="line">    --base_model path_to_original_llama2_hf_dir \</span><br><span class="line">    --lora_model path_to_chinese_llama2_or_alpaca2_lora \</span><br><span class="line">    --output_type huggingface \</span><br><span class="line">    --output_dir path_to_output_dir </span><br><span class="line">    --verbose</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li><code>--base_model</code>：存放<code>HF</code>格式的<code>Llama-2</code>模型权重和配置文件的目录</li>
<li><code>--lora_model</code>：中文<code>LLaMA-2/Alpaca-2 LoRA</code>解压后文件所在目录，也可使用🤗<code>Model Hub</code>模型调用名称（会自动下载）</li>
<li><code>--output_type</code>：指定输出格式，可为<code>pth</code>或<code>huggingface</code>。若不指定，默认为<code>huggingface</code></li>
<li><code>--output_dir</code>：指定保存全量模型权重的目录，默认为<code>./</code></li>
<li>（可选）<code>--verbose</code>：显示合并过程中的详细信息</li>
</ul>
<p><img src="/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20240101161620803.png" alt="image-20240101161620803"></p>
<h2 id="4、实验结果展示"><a href="#4、实验结果展示" class="headerlink" title="4、实验结果展示"></a>4、实验结果展示</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = <span class="string">&quot;/data0/luyifei/cant_ans_merge_weight/&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model)</span><br><span class="line">pipeline = transformers.pipeline(</span><br><span class="line">    <span class="string">&quot;conversational&quot;</span>,</span><br><span class="line">    model=model,</span><br><span class="line">    torch_dtype=torch.float16,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span>,</span><br><span class="line">)</span><br><span class="line">question = <span class="string">&quot;请根据给定下文：在返回江陵途中，写下了这首诗，抒发了诗人愉悦的心情。\n告诉我李白写过一首诗，对飞舟过峡的动态美景作了绝妙的描述，千古流传，这首诗的题目是什么?&quot;</span></span><br><span class="line">conversation = Conversation(question)</span><br><span class="line">sequences = pipeline(</span><br><span class="line">    conversation,</span><br><span class="line">    do_sample=<span class="literal">True</span>,</span><br><span class="line">    top_k=<span class="number">10</span>,</span><br><span class="line">    num_return_sequences=<span class="number">1</span>,</span><br><span class="line">    eos_token_id=tokenizer.eos_token_id,</span><br><span class="line">    max_length=<span class="number">500</span>,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;问题1是&#x27;</span>,question1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;模型的回复是：&#x27;</span>sequences.generated_responses[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>加载合并后的权重，3个测试样例如下：</p>
<p><img src="/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20240101174138733.png" alt="image-20240101174138733"></p>
<ul>
<li><p>例子1和例子3回答正确</p>
</li>
<li><p>例子2回答错误</p>
</li>
</ul>
<p>例子1中，给定的上下文中没有关于这首诗的题目，因此模型无法回答该问题。</p>
<p>例子2中，给定的上下文中给出了李白的出生地为碎叶城，但是模型却回复无法回答该问题。</p>
<p>例子3中，给定的上下文中告知b-2轰炸机是美国空军研制，模型也能正确回复答案<code>美国</code></p>
<h2 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h2><p>​		使用<code>LoRA</code>方式微调<code>LLaMa</code>，能使大模型一定程度上根据给定的上下文来回答问题。在给定上下文不包含问题的答案时能输出”对不起，我无法回答该问题”等回复，若给定上下文包含问题的答案，模型也能输出正确答案。</p>
<p>​		但是当我尝试更多样例测试时，发现模型更容易偏向输出无法回答的回复，即使给定上下文中有明确的问题答案。我总结的分析原因如下：</p>
<p>​		微调大型模型时，模型可能会倾向于输出一种相对保守的策略，即更倾向于回答无法回答的响应。这可能是因为微调过程中的数据集中，有更多的例子涉及到模型无法从给定上下文中得知答案的情况，导致模型更容易学习到这种“保守”的回答。</p>
<p>有几个可能的原因导致这种现象：</p>
<ol>
<li><strong>数据分布不均衡：</strong> 可能时微调数据中无法回答的例子相对较多，模型可能会更容易学习到输出类似于“无法回答”的响应。</li>
<li><strong>Loss 函数设计：</strong> 微调过程中使用的损失函数可能也影响了模型的学习方向。如果损失函数更倾向于对无法回答的情况进行惩罚，模型可能更倾向于产生这样的输出。</li>
<li><strong>训练数据中的噪声：</strong> 如果微调数据中包含了噪声或错误的标签，模型可能会过度拟合这些错误的标签，导致更多的“无法回答”响应。</li>
</ol>
<p>​	下一步尝试的改进方向：</p>
<p>​	1、<strong>检查数据质量：</strong> 仔细检查微调数据集，确保标签和上下文对应正确，避免包含噪声或错误的信息。</p>
<p>​    2、<strong>平衡数据集：</strong> 确保微调的数据集中有足够的例子涉及到模型可以回答的情况，以及无法回答的情况，以避免数据分布不均衡。</p>
]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>LoRA</tag>
      </tags>
  </entry>
  <entry>
    <title>Parameter Efficient Fine-Tuning(PEFT)系列论文总结(二)</title>
    <url>/2023/12/26/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/</url>
    <content><![CDATA[<p>承接上篇Parameter Efficient Fine-Tuning(PEFT)系列论文总结(一)，本篇主要介绍P-Tuning系列的微调方法。</p>
<span id="more"></span>

<h2 id="一、P-Tuning-v1"><a href="#一、P-Tuning-v1" class="headerlink" title="一、P-Tuning v1"></a>一、P-Tuning v1</h2><p>清华大学的研究者于2021年3月通过此篇论文《<a href="https://arxiv.org/pdf/2103.10385">GPT Understands, Too</a>》提出<code>P-Tuning</code></p>
<p>文章的提出为了解决这样一个问题，如下图给出：</p>
<p>大模型的<code>Prompt</code>构造方式严重影响下游任务的效果。比如：<code>GPT-3</code>采用人工构造的模版来做上下文学习<code>(in-context learning)</code>，但人工设计的模版的变化特别敏感，加一个词或者少一个词，或者变动位置都会造成比较大的变化。</p>
<img src="/2023/12/26/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/image-20240106145327787.png" alt="image-20240106145327787" style="zoom: 67%;">

<p>之前的工作都是这种离散化的<code>token</code>，搜索出来的结果可能并不是最优的，导致性能不稳定。</p>
<p>![image-20240106153147765](Parameter Efficient Fine-Tuning(PEFT)系列论文总结(二)&#x2F;image-20240106153147765.png)</p>
<p>基于此，作者提出了<code>P-Tuning</code>，设计了一种连续可微的<code>virtual token</code>（类似<code>Prefix-Tuning</code>）。<code>P-Tuning</code>成功地实现了模版的自动构建，且借助<code>P-tuning</code>，<code>GPT</code>在<code>SuperGLUE</code>上的成绩首次超过了同等级别的<code>BERT</code>模型，这颠覆了在那年之前“<code>GPT</code>不擅长<code>NLU</code>”的结论，也是该论文命名的缘由</p>
<img src="/2023/12/26/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/image-20240106150023656.png" alt="image-20240106150023656" style="zoom:80%;">

<p><strong>对于上面可微的理解：</strong></p>
<p>原文是<code>continuous</code>连续，这里可微就是可导，应该是反向传播的时候要求导数，所以可以BP优化学习&#x3D;可微。</p>
<p><strong>P-tuning和Prefix Tuning类似</strong>，也放弃了“模版由自然语言构成”这一常规要求，从而将模版的构建转化为连续参数优化问题</p>
<p>下图是一个<code>prompt search</code>针对<code>The capital of Britain is [MASK]</code>(英国的首都是哪个城市)的例子<br>即给定上下文(蓝色区域，“英国”)和目标(红色区域，“[MASK]”)，橙色区域指的是提示符号<code>prompt tokens</code></p>
<img src="/2023/12/26/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/image-20240107104210128.png" alt="image-20240107104210128" style="zoom:80%;">

<ul>
<li><p>在(a)中，提示生成器只收到离散的奖励<br><em>In (a), the prompt generator only receives discrete rewards</em></p>
</li>
<li><p>在(b)中，伪<code>prompt</code>和<code>prompt encoder</code>可以以可微的方式进行优化，有时，在(b)中添加少量与任务相关的<code>anchor tokens</code>(如<code>capital</code>)将带来进一步的改进<br><em>in (b) the pseudo prompts and prompt encoder can be optimized in a differentiable way. Sometimes, adding few task-related anchor tokens(such as “capital” in (b)) will bring further improvement</em></p>
<p>(<code>ps</code>：经过预训练的LM的词嵌入已经变得高度离散，如果随机初始化<code>virtual token</code>，容易优化到局部最优值，而这些<code>virtual token</code>理论是应该有关联的。因此，作者通过实验发现用一个<code>prompt encoder</code>来编码会收敛更快，效果更好。即用一个<code>LSTM+MLP</code>去编码这些<code>virtual token</code>以后，再输入到模型)</p>
</li>
</ul>
<p>换言之，<code>P-tuning</code>做法是用一些伪<code>prompt</code>代替这些显式的<code>prompt</code>(说白了，将自然语言提示的<code>token</code>，替换为可训练的嵌入)<br>具体的做法是可以用预训练词表中的<code>unused token</code>作为伪<code>prompt</code>「<code>BERT</code>的<code>vocab</code>里有<code>unused 1 ~ unused99</code>，就是为了方便增加词汇的」，然后通过训练去更新这些<code>token</code>的参数<br>也就是，<code>P-tuning</code>的<code> Prompt</code>不是显式的，不是我们可以看得懂的字符，而是一些隐式的、经过训练的、模型认为最好的<code>prompt token</code></p>
<p><strong>Prefix Tuning和P-Tuning v1的区别：</strong></p>
<p>1、<code>prefix tuning</code>在所有<code>transformer layer</code>都加入了<code>prompt</code>，而<code>P-Tuning</code>只在输入层加</p>
<p>2、<code>P-Tuning</code>的<code>virtual token</code>的位置也不一定是前缀，插入的位置是可选的</p>
<p>3、作者是用一个<code>prompt encoder</code>来编码收敛更快，效果更好。也就是说，用一个<code>LSTM+MLP</code>去编码这些<code>virtual token</code>以后，再输入到模型</p>
<p>苏剑林说：“在P-Tuning中，如果我们不将新插入的token视为“模版”，是将它视为模型的一部分，那么实际上P-Tuning也是一种类似Adapter的做法，同样是固定原模型的权重，然后插入一些新的可优化参数，同样是只优化这些新参数，只不过这时候新参数插入的是Embedding层，因此，从这个角度看，P-Tuning与Adapter有颇多异曲同工之处”</p>
<h2 id="二、-P-Tuning-v2"><a href="#二、-P-Tuning-v2" class="headerlink" title="二、 P-Tuning v2"></a>二、 P-Tuning v2</h2><p>之前的<code>Prompt Tuning</code>和<code>P-Tuning</code>等方法存在两个主要的问题：</p>
<p>第一，缺乏模型参数规模和任务通用性。</p>
<ul>
<li>缺乏规模通用性：<code>Prompt Tuning</code>论文中表明当模型规模超过100亿个参数时，提示优化可以与全量微调相媲美。但是对于那些较小的模型（从100M到1B），提示优化和全量微调的表现有很大差异，这大大限制了提示优化的适用性。</li>
<li>缺乏任务普遍性：尽管<code>Prompt Tuning</code>和<code>P-tuning</code>在一些 <code>NLU </code>基准测试中表现出优势，但提示调优对硬序列标记任务（即序列标注）的有效性尚未得到验证。</li>
</ul>
<p>第二，缺少深度提示优化，在<code>Prompt Tuning</code>和<code>P-tuning</code>中，连续提示只被插入<code>transformer</code>第一层的输入<code>embedding</code>序列中，在接下来的<code>transformer</code>层中，插入连续提示的位置的<code>embedding</code>是由之前的<code>transformer</code>层计算出来的，这可能导致两个可能的优化挑战：</p>
<ul>
<li>由于序列长度的限制，可调参数的数量是有限的</li>
<li>输入<code>embedding</code>对模型预测只有相对间接的影响</li>
</ul>
<p>考虑到这些问题，作者提出了<code>P-Tuning v2</code>，它利用深度提示优化（如：<code>Prefix Tuning</code>），对<code>Prompt Tuning</code>和<code>P-Tuning</code>进行改进，作为一个跨规模和<code>NLU</code>任务的通用解决方案</p>
<p><code>P-Tuning v2</code>（论文： <strong>P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</strong>），该方法在<strong>每一层</strong>都加入了<code>Prompts tokens</code>作为输入，而不是仅仅加在输入层，这带来两个方面的好处：</p>
<ul>
<li>更多可学习的参数（从<code>P-Tuning</code>和<code>Prompt Tuning</code>的0.01%增加到0.1%-3%），同时也足够参数高效。</li>
<li>加入到更深层结构中的<code>Promp</code>能给模型预测带来更直接的影响</li>
</ul>
<img src="/2023/12/26/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/image-20240107135447839.png" alt="image-20240107135447839" style="zoom:80%;">

<p>具体做法基本同<code>Prefix Tuning</code>，可以看作是将<strong>文本生成</strong>的<code>Prefix Tuning</code>技术适配到<code>NLU</code>任务中，然后做了一些改进：</p>
<ul>
<li><strong>移除重参数化的编码器</strong>。以前的方法利用重参数化功能来提高训练速度和鲁棒性（如：<code>Prefix Tuning</code>中的<code>MLP</code>、<code>P-Tuning</code>中的<code>LSTM</code>））。在 <code>P-Tuning v2</code> 中，作者发现重参数化的改进很小，尤其是对于较小的模型，同时还会影响模型的表现。</li>
<li><strong>针对不同任务采用不同的提示长度</strong>。提示长度在提示优化方法的超参数搜索中起着核心作用。在实验中，我们发现不同的理解任务通常用不同的提示长度来实现其最佳性能，这与<code>Prefix-Tuning</code>中的发现一致，不同的文本生成任务可能有不同的最佳提示长度。</li>
<li><strong>引入多任务学习</strong>。先在多任务的<code>Prompt</code>上进行预训练，然后再适配下游任务。多任务学习对我们的方法来说是可选的，但可能是相当有帮助的。一方面，连续提示的随机惯性给优化带来了困难，这可以通过更多的训练数据或与任务相关的无监督预训练来缓解；另一方面，连续提示是跨任务和数据集的特定任务知识的完美载体。我们的实验表明，在一些困难的序列任务中，多任务学习可以作为<code>P-Tuning v2</code>的有益补充。</li>
<li><strong>回归传统的分类标签范式，而不是映射器</strong>。标签词映射器（Label Word Verbalizer）一直是提示优化的核心组成部分，它将<code>one-hot</code>类标签变成有意义的词，以利用预训练语言模型头。尽管它在<code>few-shot</code>设置中具有潜在的必要性，但在全数据监督设置中，<code>Verbalizer</code>并不是必须的。它阻碍了提示调优在我们需要无实际意义的标签和句子嵌入的场景中的应用。因此，<code>P-Tuning v2</code>回归传统的<code>CLS</code>标签分类范式，采用随机初始化的分类头（Classification Head）应用于<code>tokens</code>之上，以增强通用性，可以适配到序列标注任务。</li>
</ul>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformers</title>
    <url>/2023/11/19/Transformers/</url>
    <content><![CDATA[<h1 id="Transformers理解"><a href="#Transformers理解" class="headerlink" title="Transformers理解"></a>Transformers理解</h1><h2 id="1、模型结构概览"><a href="#1、模型结构概览" class="headerlink" title="1、模型结构概览"></a>1、模型结构概览</h2><p><strong>大致结构图：</strong></p>
<p><img src="/2023/11/19/Transformers/transformer3.png" alt="img"></p>
<p><strong>细致结构图：</strong></p>
<p><img src="/2023/11/19/Transformers/transformer.jpg" alt="img"></p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">模型大致分为Encoder(编码器)和Decoder(解码器)两个部分，分别对应上图中的左右两部分。</span><br><span class="line">其中编码器由N个相同的层堆叠在一起(原论文取N=6)，每一层又有两个子层</span><br><span class="line"></span><br><span class="line">第一个子层是一个Multi-Head Attention(多头的自注意机制)</span><br><span class="line">第二个子层是一个简单的Feed Forward(全连接前馈网络)</span><br><span class="line">两个子层都添加了一个残差连接+layer normalization的操作</span><br><span class="line"></span><br><span class="line">模型的解码器同样是堆叠了N个相同的层，不过和编码器中每层的结构稍有不同。</span><br><span class="line">对于解码器的每一层，除了编码器中的两个子层Multi-Head Attention和Feed Forward，解码器还包含一个子层Masked Multi-Head Attention，如图中所示每个子层同样也用了residual以及layer normalization</span><br><span class="line"></span><br><span class="line">模型的输入由Input Embedding和Positional Encoding(位置编码)两部分组合而成(直接对位相加)</span><br><span class="line">模型的输出由Decoder的输出简单的经过softmax得到</span><br><span class="line"></span><br><span class="line">结合上图对Transformer模型的结构做了个大致的梳理，下面对提及的每个模块进行详细介绍</span><br></pre></td></tr></table></figure>

<h2 id="2、模型的输入部分"><a href="#2、模型的输入部分" class="headerlink" title="2、模型的输入部分"></a>2、模型的输入部分</h2><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">输入部分包含两个模块</span><br><span class="line">Embedding 和 Positional Encoding</span><br></pre></td></tr></table></figure>

<h3 id="2-1-Eebedding层"><a href="#2-1-Eebedding层" class="headerlink" title="2.1 Eebedding层"></a>2.1 Eebedding层</h3><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">Embedding层的作用是将某种格式的输入数据，例如文本</span><br><span class="line">转变为模型可以处理的向量表示，来描述原始数据所包含的信息。</span><br><span class="line"></span><br><span class="line">Embedding层输出的可以理解为当前时间步的特征，如果是文本任务，这里就可以是Word Embedding</span><br><span class="line">如果是其他任务，就可以是任何合理方法所提取的特征。</span><br><span class="line"></span><br><span class="line">构建Embedding层的代码的核心是借助torch提供的nn.Embedding，如下：</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        类的初始化函数</span></span><br><span class="line"><span class="string">        d_model：指词嵌入的维度</span></span><br><span class="line"><span class="string">        vocab:指词表的大小                           </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        <span class="comment">#之后就是调用nn中的预定义层Embedding，获得一个词嵌入对象self.lut</span></span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        <span class="comment">#最后就是将d_model传入类中</span></span><br><span class="line">        self.d_model =d_model</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Embedding层的前向传播逻辑</span></span><br><span class="line"><span class="string">        参数x：这里代表输入给模型的单词文本通过词表映射后的one-hot向量</span></span><br><span class="line"><span class="string">        将x传给self.lut并与根号下self.d_model相乘作为结果返回</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        embedds = self.lut(x)</span><br><span class="line">        <span class="keyword">return</span> embedds * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>

<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">self.lut(x) 的操作实际上是在权重矩阵中查找与 x 中的每个整数索引相对应的行</span><br><span class="line">每个整数索引对应于权重矩阵中的一行，该行包含了对应的词嵌入向量</span><br></pre></td></tr></table></figure>



<h3 id="2-2-位置编码"><a href="#2-2-位置编码" class="headerlink" title="2.2 位置编码"></a>2.2 位置编码</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Positional Encodding位置编码的作用是为模型提供当前时间步的前后出现顺序的信息</span><br><span class="line">因为Transformer不像RNN那样的循环结构有前后不同时间步输入间天然的先后顺序</span><br><span class="line">所有的时间步是同时输入，并行推理的，因此在时间步的特征中融合进位置编码的信息是合理的。</span><br><span class="line"></span><br><span class="line">位置编码可以有很多选择，可以是固定的，也可以设置成可学习的参数。</span><br></pre></td></tr></table></figure>

<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">原论文中使用固定的位置编码。具体地，使用不同频率的sin和cos函数来进行位置编码，如下所示：</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231107193737530.png" alt="image-20231107193737530"></p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">其中pos代表单词位置，向量PE(pos)就是该单词对应的位置编码</span><br><span class="line">编码长度同Embedding层，这里设置的是512</span><br><span class="line">上面有两个公式，代表着位置编码向量中的元素，奇数位置和偶数位置使用两个不同的公式</span><br></pre></td></tr></table></figure>

<p>​                  <img src="/2023/11/19/Transformers/image-20231107195151986.png" alt="image-20231107195151986">                                                                                                                                         </p>
<p><strong>下面是位置编码模块的代码实现：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        位置编码器类的初始化函数</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        共有三个参数，分别是</span></span><br><span class="line"><span class="string">        d_model：词嵌入维度</span></span><br><span class="line"><span class="string">        dropout: dropout触发比率</span></span><br><span class="line"><span class="string">        max_len：每个句子的最大长度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings</span></span><br><span class="line">        <span class="comment"># 注意下面代码的计算方式与公式中给出的是不同的，但是是等价的，你可以尝试简单推导证明一下。</span></span><br><span class="line">        <span class="comment"># 这样计算是为了避免中间的数值计算结果超出float的范围，</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)                                                                                                                                                                                   </span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>

<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">因此，可以认为，最终模型的输入是若干个时间步对应的embedding，每一个时间步对应一个embedding</span><br><span class="line">可以理解为是当前时间步的一个综合的特征信息</span><br><span class="line">即包含了本身的语义信息，又包含了当前时间步在整个句子中的位置信息</span><br></pre></td></tr></table></figure>

<h3 id="2-3-Encoder-和-Decoder-包含的输入模块"><a href="#2-3-Encoder-和-Decoder-包含的输入模块" class="headerlink" title="2.3 Encoder 和 Decoder 包含的输入模块"></a>2.3 Encoder 和 Decoder 包含的输入模块</h3><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">两部分的输入的结构是相同的，只是推理时的用法不同</span><br><span class="line">编码器只推理一次，而解码器是类似RNN那样循环推理，不断生成预测结果的</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231107203101696.png" alt="image-20231107203101696"></p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">怎么理解？假设现在做的是一个法语-英语的机器翻译任务</span><br><span class="line">想把`Je suis étudiant`翻译为`I am a student`。</span><br><span class="line"></span><br><span class="line">那么我们输入给编码器的就是时间步数为3的embedding数组</span><br><span class="line">编码器只进行一次并行推理，即获得了对于输入的法语句子所提取的若干特征信息。</span><br><span class="line"></span><br><span class="line">而对于解码器，是循环推理，逐个单词生成结果的</span><br><span class="line">最开始，由于什么都还没预测，我们会将编码器提取的特征，以及一个句子起始符传给解码器</span><br><span class="line">解码器预期会输出一个单词`I`。然后有了预测的第一个单词</span><br><span class="line">我们就将`I`输入给解码器，会再预测出下一个单词`am`，</span><br><span class="line">再然后我们将`I am`作为输入喂给解码器</span><br><span class="line">以此类推直到预测出句子终止符完成预测</span><br></pre></td></tr></table></figure>

<h2 id="3、Encoder编码器"><a href="#3、Encoder编码器" class="headerlink" title="3、Encoder编码器"></a>3、Encoder编码器</h2><h3 id="3-1-Encoder"><a href="#3-1-Encoder" class="headerlink" title="3.1 Encoder"></a>3.1 Encoder</h3><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">编码器作用是用于对输入进行特征提取，为解码环节提供有效的语义信息</span><br><span class="line"></span><br><span class="line">整体来看编码器由N个编码器层简单堆叠而成，因此实现非常简单，代码如下</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义一个clones函数，来更方便的将某个结构复制若干份</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Encoder</span></span><br><span class="line"><span class="string">    The encoder is composed of a stack of N=6 identical layers.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        <span class="comment"># 调用时会将编码器层传进来，我们简单克隆N分，叠加在一起，组成完整的Encoder</span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mask放到后面会讲解</span><br></pre></td></tr></table></figure>

<h3 id="3-2-EncoderLayer"><a href="#3-2-EncoderLayer" class="headerlink" title="3.2 EncoderLayer"></a>3.2 EncoderLayer</h3><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">每个编码器层由两个子层连接结构组成：</span><br><span class="line"></span><br><span class="line">第一个子层包括一个多头自注意力层和层归一化以及一个残差连接；</span><br><span class="line"></span><br><span class="line">第二个子层包括一个前馈全连接层和层归一化以及一个残差连接；</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231107203952596.png" alt="image-20231107203952596"></p>
<p><strong>先定义一个SubLayerConnection类来描述这种结构关系</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    实现子层连接结构的类</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 原paper的方案</span></span><br><span class="line">        <span class="comment">#sublayer_out = sublayer(x)</span></span><br><span class="line">        <span class="comment">#x_norm = self.norm(x + self.dropout(sublayer_out))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 稍加调整的版本</span></span><br><span class="line">        sublayer_out = sublayer(x)</span><br><span class="line">        sublayer_out = self.dropout(sublayer_out)</span><br><span class="line">        x_norm = x + self.norm(sublayer_out)</span><br><span class="line">        <span class="keyword">return</span> x_norm</span><br></pre></td></tr></table></figure>

<p><strong>定义好了SubLayerConnection，就可以实现EncoderLayer的结构了</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;EncoderLayer is made up of two sublayer: self-attn and feed forward&quot;</span>                                                                                                         </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size   <span class="comment"># embedding&#x27;s dimention of model, 默认512</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="comment"># attention sub layer</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="comment"># feed forward sub layer</span></span><br><span class="line">        z = self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>

<p><strong>继续往下拆解，需要了解 attention层 和 feed_forward层的结构以及如何实现</strong></p>
<h3 id="3-3-Atention"><a href="#3-3-Atention" class="headerlink" title="3.3 Atention"></a>3.3 Atention</h3><p><img src="/2023/11/19/Transformers/image-20231108132102982.png" alt="image-20231108132102982"></p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">为什么叫Self - Attention呢?</span><br><span class="line">假设我们在执行机器翻译任务, 这个Attention不再是作用于我们给出的一种语言的输入Source和目标语言的输出Target, 而是作用于Source和Source内部, 即源语言的语义编码与原始输入Source之间的Attention, 这样能够获得单词在句子中更好的表示</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231108132854314.png" alt="image-20231108132854314"></p>
<p><img src="/2023/11/19/Transformers/image-20231108132945867.png" alt="image-20231108132945867"></p>
<p><img src="/2023/11/19/Transformers/image-20231108132838428.png" alt="image-20231108132838428"></p>
<p><img src="/2023/11/19/Transformers/scaledotattention.jpg" alt="img"></p>
<p><img src="/2023/11/19/Transformers/image-20231108133243393.png" alt="image-20231108133243393"></p>
<p><img src="/2023/11/19/Transformers/transformer10.png" alt="img"></p>
<p><img src="/2023/11/19/Transformers/image-20231108133713377.png" alt="image-20231108133713377"></p>
<p><img src="/2023/11/19/Transformers/transformer11.png" alt="img"></p>
<p><strong>综上, 将自注意力总结为:</strong></p>
<p><img src="/2023/11/19/Transformers/image-20231108133742066.png" alt="image-20231108133742066"></p>
<p><img src="/2023/11/19/Transformers/image-20231108133748987.png" alt="image-20231108133748987"></p>
<p><img src="/2023/11/19/Transformers/image-20231108190530270.png" alt="image-20231108190530270"></p>
<p><img src="/2023/11/19/Transformers/image-20231108190539290.png" alt="image-20231108190539290"></p>
<p><strong>下面是注意力模块的实现代码</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#首先取query的最后一维的大小，对应词嵌入维度</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#按照注意力公式，将query与key的转置相乘，这里面key是将最后两个维度进行转置，再除以缩放系数得到注意力得分张量scores</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#接着判断是否使用掩码张量</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment">#使用tensor的masked_fill方法，将掩码张量和scores张量每个位置一一比较，如果掩码张量则对应的scores张量用-1e9这个置来替换</span></span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#对scores的最后一维进行softmax操作，使用F.softmax方法，这样获得最终的注意力张量</span></span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#之后判断是否使用dropout进行随机置0</span></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#最后，根据公式将p_attn与value张量相乘获得最终的query注意力表示，同时返回注意力张量</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>



<h3 id="3-4-MultiHead-Attention"><a href="#3-4-MultiHead-Attention" class="headerlink" title="3.4 MultiHead-Attention"></a><strong>3.4 MultiHead-Attention</strong></h3><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">Multi - head Attention的思路和CNN中的多个卷积核起到的作用明显是一致的</span><br><span class="line">所谓”多头”, 放在卷积神经网络里就是卷积层多个卷积核的特征提取过程</span><br><span class="line">在这里就是进行多次注意力的提取, 就像多个卷积核一样</span><br><span class="line">多次不同的初始化矩阵经过训练可能会有多种不同的特征, 更有利于不同角度的特征抽取和信息提取.</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231108190908781.png" alt="image-20231108190908781"></p>
<p><strong>论文中采用了8个头的注意力, 即ℎ&#x3D;8, 得到多个提取出来的特征:</strong></p>
<p><img src="/2023/11/19/Transformers/image-20231108191039102.png" alt="image-20231108191039102"></p>
<p><img src="/2023/11/19/Transformers/image-20231108191052626.png" alt="image-20231108191052626"></p>
<p><strong>下面是多头注意力模块的实现代码</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="comment">#在类的初始化时，会传入三个参数，h代表头数，d_model代表词嵌入的维度，dropout代表进行dropout操作时置0比率，默认是0.1</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="comment">#在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，这是因为我们之后要给每个头分配等量的词特征，也就是embedding_dim/head个</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment">#得到每个头获得的分割词向量维度d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        <span class="comment">#传入头数h</span></span><br><span class="line">        self.h = h</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#创建linear层，通过nn的Linear实例化，它的内部变换矩阵是embedding_dim x embedding_dim，然后使用，为什么是四个呢，这是因为在多头注意力中，Q,K,V各需要一个，最后拼接的矩阵还需要一个，因此一共是四个</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        <span class="comment">#self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None</span></span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment">#前向逻辑函数，它输入参数有四个，前三个就是注意力机制需要的Q,K,V，最后一个是注意力机制中可能需要的mask掩码张量，默认是None</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            <span class="comment">#使用unsqueeze扩展维度，代表多头中的第n头</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本</span></span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        <span class="comment"># 首先利用zip将输入QKV与三个线性层组到一起，然后利用for循环，将输入QKV分别传到线性层中，做完线性变换后，开始为每个头分割输入，这里使用view方法对线性变换的结构进行维度重塑，多加了一个维度h代表头，这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，计算机会根据这种变换自动计算这里的值，然后对第二维和第三维进行转置操作，为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，从attention函数中可以看到，利用的是原始输入的倒数第一和第二维，这样我们就得到了每个头的输入</span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        <span class="comment"># 得到每个头的输入后，接下来就是将他们传入到attention中，这里直接调用我们之前实现的attention函数，同时也将mask和dropout传入其中</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">        <span class="comment"># 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的形状以方便后续的计算，因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，然后使用contiguous方法。这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，所以，下一步就是使用view重塑形状，变成和输入形状相同。  </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="comment">#最后使用线性层列表中的最后一个线性变换得到最终的多头注意力结构的输出</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>

<h3 id="3-5-Position-wise-Feed-Forward-neural-network"><a href="#3-5-Position-wise-Feed-Forward-neural-network" class="headerlink" title="3.5 Position-wise Feed Forward neural network"></a>3.5 Position-wise Feed Forward neural network</h3><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">在进行了Attention操作之后</span><br><span class="line">encoder和decoder中的每一层都包含了一个全连接前向网络</span><br><span class="line">对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109125320365.png" alt="image-20231109125320365"></p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">对于并行计算的不同单词, 通过的FFN参数是共享的</span><br><span class="line">也可以看做不同单词先后通过同一个FFN, 如下图所示:</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109125607481.png" alt="image-20231109125607481"></p>
<h3 id="3-6-Layer-Norm"><a href="#3-6-Layer-Norm" class="headerlink" title="3.6 Layer Norm"></a><strong>3.6 Layer Norm</strong></h3><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">你应该接触过Batch Norm, Layer Norm也是一种类似于Batch Norm的归一化方式</span><br><span class="line">同样能起到加快收敛的作用, 在NLP任务中比较常用</span><br><span class="line">在Batch Norm中, 记录下多个Batch中每维Feature的均值和方差, 并进行放缩和平移</span><br><span class="line">即对不同样本的同一个通道特征进行归一化</span><br><span class="line">在Layer Norm中, 只是换了一个维度, 我们对同一个样本的不同特征进行归一化</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109130306906.png" alt="image-20231109130306906"></p>
<p>​                                                                                                                                                                                                                                                                    </p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">如果想了解它为什么和NLP领域比较契合, 详见下文:</span><br><span class="line"></span><br><span class="line">大致原因是Batch Norm对于Embedding后的数据进行归一化, 假设每个Batch是由多个Embedding组成的, 按照Batch方向对每个归一化, 就是对每个词的Embedding整体归一化. 这样做非常没有道理, 不符合NLP的规律, 它反而加强了不同词之间的相关性.</span><br><span class="line"></span><br><span class="line">但如果按照Layer Norm, 按照Layer方向, 实际上是分别对每个Embedding后的词向量进行归一化, 这样每个词向量相对独立.</span><br><span class="line"></span><br><span class="line">这主要还是CV和NLP的数据属性决定的. 在CV中, 不同样本之间的Channel信息是具有共性的(因为图像还是要用2D来表示), 这部分信息非常重要, 如果归一化会损失很多信息. 而NLP中, 数据是Embedding来的, 本来也没有包含位置信息, 反而不同词向量之间毫无相关性, 关注单词本身的归一化效果会更好.</span><br></pre></td></tr></table></figure>

<h3 id="3-7-Residual"><a href="#3-7-Residual" class="headerlink" title="3.7 Residual"></a>3.7 Residual</h3><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">在Encoder中残差连接伴随着Layer Norm, 每次经过一个子层都要做一次残差连接</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109131015757.png" alt="image-20231109131015757"></p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">在Decoder中也是同样的, 每次经过子层也都要做残差连接</span><br></pre></td></tr></table></figure>

<h2 id="4、Decoder编码器"><a href="#4、Decoder编码器" class="headerlink" title="4、Decoder编码器"></a>4、Decoder编码器</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">当了解了Encoder的结构后, 结合起Decoder来看一下信息流</span><br><span class="line">假设只有两个Encoder和两个Decoder的堆叠, 那么信息的流动方向是这样的:</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109131412290.png" alt="image-20231109131412290"></p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">确实只有最后的Encoder将输出传递给了Decoder的Encoder - Decoder Attention</span><br><span class="line">Encoder的输出也是和Decoder唯一的交互数据，其最终输出就是经过多个堆叠的Encoder计算得来的与Encoder输入大小相同的向量，在交互的Encoder - Decoder Attention中会体现出来</span><br></pre></td></tr></table></figure>

<h3 id="4-1-Encoder-Decoder-Attention-Cross-Attention"><a href="#4-1-Encoder-Decoder-Attention-Cross-Attention" class="headerlink" title="4.1 Encoder-Decoder Attention(Cross Attention)"></a>4.1 Encoder-Decoder Attention(Cross Attention)</h3><p><img src="/2023/11/19/Transformers/image-20231109131850445.png" alt="image-20231109131850445"></p>
<h3 id="4-2-Outputs"><a href="#4-2-Outputs" class="headerlink" title="4.2 Outputs"></a>4.2 Outputs</h3><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">Decoder每过一个时间步不光接受Encoder的输出</span><br><span class="line">还接受了上一个Timestep的Decoder输入, 即论文中提到的”shifted right“.</span><br></pre></td></tr></table></figure>

<h3 id="4-3-Masked-Multi-Head-Attention"><a href="#4-3-Masked-Multi-Head-Attention" class="headerlink" title="4.3 Masked Multi-Head Attention"></a>4.3 Masked Multi-Head Attention</h3><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">Mask是Transformer中一个关键点</span><br><span class="line">Masked Multi-Head Attention只出现在Decoder中</span><br><span class="line">到了Decoder, 可就不再像Encoder那样直接把数据拿过来并行训练了</span><br><span class="line">如果也像Encoder那样把所有输入的词向量全一股脑堆进去, Decoder做Self-Attention可以无视解码的时间跨度, 获知全部的信息, 因此需要用Mask将当前预测的单词和之后的单词全都遮盖, 否则就没法训练了</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109132714857.png" alt="image-20231109132714857"></p>
<p><img src="/2023/11/19/Transformers/image-20231109133414985.png" alt="image-20231109133414985"></p>
<p><img src="/2023/11/19/Transformers/image-20231109133425348.png" alt="image-20231109133425348"></p>
<p><strong>做Softmax时, 所有的负无穷全变成了0, 不再干扰计算:</strong></p>
<p><img src="/2023/11/19/Transformers/image-20231109133439718.png" alt="image-20231109133439718"></p>
<p>其实Mask在对句子的<strong>无效部分填充</strong>时, 也是用同样方法将所有句子补齐, 无效部分用负无穷填充的</p>
<h2 id="5、最终输出"><a href="#5、最终输出" class="headerlink" title="5、最终输出"></a>5、最终输出</h2><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">最终输出很简单, 根据Decoder的输出经过FC层和Softmax得到对应的单词</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109133548546.png" alt="image-20231109133548546"></p>
<p>注意, Decoder的Embedding层和最后输出经过Softmax前的Linear层也是<strong>共享权重</strong>的</p>
]]></content>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>Parameter Efficient Fine-Tuning(PEFT)系列论文总结(三)</title>
    <url>/2023/12/27/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%B8%89)/</url>
    <content><![CDATA[<p>承接上篇Parameter Efficient Fine-Tuning(PEFT)系列论文总结(二)，本篇主要介绍LoRA及其各种变体的微调方法。</p>
<span id="more"></span>]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2023/11/17/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Leetcode 数组篇</title>
    <url>/2024/01/06/%E6%95%B0%E7%BB%84%E7%AF%87/</url>
    <content><![CDATA[<p>Leetcode刷题记录——数组篇</p>
<span id="more"></span>

<h2 id="Leetcode704-二分查找"><a href="#Leetcode704-二分查找" class="headerlink" title="Leetcode704  二分查找"></a>Leetcode704  二分查找</h2><p><strong>使用二分查找的前提条件：</strong></p>
<ul>
<li>数组有序</li>
<li>数组内无重复元素（因为一旦有重复元素，使用二分查找法返回的元素下标可能不是唯一的）</li>
</ul>
<p>二分查找涉及的很多的边界条件，逻辑比较简单，但就是写不好。例如到底是 <code>while(left &lt; right)</code> 还是 <code>while(left &lt;= right)</code>，到底是<code>right = middle</code>呢，还是要<code>right = middle - 1</code>？</p>
<p>因此要遵循<strong>循环不变量原则</strong>，就是在while寻找中每一次边界的处理都要坚持根据区间的定义来操作</p>
<p>区间的定义一般为两种，左闭右闭即<code>[left, right]</code>，或者左闭右开即<code>[left, right)</code>，本题采用左闭右闭的写法</p>
<p>区间的定义这就决定了二分法的代码应该如何写，<strong>因为定义target在[left, right]区间，所以有如下两点：</strong></p>
<ul>
<li><code>while (left &lt;= right)</code> 要使用 <code>&lt;=</code> ，因为<code>left == right</code>是有意义的，在<code>[left, right]</code>区间是合法的，所以使用 <code>&lt;=</code></li>
<li><code>if (nums[middle] &gt; target)</code> <code>right</code> 要赋值为 <code>middle - 1</code>，因为当前这个<code>nums[middle]</code>一定不是<code>target</code>，那么接下来要查找的左区间结束下标位置就是 <code>middle - 1</code></li>
</ul>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">            middle = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> target &lt; nums[middle]:</span><br><span class="line">                right = middle -<span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> target &gt; nums[middle]:</span><br><span class="line">                left = middle + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> middle</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure>





<h2 id="Leetcode27-移除元素"><a href="#Leetcode27-移除元素" class="headerlink" title="Leetcode27  移除元素"></a>Leetcode27  移除元素</h2><p>双指针法（快慢指针法）： <strong>通过一个快指针和慢指针在一个for循环下完成两个for循环的工作。</strong></p>
<p>定义快慢指针</p>
<ul>
<li>快指针：寻找新数组的元素 ，新数组就是不含有目标元素的数组</li>
<li>慢指针：指向更新 新数组下标的位置</li>
</ul>
<p>也就是快指针去寻找符合条件的元素，慢指针用来更新需要返回的数组，这样在一次遍历中，即可完成数组的更新</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeElement</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], val: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        slow = <span class="number">0</span></span><br><span class="line">        fast = <span class="number">0</span></span><br><span class="line">        length = <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> fast &lt; length:</span><br><span class="line">            <span class="keyword">if</span> nums[fast] != val:</span><br><span class="line">                nums[slow] = nums[fast]</span><br><span class="line">                slow = slow + <span class="number">1</span></span><br><span class="line">            fast = fast + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> slow</span><br></pre></td></tr></table></figure>





<h2 id="Leetcode977-有序数组的平方"><a href="#Leetcode977-有序数组的平方" class="headerlink" title="Leetcode977 有序数组的平方"></a>Leetcode977 有序数组的平方</h2><p>数组其实是有序的， 只不过负数平方之后可能成为最大数了。</p>
<p>那么数组平方的最大值就在数组的两端，不是最左边就是最右边，不可能是中间。</p>
<p>此时可以考虑双指针法了，<code>head</code>指向起始位置，<code>tail</code>指向终止位置。</p>
<p>定义一个新数组<code>res</code>，和<code>nums</code>数组一样的大小，让<code>index</code>指向<code>res</code>数组终止位置。</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sortedSquares</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        length = <span class="built_in">len</span>(nums)</span><br><span class="line">        head = <span class="number">0</span></span><br><span class="line">        tail = length - <span class="number">1</span></span><br><span class="line">        index = length - <span class="number">1</span></span><br><span class="line">        res = [<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)] * <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> head &lt;= tail:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">abs</span>(nums[head]) &gt; <span class="built_in">abs</span>(nums[tail]):</span><br><span class="line">                res[index] = nums[head] * nums[head]</span><br><span class="line">                index -= <span class="number">1</span></span><br><span class="line">                head += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res[index] = nums[tail] * nums[tail]</span><br><span class="line">                index -= <span class="number">1</span></span><br><span class="line">                tail -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>





<h2 id="Leetcode209-长度最小的子数组"><a href="#Leetcode209-长度最小的子数组" class="headerlink" title="Leetcode209 长度最小的子数组"></a>Leetcode209 长度最小的子数组</h2><p>首先想到的暴力法，思路很简单，两个<code>for</code>循环，找到所有的连续子组合，如果该组合<code>&gt;=target</code>则记录长度，最终返回长度最小的记录即可</p>
<p>这道题可以用<strong>滑动窗口</strong>的思想来解决：</p>
<p>在暴力解法中，是一个<code>for</code>循环为滑动窗口的起始位置，一个<code>for</code>循环为滑动窗口的终止位置，用两个<code>for</code>循环 完成了一个不断搜索区间的过程</p>
<p>那么滑动窗口如何用一个<code>for</code>循环来完成这个操作呢？</p>
<p>首先要思考，如果用一个<code>for</code>循环，那么应该表示滑动窗口的起始位置，还是终止位置？</p>
<p>如果只用一个<code>for</code>循环来表示滑动窗口的起始位置，那么如何遍历剩下的终止位置？</p>
<p>此时难免再次陷入暴力解法的怪圈</p>
<p>所以 如果只用一个<code>for</code>循环，那么这个循环的索引，一定是表示滑动窗口的终止位置</p>
<p><strong>Python代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">minSubArrayLen</span>(<span class="params">self, target: <span class="built_in">int</span>, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line">        end = <span class="number">0</span></span><br><span class="line">        length = <span class="built_in">len</span>(nums)</span><br><span class="line">        max_len = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> end <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">            <span class="built_in">sum</span> = <span class="built_in">sum</span> + nums[end]</span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">sum</span> &gt;= target:</span><br><span class="line">                <span class="comment"># 如果此时窗口内的和&gt;=target,那么试着剔除窗口的第一个元素，看是否还成立</span></span><br><span class="line">                <span class="comment"># 但操作前,需要先记录此时成立情况下的窗口长度</span></span><br><span class="line">                res = end - start + <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> res &lt; max_len:</span><br><span class="line">                    max_len = res</span><br><span class="line">                <span class="built_in">sum</span> = <span class="built_in">sum</span> - nums[start]</span><br><span class="line">                start += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> max_len == inf:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> max_len</span><br></pre></td></tr></table></figure>





<h2 id="Leetcode59-螺旋矩阵II"><a href="#Leetcode59-螺旋矩阵II" class="headerlink" title="Leetcode59 螺旋矩阵II"></a>Leetcode59 螺旋矩阵II</h2><p>这道题考察对代码的掌控能力，仍然会遇到复杂的边界问题，坚持<strong>循环不变量原则</strong></p>
<p>模拟顺时针画矩阵的过程:</p>
<ul>
<li>填充上行从左到右</li>
<li>填充右列从上到下</li>
<li>填充下行从右到左</li>
<li>填充左列从下到上</li>
</ul>
<p>要处理好边界问题，就要坚持<strong>循环不变量原则</strong></p>
<p>本题采用<strong>左闭右开</strong>的写法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generateMatrix</span>(<span class="params">self, n: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span><br><span class="line">        <span class="comment"># 先创建 n * n 的全0矩阵</span></span><br><span class="line">        nums = [[<span class="number">0</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        x = <span class="number">0</span></span><br><span class="line">        y = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 偶数n能走完整圈,奇数n会留中间一个空</span></span><br><span class="line">        loop = n // <span class="number">2</span></span><br><span class="line">        <span class="comment"># n为奇数时, 矩阵的中间位置为nums[mid][mid]</span></span><br><span class="line">        mid = n // <span class="number">2</span></span><br><span class="line">        <span class="comment"># 用来给矩阵中每一个空格赋值</span></span><br><span class="line">        count = <span class="number">1</span></span><br><span class="line">        offset = <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> loop:</span><br><span class="line">            <span class="comment"># 从左到右</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x, n - offset):</span><br><span class="line">                nums[x][i] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 从上到下</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(y, n - offset):</span><br><span class="line">                nums[i][n - offset] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 从右到左</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - offset, x, -<span class="number">1</span>):</span><br><span class="line">                nums[n - offset][i] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 从下到上</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - offset, y, -<span class="number">1</span>):</span><br><span class="line">                nums[i][y] = count</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">            x += <span class="number">1</span></span><br><span class="line">            y += <span class="number">1</span></span><br><span class="line">            offset += <span class="number">1</span></span><br><span class="line">            loop -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> n % <span class="number">2</span> != <span class="number">0</span>:</span><br><span class="line">            nums[mid][mid] = count</span><br><span class="line">        <span class="keyword">return</span> nums</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Leetcode 刷题</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title>Parameter Efficient Fine-Tuning(PEFT)系列论文总结(一)</title>
    <url>/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>本篇主要介绍早期的<code>PEFT</code>方法，包括<code>Adapter</code>适配器方法、<code>PET</code>、<code>Prefix Tuning</code>以及<code>Prompt Tuning</code>。</p>
<span id="more"></span>

<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>当前主流微调方法分为：<code>Fine-tune</code>和<code>PEFT</code>。</p>
<p><code>Fine-tune</code>，也叫全参微调。在<code>LLM</code>出现之前，<code>Bert</code>系列微调模型一直用的这种方法，即模型的全部参数权重参与更新以适配领域数据(有硬件条件的话自然是最好的选择)。</p>
<p><code>PEFT</code>, 包括<code>Prefix Tuning</code>、<code>P-Tuning V1/V2</code>、<code>LoRA</code>、<code>AdaLoRA</code>、<code>QLoRA</code>等方法，即部分模型参数参与微调。这种方式训练快，显存占用少，但是效果可能跟<code>FT（fine-tune）</code>比会稍有损失。</p>
<h2 id="二、Adapter适配器方法"><a href="#二、Adapter适配器方法" class="headerlink" title="二、Adapter适配器方法"></a>二、Adapter适配器方法</h2><p>谷歌的研究人员于2019年在论文<code>《Parameter-Efficient Transfer Learning for NLP》</code>提出针对 <code>BERT </code>的 <code>PEFT</code> 微调方式，拉开了 <code>PEFT </code>研究的序幕。他们指出：</p>
<ul>
<li>在面对特定的下游任务时，如果进行 <code>Full-fintuning</code>（即预训练模型中的所有参数都进行微调），过于低效</li>
<li>而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果</li>
</ul>
<p>于是他们设计了如下图所示的 <code>Adapter</code> 结构，作为全模型微调的一种替代方案:</p>
<img src="/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226185027290.png" alt="image-20231226185027290" style="zoom:67%;">

<p>在预训练模型每一层(或某些层)中添加<code>Adapter</code>模块(如上图左侧结构所示)，微调时冻结预训练模型主体，由<code>Adapter</code>模块学习特定下游任务的知识。每个<code>Adapter</code>模块由两个前馈子层组成，第一个前馈子层将<code>Transformer</code>块的输出作为输入，将原始输入维度<code>d</code>投影到<code>m</code>，通过控制<code>m</code>的大小来限制<code>Adapte</code>r模块的参数量，通常情况下<code>m&lt;&lt;d</code>。在输出阶段，通过第二个前馈子层还原输入维度，将<code>m</code>重新投影到<code>d</code>，作为<code>Adapter</code>模块的输出(如上图右侧结构)。</p>
<p>可以看到每一个<code>Adapter Layer</code>需要训练的参数，包括偏置的话是: <code>2md + m + d</code></p>
<p>通过添加<code>Adapter</code>模块来产生一个易于扩展的下游模型，每当出现新的下游任务，通过添加<code>Adapter</code>模块来避免全模型微调与灾难性遗忘的问题。<code>Adapter</code>方法不需要微调预训练模型的全部参数，通过引入少量针对特定任务的参数，来存储有关该任务的知识，降低对模型微调的算力要求。</p>
<img src="/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226192458146.png" alt="image-20231226192458146" style="zoom:67%;">

<img src="/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240101220641347.png" alt="image-20240101220641347" style="zoom:80%;">

<p>从实验结果来看，该方法能够在只额外对增加的<code>3.6%</code>参数规模（相比原来预训练模型的参数量）的情况下取得和<code>Full-finetuning</code>接近的效果（<code>GLUE</code>指标在<code>0.4%</code>以内）</p>
<h2 id="三、Pattern-Exploiting-Training-PET"><a href="#三、Pattern-Exploiting-Training-PET" class="headerlink" title="三、Pattern-Exploiting Training(PET)"></a>三、Pattern-Exploiting Training(PET)</h2><p>想要更好的理解下文将讲的<code>Prefix Tuning/P-Tuning</code>，便不得不提<code>Pattern-Exploiting Training(PET)</code>，所谓<code>PET</code>，主要的思想是借助由自然语言构成的模版(英文常称<code>Pattern</code>或<code>Prompt</code>)，将下游任务也转化为一个完形填空任务，这样就可以用<code>BERT</code>的<code>MLM</code>模型来进行预测了。</p>
<p>比如下图中通过条件前缀来实现情感分类和主题分类的例子:</p>
<img src="/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/957788384.png" alt="img" style="zoom:80%;">

<p>当然，这种方案也不是只有<code>MLM</code>模型可行，用<code>GPT</code>这样的单向语言模型其实也很简单：</p>
<img src="/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/2581387139.png" alt="img" style="zoom:80%;">

<p>不过由于语言模型是从左往右解码的，因此预测部分只能放在句末了(但还可以往补充前缀说明，只不过预测部分放在最后)</p>
<p>这种人为构造提示模板，就是在输入上加<code>Prompt</code>文本，再对输出进行映射。但这种方式怎么想都不是很优雅，无法避免人工的介入。即使有方法可以批量挖掘，但也有些复杂（有这个功夫能标不少高质量语料），而且模型毕竟是黑盒，对离散文本输入的鲁棒性很差。</p>
<h2 id="四、Prefix-Tuning"><a href="#四、Prefix-Tuning" class="headerlink" title="四、Prefix Tuning"></a>四、Prefix Tuning</h2><p>在<code>Prefix Tuning</code>之前的工作主要是人工设计离散的<code>template</code>或者自动化搜索离散template，问题在于最终的性能对人工设计的<code>template</code>的特别敏感：加一个词或者少一个词，或者变动位置，都会造成很大的变化，所以这种离散化的<code>token</code>的搜索出来的结果可能并不是最优的，下图给出的是一个例子：<br><img src="/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240106145327787.png" alt="image-20240106145327787"></p>
<p><strong>论文摘要：</strong>微调是利用大型预训练语言模型执行下游任务的实际方法。然而，它修改了所有语言模型参数，因此需要为每个任务存储一个完整的副本。在本文中，我们提出了一种名为“前缀调优”的轻量级微调替代方法，用于自然语言生成任务。这种方法保持语言模型参数不变，但优化了一个小型的连续任务特定向量(称为前缀)。前缀调优从提示方法中获得灵感，允许后续标记关注这个前缀，就像它们是”虚拟标记”。我们将前缀调优应用于<code>GPT-2</code>进行表格到文本的生成，以及使用BART进行摘要生成。我们发现，通过仅学习<code>0.1%</code>的参数，前缀调优在完整数据集中获得了与微调相当的性能，在低数据设置中表现更好，并且能够更好地推广到训练中未见的主题。</p>
<p><code>Prefix Tuning</code>是<code>PEFT</code>方法之一，<code>Prefix Tuning</code>之前的工作主要是人工设计模板或者自动化搜索模板，也是<code>prompt</code>范式的第一阶段，就是在输入上加上<code>prompt</code>文本，再对输出进行映射。这种离散模板对模型的鲁棒性很差。所以后续的研究都将离散的方式转成连续。<code>Prefix Tuning</code>在模型输入前添加一个连续的且任务特定的向量序列称之为<code>prefix</code>，固定<code>PLM(预训练模型)</code>的所有参数，只更新优化特定任务的<code>prefix</code>。</p>
<img src="/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231226200610499.png" alt="image-20231226200610499" style="zoom:67%;">

<h3 id="4-1-适配不同任务的prefix构造形式"><a href="#4-1-适配不同任务的prefix构造形式" class="headerlink" title="4.1 适配不同任务的prefix构造形式"></a>4.1 适配不同任务的prefix构造形式</h3><p>针对不同的模型结构，需要构造不同的 <code>Prefix</code>。</p>
<ul>
<li>针对自回归架构模型：在句子前面添加前缀，得到 <code>z = [PREFIX; x; y]</code>，合适的上文能够在固定 <code>LM</code> 的情况下去引导生成下文（比如：<code>GPT3</code>的上下文学习）。</li>
<li>针对编码器-解码器架构模型：<code>Encoder</code>和<code>Decoder</code>都增加了前缀，得到 <code>z = [PREFIX; x; PREFIX&#39;; y]</code>。</li>
<li><code>Encoder</code>端增加前缀是为了引导输入部分的编码，<code>Decoder</code> 端增加前缀是为了引导后续<code>token</code>的生成。</li>
</ul>
<img src="/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20231228111353270.png" alt="image-20231228111353270" style="zoom:67%;">

<h3 id="4-2-对virtual-token的编码方式"><a href="#4-2-对virtual-token的编码方式" class="headerlink" title="4.2 对virtual token的编码方式"></a>4.2 对virtual token的编码方式</h3><p>​		同时，为了防止直接更新 <code>Prefix</code> 的参数导致训练不稳定和性能下降的情况，在 <code>Prefix</code> 层前面加了 <code>MLP</code> 结构，训练完成后，只保留 <code>Prefix</code> 的参数。</p>
<p>​		除此之外，通过消融实验证实，只调整<code>embedding</code>层的表现力不够，将导致性能显著下降，因此，在每层<code>Transformer</code>的输入部分都加了<code>prompt</code>的参数，改动较大。</p>
<h2 id="四、Prompt-Tuning"><a href="#四、Prompt-Tuning" class="headerlink" title="四、Prompt Tuning"></a>四、Prompt Tuning</h2><p>2021年4月，<code>Google Research</code>通过此篇论文《<a href="https://arxiv.org/pdf/2104.08691">The Power of Scale for Parameter-Efficient Prompt Tuning</a>》提出了<code>Prompt Tuning</code>，论文中指出，该方法可以看作是<code>Prefix Tuning</code>的简化版本。</p>
<ul>
<li><p><code>Prefix Tuning</code>在每层<code>Transformer</code>的输入部分都加了<code>prompt</code>的参数，相比之下，<code>Prompt Tuning</code>使用单个提示表示，该表示前置于嵌入式输入。除了需要更少的参数外，所提出方法允许<code>Transformer</code>更新中间层任务表示。</p>
</li>
<li><p>此外，<code>Prefix tuning</code>也依赖于前缀的重新参数化来稳定学习，这在训练期间增加了大量参数，而<code>Prefix tuning</code>的配置不需要这种重新参数化，并且在<code>SuperGLUE</code>任务和模型尺寸上都是鲁棒的。</p>
</li>
<li><p>它冻结整个预训练模型，只允许每个下游任务在输入文本前添加额外的k个可调<code>tokens</code>(意味着它给每个任务都定义了自己的<code>Prompt</code>，在输入层加入<code>prompt tokens</code>)</p>
</li>
</ul>
<p>具体而言，如下图所示：</p>
<img src="/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240107102116261.png" alt="image-20240107102116261" style="zoom:80%;">

<ul>
<li><p><code>Model Tuning</code>需要为每个下游任务生成整个预训练模型的任务特定副本，并且推理必须分批执行</p>
</li>
<li><p><code>Prompt Tuning</code>只需要为每个任务存储一个小的特定于任务的提示，并使用原始的预训练模型支持混合任务推理</p>
</li>
</ul>
<p>且通过实验发现，随着预训练模型参数量的增加，<code>Prompt Tuning</code>的方法会逼近全参数微调的结果</p>
<img src="/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/image-20240107103047631.png" alt="image-20240107103047631" style="zoom:80%;">
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>PEFT</tag>
      </tags>
  </entry>
</search>
