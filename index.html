<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/logo.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/logo.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo.jpg">
  <link rel="mask-icon" href="/images/logo.jpg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"limokii.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Okii&#39;s blog">
<meta property="og:url" content="https://limokii.github.io/index.html">
<meta property="og:site_name" content="Okii&#39;s blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Okii">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://limokii.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Okii's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Okii's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">仅作为记录自己的学习过程</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">7</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">5</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">9</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://limokii.github.io/2024/01/06/%E6%95%B0%E7%BB%84%E7%AF%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Okii">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Okii's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/06/%E6%95%B0%E7%BB%84%E7%AF%87/" class="post-title-link" itemprop="url">Leetcode 数组篇</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-01-06 23:12:11" itemprop="dateCreated datePublished" datetime="2024-01-06T23:12:11+08:00">2024-01-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Leetcode-%E5%88%B7%E9%A2%98/" itemprop="url" rel="index"><span itemprop="name">Leetcode 刷题</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Leetcode刷题记录——数组篇</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2024/01/06/%E6%95%B0%E7%BB%84%E7%AF%87/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://limokii.github.io/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Okii">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Okii's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/" class="post-title-link" itemprop="url">LoRA微调实战</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-31 23:12:11" itemprop="dateCreated datePublished" datetime="2023-12-31T23:12:11+08:00">2023-12-31</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/" itemprop="url" rel="index"><span itemprop="name">项目实战</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>15 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>使用LoRA微调LLaMa2，训练LLM在给定上下文无法回答用户问题时拒绝回答的能力，而不是胡说。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/12/31/LoRA%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://limokii.github.io/2023/12/27/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%B8%89)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Okii">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Okii's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/27/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%B8%89)/" class="post-title-link" itemprop="url">Parameter Efficient Fine-Tuning(PEFT)系列论文总结(三)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-27 23:12:11" itemprop="dateCreated datePublished" datetime="2023-12-27T23:12:11+08:00">2023-12-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>71</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>承接上篇Parameter Efficient Fine-Tuning(PEFT)系列论文总结(二)，本篇主要介绍LoRA及其各种变体的微调方法。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/12/27/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%B8%89)/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://limokii.github.io/2023/12/26/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Okii">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Okii's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/26/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/" class="post-title-link" itemprop="url">Parameter Efficient Fine-Tuning(PEFT)系列论文总结(二)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-26 23:12:11" itemprop="dateCreated datePublished" datetime="2023-12-26T23:12:11+08:00">2023-12-26</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>承接上篇Parameter Efficient Fine-Tuning(PEFT)系列论文总结(一)，本篇主要介绍P-Tuning系列的微调方法。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/12/26/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93(%E4%BA%8C)/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://limokii.github.io/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Okii">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Okii's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">Parameter Efficient Fine-Tuning(PEFT)系列论文总结(一)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-25 23:12:11" itemprop="dateCreated datePublished" datetime="2023-12-25T23:12:11+08:00">2023-12-25</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87/" itemprop="url" rel="index"><span itemprop="name">论文</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本篇主要介绍早期的<code>PEFT</code>方法，包括<code>Adapter</code>适配器方法、<code>PET</code>、<code>Prefix Tuning</code>以及<code>Prompt Tuning</code>。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/12/25/Parameter%20Efficient%20Fine-Tuning(PEFT)%E7%B3%BB%E5%88%97%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://limokii.github.io/2023/12/18/Huggingface%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E9%95%9C%E5%83%8F%E4%B8%8B%E8%BD%BD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Okii">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Okii's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/18/Huggingface%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E9%95%9C%E5%83%8F%E4%B8%8B%E8%BD%BD/" class="post-title-link" itemprop="url">Huggingface 服务器端镜像下载</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-18 23:12:11" itemprop="dateCreated datePublished" datetime="2023-12-18T23:12:11+08:00">2023-12-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%91%BD%E4%BB%A4/" itemprop="url" rel="index"><span itemprop="name">服务器命令</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>748</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>使用<code>Huggingface</code> 官方提供的 <a target="_blank" rel="noopener" href="https://hf-mirror.com/docs/huggingface_hub/guides/download#download-from-the-cli">*<em><code>huggingface-cli</code>*</em> </a>命令行工具在服务器端镜像下载权重或文件。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/12/18/Huggingface%20%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E9%95%9C%E5%83%8F%E4%B8%8B%E8%BD%BD/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://limokii.github.io/2023/12/13/Hexo%20+%20Typora%20+%20Github%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Okii">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Okii's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/13/Hexo%20+%20Typora%20+%20Github%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/" class="post-title-link" itemprop="url">Hexo + Typora + Github 博客搭建</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-12-13 23:12:11" itemprop="dateCreated datePublished" datetime="2023-12-13T23:12:11+08:00">2023-12-13</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%8D%9A%E5%AE%A2/" itemprop="url" rel="index"><span itemprop="name">博客</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>7 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Hexo 是一个高效简洁的静态博客框架，支持 Markdown 写作语法，插件丰富，主题优雅，部署方便。目前已成为多数人博客建站的选择，本博客采用Hexo搭建，Markdown编辑软件为Typora并且部署在Github Page上。</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2023/12/13/Hexo%20+%20Typora%20+%20Github%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://limokii.github.io/2023/11/19/Transformers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Okii">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Okii's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/11/19/Transformers/" class="post-title-link" itemprop="url">Transformers</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-11-19 23:12:11" itemprop="dateCreated datePublished" datetime="2023-11-19T23:12:11+08:00">2023-11-19</time>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>17 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Transformers理解"><a href="#Transformers理解" class="headerlink" title="Transformers理解"></a>Transformers理解</h1><h2 id="1、模型结构概览"><a href="#1、模型结构概览" class="headerlink" title="1、模型结构概览"></a>1、模型结构概览</h2><p><strong>大致结构图：</strong></p>
<p><img src="/2023/11/19/Transformers/transformer3.png" alt="img"></p>
<p><strong>细致结构图：</strong></p>
<p><img src="/2023/11/19/Transformers/transformer.jpg" alt="img"></p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">模型大致分为Encoder(编码器)和Decoder(解码器)两个部分，分别对应上图中的左右两部分。</span><br><span class="line">其中编码器由N个相同的层堆叠在一起(原论文取N=6)，每一层又有两个子层</span><br><span class="line"></span><br><span class="line">第一个子层是一个Multi-Head Attention(多头的自注意机制)</span><br><span class="line">第二个子层是一个简单的Feed Forward(全连接前馈网络)</span><br><span class="line">两个子层都添加了一个残差连接+layer normalization的操作</span><br><span class="line"></span><br><span class="line">模型的解码器同样是堆叠了N个相同的层，不过和编码器中每层的结构稍有不同。</span><br><span class="line">对于解码器的每一层，除了编码器中的两个子层Multi-Head Attention和Feed Forward，解码器还包含一个子层Masked Multi-Head Attention，如图中所示每个子层同样也用了residual以及layer normalization</span><br><span class="line"></span><br><span class="line">模型的输入由Input Embedding和Positional Encoding(位置编码)两部分组合而成(直接对位相加)</span><br><span class="line">模型的输出由Decoder的输出简单的经过softmax得到</span><br><span class="line"></span><br><span class="line">结合上图对Transformer模型的结构做了个大致的梳理，下面对提及的每个模块进行详细介绍</span><br></pre></td></tr></table></figure>

<h2 id="2、模型的输入部分"><a href="#2、模型的输入部分" class="headerlink" title="2、模型的输入部分"></a>2、模型的输入部分</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入部分包含两个模块</span><br><span class="line">Embedding 和 Positional Encoding</span><br></pre></td></tr></table></figure>

<h3 id="2-1-Eebedding层"><a href="#2-1-Eebedding层" class="headerlink" title="2.1 Eebedding层"></a>2.1 Eebedding层</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Embedding层的作用是将某种格式的输入数据，例如文本</span><br><span class="line">转变为模型可以处理的向量表示，来描述原始数据所包含的信息。</span><br><span class="line"></span><br><span class="line">Embedding层输出的可以理解为当前时间步的特征，如果是文本任务，这里就可以是Word Embedding</span><br><span class="line">如果是其他任务，就可以是任何合理方法所提取的特征。</span><br><span class="line"></span><br><span class="line">构建Embedding层的代码的核心是借助torch提供的nn.Embedding，如下：</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        类的初始化函数</span></span><br><span class="line"><span class="string">        d_model：指词嵌入的维度</span></span><br><span class="line"><span class="string">        vocab:指词表的大小                           </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        <span class="comment">#之后就是调用nn中的预定义层Embedding，获得一个词嵌入对象self.lut</span></span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        <span class="comment">#最后就是将d_model传入类中</span></span><br><span class="line">        self.d_model =d_model</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Embedding层的前向传播逻辑</span></span><br><span class="line"><span class="string">        参数x：这里代表输入给模型的单词文本通过词表映射后的one-hot向量</span></span><br><span class="line"><span class="string">        将x传给self.lut并与根号下self.d_model相乘作为结果返回</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        embedds = self.lut(x)</span><br><span class="line">        <span class="keyword">return</span> embedds * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>

<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.lut(x) 的操作实际上是在权重矩阵中查找与 x 中的每个整数索引相对应的行</span><br><span class="line">每个整数索引对应于权重矩阵中的一行，该行包含了对应的词嵌入向量</span><br></pre></td></tr></table></figure>



<h3 id="2-2-位置编码"><a href="#2-2-位置编码" class="headerlink" title="2.2 位置编码"></a>2.2 位置编码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Positional Encodding位置编码的作用是为模型提供当前时间步的前后出现顺序的信息</span><br><span class="line">因为Transformer不像RNN那样的循环结构有前后不同时间步输入间天然的先后顺序</span><br><span class="line">所有的时间步是同时输入，并行推理的，因此在时间步的特征中融合进位置编码的信息是合理的。</span><br><span class="line"></span><br><span class="line">位置编码可以有很多选择，可以是固定的，也可以设置成可学习的参数。</span><br></pre></td></tr></table></figure>

<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">原论文中使用固定的位置编码。具体地，使用不同频率的sin和cos函数来进行位置编码，如下所示：</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231107193737530.png" alt="image-20231107193737530"></p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">其中pos代表单词位置，向量PE(pos)就是该单词对应的位置编码</span><br><span class="line">编码长度同Embedding层，这里设置的是512</span><br><span class="line">上面有两个公式，代表着位置编码向量中的元素，奇数位置和偶数位置使用两个不同的公式</span><br></pre></td></tr></table></figure>

<p>​                  <img src="/2023/11/19/Transformers/image-20231107195151986.png" alt="image-20231107195151986">                                                                                                                                         </p>
<p><strong>下面是位置编码模块的代码实现：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        位置编码器类的初始化函数</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        共有三个参数，分别是</span></span><br><span class="line"><span class="string">        d_model：词嵌入维度</span></span><br><span class="line"><span class="string">        dropout: dropout触发比率</span></span><br><span class="line"><span class="string">        max_len：每个句子的最大长度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings</span></span><br><span class="line">        <span class="comment"># 注意下面代码的计算方式与公式中给出的是不同的，但是是等价的，你可以尝试简单推导证明一下。</span></span><br><span class="line">        <span class="comment"># 这样计算是为了避免中间的数值计算结果超出float的范围，</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)                                                                                                                                                                                   </span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>

<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">因此，可以认为，最终模型的输入是若干个时间步对应的embedding，每一个时间步对应一个embedding</span><br><span class="line">可以理解为是当前时间步的一个综合的特征信息</span><br><span class="line">即包含了本身的语义信息，又包含了当前时间步在整个句子中的位置信息</span><br></pre></td></tr></table></figure>

<h3 id="2-3-Encoder-和-Decoder-包含的输入模块"><a href="#2-3-Encoder-和-Decoder-包含的输入模块" class="headerlink" title="2.3 Encoder 和 Decoder 包含的输入模块"></a>2.3 Encoder 和 Decoder 包含的输入模块</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">两部分的输入的结构是相同的，只是推理时的用法不同</span><br><span class="line">编码器只推理一次，而解码器是类似RNN那样循环推理，不断生成预测结果的</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231107203101696.png" alt="image-20231107203101696"></p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">怎么理解？假设现在做的是一个法语-英语的机器翻译任务</span><br><span class="line">想把`Je suis étudiant`翻译为`I am a student`。</span><br><span class="line"></span><br><span class="line">那么我们输入给编码器的就是时间步数为3的embedding数组</span><br><span class="line">编码器只进行一次并行推理，即获得了对于输入的法语句子所提取的若干特征信息。</span><br><span class="line"></span><br><span class="line">而对于解码器，是循环推理，逐个单词生成结果的</span><br><span class="line">最开始，由于什么都还没预测，我们会将编码器提取的特征，以及一个句子起始符传给解码器</span><br><span class="line">解码器预期会输出一个单词`I`。然后有了预测的第一个单词</span><br><span class="line">我们就将`I`输入给解码器，会再预测出下一个单词`am`，</span><br><span class="line">再然后我们将`I am`作为输入喂给解码器</span><br><span class="line">以此类推直到预测出句子终止符完成预测</span><br></pre></td></tr></table></figure>

<h2 id="3、Encoder编码器"><a href="#3、Encoder编码器" class="headerlink" title="3、Encoder编码器"></a>3、Encoder编码器</h2><h3 id="3-1-Encoder"><a href="#3-1-Encoder" class="headerlink" title="3.1 Encoder"></a>3.1 Encoder</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">编码器作用是用于对输入进行特征提取，为解码环节提供有效的语义信息</span><br><span class="line"></span><br><span class="line">整体来看编码器由N个编码器层简单堆叠而成，因此实现非常简单，代码如下</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个clones函数，来更方便的将某个结构复制若干份</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Encoder</span></span><br><span class="line"><span class="string">    The encoder is composed of a stack of N=6 identical layers.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        <span class="comment"># 调用时会将编码器层传进来，我们简单克隆N分，叠加在一起，组成完整的Encoder</span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mask放到后面会讲解</span><br></pre></td></tr></table></figure>

<h3 id="3-2-EncoderLayer"><a href="#3-2-EncoderLayer" class="headerlink" title="3.2 EncoderLayer"></a>3.2 EncoderLayer</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">每个编码器层由两个子层连接结构组成：</span><br><span class="line"></span><br><span class="line">第一个子层包括一个多头自注意力层和层归一化以及一个残差连接；</span><br><span class="line"></span><br><span class="line">第二个子层包括一个前馈全连接层和层归一化以及一个残差连接；</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231107203952596.png" alt="image-20231107203952596"></p>
<p><strong>先定义一个SubLayerConnection类来描述这种结构关系</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    实现子层连接结构的类</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 原paper的方案</span></span><br><span class="line">        <span class="comment">#sublayer_out = sublayer(x)</span></span><br><span class="line">        <span class="comment">#x_norm = self.norm(x + self.dropout(sublayer_out))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 稍加调整的版本</span></span><br><span class="line">        sublayer_out = sublayer(x)</span><br><span class="line">        sublayer_out = self.dropout(sublayer_out)</span><br><span class="line">        x_norm = x + self.norm(sublayer_out)</span><br><span class="line">        <span class="keyword">return</span> x_norm</span><br></pre></td></tr></table></figure>

<p><strong>定义好了SubLayerConnection，就可以实现EncoderLayer的结构了</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;EncoderLayer is made up of two sublayer: self-attn and feed forward&quot;</span>                                                                                                         </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size   <span class="comment"># embedding&#x27;s dimention of model, 默认512</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="comment"># attention sub layer</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="comment"># feed forward sub layer</span></span><br><span class="line">        z = self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line">        <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>

<p><strong>继续往下拆解，需要了解 attention层 和 feed_forward层的结构以及如何实现</strong></p>
<h3 id="3-3-Atention"><a href="#3-3-Atention" class="headerlink" title="3.3 Atention"></a>3.3 Atention</h3><p><img src="/2023/11/19/Transformers/image-20231108132102982.png" alt="image-20231108132102982"></p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">为什么叫Self - Attention呢?</span><br><span class="line">假设我们在执行机器翻译任务, 这个Attention不再是作用于我们给出的一种语言的输入Source和目标语言的输出Target, 而是作用于Source和Source内部, 即源语言的语义编码与原始输入Source之间的Attention, 这样能够获得单词在句子中更好的表示</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231108132854314.png" alt="image-20231108132854314"></p>
<p><img src="/2023/11/19/Transformers/image-20231108132945867.png" alt="image-20231108132945867"></p>
<p><img src="/2023/11/19/Transformers/image-20231108132838428.png" alt="image-20231108132838428"></p>
<p><img src="/2023/11/19/Transformers/scaledotattention.jpg" alt="img"></p>
<p><img src="/2023/11/19/Transformers/image-20231108133243393.png" alt="image-20231108133243393"></p>
<p><img src="/2023/11/19/Transformers/transformer10.png" alt="img"></p>
<p><img src="/2023/11/19/Transformers/image-20231108133713377.png" alt="image-20231108133713377"></p>
<p><img src="/2023/11/19/Transformers/transformer11.png" alt="img"></p>
<p><strong>综上, 将自注意力总结为:</strong></p>
<p><img src="/2023/11/19/Transformers/image-20231108133742066.png" alt="image-20231108133742066"></p>
<p><img src="/2023/11/19/Transformers/image-20231108133748987.png" alt="image-20231108133748987"></p>
<p><img src="/2023/11/19/Transformers/image-20231108190530270.png" alt="image-20231108190530270"></p>
<p><img src="/2023/11/19/Transformers/image-20231108190539290.png" alt="image-20231108190539290"></p>
<p><strong>下面是注意力模块的实现代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#首先取query的最后一维的大小，对应词嵌入维度</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#按照注意力公式，将query与key的转置相乘，这里面key是将最后两个维度进行转置，再除以缩放系数得到注意力得分张量scores</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#接着判断是否使用掩码张量</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment">#使用tensor的masked_fill方法，将掩码张量和scores张量每个位置一一比较，如果掩码张量则对应的scores张量用-1e9这个置来替换</span></span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#对scores的最后一维进行softmax操作，使用F.softmax方法，这样获得最终的注意力张量</span></span><br><span class="line">    p_attn = F.softmax(scores, dim = -<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#之后判断是否使用dropout进行随机置0</span></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#最后，根据公式将p_attn与value张量相乘获得最终的query注意力表示，同时返回注意力张量</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>



<h3 id="3-4-MultiHead-Attention"><a href="#3-4-MultiHead-Attention" class="headerlink" title="3.4 MultiHead-Attention"></a><strong>3.4 MultiHead-Attention</strong></h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Multi - head Attention的思路和CNN中的多个卷积核起到的作用明显是一致的</span><br><span class="line">所谓”多头”, 放在卷积神经网络里就是卷积层多个卷积核的特征提取过程</span><br><span class="line">在这里就是进行多次注意力的提取, 就像多个卷积核一样</span><br><span class="line">多次不同的初始化矩阵经过训练可能会有多种不同的特征, 更有利于不同角度的特征抽取和信息提取.</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231108190908781.png" alt="image-20231108190908781"></p>
<p><strong>论文中采用了8个头的注意力, 即ℎ&#x3D;8, 得到多个提取出来的特征:</strong></p>
<p><img src="/2023/11/19/Transformers/image-20231108191039102.png" alt="image-20231108191039102"></p>
<p><img src="/2023/11/19/Transformers/image-20231108191052626.png" alt="image-20231108191052626"></p>
<p><strong>下面是多头注意力模块的实现代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="comment">#在类的初始化时，会传入三个参数，h代表头数，d_model代表词嵌入的维度，dropout代表进行dropout操作时置0比率，默认是0.1</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="comment">#在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，这是因为我们之后要给每个头分配等量的词特征，也就是embedding_dim/head个</span></span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment">#得到每个头获得的分割词向量维度d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        <span class="comment">#传入头数h</span></span><br><span class="line">        self.h = h</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#创建linear层，通过nn的Linear实例化，它的内部变换矩阵是embedding_dim x embedding_dim，然后使用，为什么是四个呢，这是因为在多头注意力中，Q,K,V各需要一个，最后拼接的矩阵还需要一个，因此一共是四个</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        <span class="comment">#self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None</span></span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment">#前向逻辑函数，它输入参数有四个，前三个就是注意力机制需要的Q,K,V，最后一个是注意力机制中可能需要的mask掩码张量，默认是None</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            <span class="comment">#使用unsqueeze扩展维度，代表多头中的第n头</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本</span></span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        <span class="comment"># 首先利用zip将输入QKV与三个线性层组到一起，然后利用for循环，将输入QKV分别传到线性层中，做完线性变换后，开始为每个头分割输入，这里使用view方法对线性变换的结构进行维度重塑，多加了一个维度h代表头，这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，计算机会根据这种变换自动计算这里的值，然后对第二维和第三维进行转置操作，为了让代表句子长度维度和词向量维度能够相邻，这样注意力机制才能找到词义与句子位置的关系，从attention函数中可以看到，利用的是原始输入的倒数第一和第二维，这样我们就得到了每个头的输入</span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        <span class="comment"># 得到每个头的输入后，接下来就是将他们传入到attention中，这里直接调用我们之前实现的attention函数，同时也将mask和dropout传入其中</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">        <span class="comment"># 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，我们需要将其转换为输入的形状以方便后续的计算，因此这里开始进行第一步处理环节的逆操作，先对第二和第三维进行转置，然后使用contiguous方法。这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，所以，下一步就是使用view重塑形状，变成和输入形状相同。  </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="comment">#最后使用线性层列表中的最后一个线性变换得到最终的多头注意力结构的输出</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>

<h3 id="3-5-Position-wise-Feed-Forward-neural-network"><a href="#3-5-Position-wise-Feed-Forward-neural-network" class="headerlink" title="3.5 Position-wise Feed Forward neural network"></a>3.5 Position-wise Feed Forward neural network</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在进行了Attention操作之后</span><br><span class="line">encoder和decoder中的每一层都包含了一个全连接前向网络</span><br><span class="line">对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活输出</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109125320365.png" alt="image-20231109125320365"></p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">对于并行计算的不同单词, 通过的FFN参数是共享的</span><br><span class="line">也可以看做不同单词先后通过同一个FFN, 如下图所示:</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109125607481.png" alt="image-20231109125607481"></p>
<h3 id="3-6-Layer-Norm"><a href="#3-6-Layer-Norm" class="headerlink" title="3.6 Layer Norm"></a><strong>3.6 Layer Norm</strong></h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">你应该接触过Batch Norm, Layer Norm也是一种类似于Batch Norm的归一化方式</span><br><span class="line">同样能起到加快收敛的作用, 在NLP任务中比较常用</span><br><span class="line">在Batch Norm中, 记录下多个Batch中每维Feature的均值和方差, 并进行放缩和平移</span><br><span class="line">即对不同样本的同一个通道特征进行归一化</span><br><span class="line">在Layer Norm中, 只是换了一个维度, 我们对同一个样本的不同特征进行归一化</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109130306906.png" alt="image-20231109130306906"></p>
<p>​                                                                                                                                                                                                                                                                    </p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">如果想了解它为什么和NLP领域比较契合, 详见下文:</span><br><span class="line"></span><br><span class="line">大致原因是Batch Norm对于Embedding后的数据进行归一化, 假设每个Batch是由多个Embedding组成的, 按照Batch方向对每个归一化, 就是对每个词的Embedding整体归一化. 这样做非常没有道理, 不符合NLP的规律, 它反而加强了不同词之间的相关性.</span><br><span class="line"></span><br><span class="line">但如果按照Layer Norm, 按照Layer方向, 实际上是分别对每个Embedding后的词向量进行归一化, 这样每个词向量相对独立.</span><br><span class="line"></span><br><span class="line">这主要还是CV和NLP的数据属性决定的. 在CV中, 不同样本之间的Channel信息是具有共性的(因为图像还是要用2D来表示), 这部分信息非常重要, 如果归一化会损失很多信息. 而NLP中, 数据是Embedding来的, 本来也没有包含位置信息, 反而不同词向量之间毫无相关性, 关注单词本身的归一化效果会更好.</span><br></pre></td></tr></table></figure>

<h3 id="3-7-Residual"><a href="#3-7-Residual" class="headerlink" title="3.7 Residual"></a>3.7 Residual</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在Encoder中残差连接伴随着Layer Norm, 每次经过一个子层都要做一次残差连接</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109131015757.png" alt="image-20231109131015757"></p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在Decoder中也是同样的, 每次经过子层也都要做残差连接</span><br></pre></td></tr></table></figure>

<h2 id="4、Decoder编码器"><a href="#4、Decoder编码器" class="headerlink" title="4、Decoder编码器"></a>4、Decoder编码器</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">当了解了Encoder的结构后, 结合起Decoder来看一下信息流</span><br><span class="line">假设只有两个Encoder和两个Decoder的堆叠, 那么信息的流动方向是这样的:</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109131412290.png" alt="image-20231109131412290"></p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">确实只有最后的Encoder将输出传递给了Decoder的Encoder - Decoder Attention</span><br><span class="line">Encoder的输出也是和Decoder唯一的交互数据，其最终输出就是经过多个堆叠的Encoder计算得来的与Encoder输入大小相同的向量，在交互的Encoder - Decoder Attention中会体现出来</span><br></pre></td></tr></table></figure>

<h3 id="4-1-Encoder-Decoder-Attention-Cross-Attention"><a href="#4-1-Encoder-Decoder-Attention-Cross-Attention" class="headerlink" title="4.1 Encoder-Decoder Attention(Cross Attention)"></a>4.1 Encoder-Decoder Attention(Cross Attention)</h3><p><img src="/2023/11/19/Transformers/image-20231109131850445.png" alt="image-20231109131850445"></p>
<h3 id="4-2-Outputs"><a href="#4-2-Outputs" class="headerlink" title="4.2 Outputs"></a>4.2 Outputs</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Decoder每过一个时间步不光接受Encoder的输出</span><br><span class="line">还接受了上一个Timestep的Decoder输入, 即论文中提到的”shifted right“.</span><br></pre></td></tr></table></figure>

<h3 id="4-3-Masked-Multi-Head-Attention"><a href="#4-3-Masked-Multi-Head-Attention" class="headerlink" title="4.3 Masked Multi-Head Attention"></a>4.3 Masked Multi-Head Attention</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Mask是Transformer中一个关键点</span><br><span class="line">Masked Multi-Head Attention只出现在Decoder中</span><br><span class="line">到了Decoder, 可就不再像Encoder那样直接把数据拿过来并行训练了</span><br><span class="line">如果也像Encoder那样把所有输入的词向量全一股脑堆进去, Decoder做Self-Attention可以无视解码的时间跨度, 获知全部的信息, 因此需要用Mask将当前预测的单词和之后的单词全都遮盖, 否则就没法训练了</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109132714857.png" alt="image-20231109132714857"></p>
<p><img src="/2023/11/19/Transformers/image-20231109133414985.png" alt="image-20231109133414985"></p>
<p><img src="/2023/11/19/Transformers/image-20231109133425348.png" alt="image-20231109133425348"></p>
<p><strong>做Softmax时, 所有的负无穷全变成了0, 不再干扰计算:</strong></p>
<p><img src="/2023/11/19/Transformers/image-20231109133439718.png" alt="image-20231109133439718"></p>
<p>其实Mask在对句子的<strong>无效部分填充</strong>时, 也是用同样方法将所有句子补齐, 无效部分用负无穷填充的</p>
<h2 id="5、最终输出"><a href="#5、最终输出" class="headerlink" title="5、最终输出"></a>5、最终输出</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最终输出很简单, 根据Decoder的输出经过FC层和Softmax得到对应的单词</span><br></pre></td></tr></table></figure>

<p><img src="/2023/11/19/Transformers/image-20231109133548546.png" alt="image-20231109133548546"></p>
<p>注意, Decoder的Embedding层和最后输出经过Softmax前的Linear层也是<strong>共享权重</strong>的</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://limokii.github.io/2023/11/17/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Okii">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Okii's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/11/17/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-11-17 12:39:38" itemprop="dateCreated datePublished" datetime="2023-11-17T12:39:38+08:00">2023-11-17</time>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>367</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Okii"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Okii</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/LimOkii" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;LimOkii" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lyfei1126@gmail.com" title="E-Mail → mailto:lyfei1126@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Okii</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
